{"cells":[{"metadata":{},"cell_type":"markdown","source":"Principal Files\nThis data challenge contains one dataset of 2000 training sequences. The main files available are the following ones\n\n* Xtr.csv - the training sequences.\n* \n* Xte.csv - the test sequences.\n* \n* Ytr.csv - the sequence labels of the training sequences indicating bound (1) or not (0).\n* \nEach row of Xtr.csv represents a sequence. Xte.csv contains 1000 test sequences, for which you need to predict. Ytr.csv contains the labels corresponding to the training data, in the same format as a submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nimport optuna\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score, precision_score\n\nimport os\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xtr.csv\", sep=' ',index_col=0).values\n\nlabels=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Ytr.csv\", sep=',',index_col=0).values\n\n\ntest_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xte.csv\",sep=' ', index_col=0).values\n\n\n#optional data \n\ntrain_op_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xtr_mat100.csv\", sep=' ',header=None).values\ntest_op_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xte_mat100.csv\", sep=' ',header=None).values\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels= np.where(labels==0,-1,1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploring"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape,test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# optional data view"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_op_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_one=[train_op_data[i], labels[i]==1, i in range(len(labels)) ]\nlabels_zeros=[train_op_data[i], labels[i]==0, i in range(len(labels)) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(labels_one)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def plot_margin(X1_train, X2_train, clf):\n        \n#         def f(x, w, b, c=0):\n#             # given x, return y such that [x,y] in on the line\n#             # w.x + b = c\n#             return (-w[0] * x - b + c) / w[1]\n        \n#         pl.plot(X1_train[:,0], X1_train[:,1], \"ro\")\n#         pl.plot(X2_train[:,0], X2_train[:,1], \"bo\")\n#         pl.scatter(clf.sv[:,0], clf.sv[:,1], s=100, c=\"g\")\n        \n#         # w.x + b = 0\n#         a0 = -4; a1 = f(a0, clf.w, clf.b)\n#         b0 = 4; b1 = f(b0, clf.w, clf.b)\n#         pl.plot([a0,b0], [a1,b1], \"k\")\n        \n#         # w.x + b = 1\n#         a0 = -4; a1 = f(a0, clf.w, clf.b, 1)\n#         b0 = 4; b1 = f(b0, clf.w, clf.b, 1)\n#         pl.plot([a0,b0], [a1,b1], \"k--\")\n        \n#         # w.x + b = -1\n#         a0 = -4; a1 = f(a0, clf.w, clf.b, -1)\n#         b0 = 4; b1 = f(b0, clf.w, clf.b, -1)\n#         pl.plot([a0,b0], [a1,b1], \"k--\")\n        \n#         pl.axis(\"tight\")\n#         pl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_margin()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split the training dataset in train and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train_mat100 = scale(train_op_data)\n# \nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_mat100, labels, test_size=0.33, random_state=42)\n\nprint(X_train.shape,X_val.shape,y_train.shape, y_val.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hard margin svm with SGD from scratch"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_cost(W, X, Y):\n    # calculate hinge loss\n    N = X.shape[0]\n    distances = 1 - Y * (np.dot(X, W))\n    distances[distances < 0] = 0  # equivalent to max(0, distance)\n    hinge_loss = regularization_strength * (np.sum(distances) / N)\n\n    # calculate cost\n    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_cost_gradient(W, X_batch, Y_batch):\n    # if only one example is passed (eg. in case of SGD)\n    if type(Y_batch) == np.float64:\n        Y_batch = np.array([Y_batch])\n        X_batch = np.array([X_batch])  # gives multidimensional array\n\n    distance = 1 - (Y_batch * np.dot(X_batch, W))\n    dw = np.zeros(len(W))\n\n    for ind, d in enumerate(distance):\n        if max(0, d) == 0:\n            di = W\n        else:\n            di = W - (regularization_strength * Y_batch[ind] * X_batch[ind])\n        dw += di\n\n    dw = dw/len(Y_batch)  # average\n    return dw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sgd(features, outputs,learning_rate=0.01):\n    max_epochs = 5000\n    weights = np.zeros(features.shape[1])\n    nth = 0\n    prev_cost = float(\"inf\")\n    cost_threshold = 2  # in percent\n    # stochastic gradient descent\n    for epoch in range(1, max_epochs):\n        # shuffle to prevent repeating update cycles\n        X, Y = (features, outputs)\n        for ind, x in enumerate(X):\n            ascent = calculate_cost_gradient(weights, x, Y[ind])\n            weights = weights -(learning_rate * ascent)\n\n        # convergence check on 2^nth epoch\n        if epoch == max_epochs - 1:\n            cost = compute_cost(weights, features, outputs)\n            print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n            # stoppage criterion\n            if abs(prev_cost - cost) < cost_threshold * prev_cost:\n                return weights\n            prev_cost = cost\n            nth += 1\n    return weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def insert_intercept(train_op_data):\n    N=train_op_data.shape[0]\n   \n    a = np.ones((N,1))\n    \n    train_op_data=np.append(train_op_data,a, axis=1)\n    return train_op_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_op_data=insert_intercept(train_op_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(x_train,y_train, learning_rate=7):\n    W = sgd(X_train, y_train)\n    print(\"training finished.\")\n    return W\n\nw=train(X_train,y_train)\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validation(x_val,y_val,w):\n    y_test_predicted = np.array([])\n    for i in range(x_val.shape[0]):\n        yp = np.sign(np.dot(W, x_val[i])) #model\n        y_test_predicted = np.append(y_test_predicted, yp)\n    return accuracy_score(y_val, y_test_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation(X_val,y_val,w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef cross_validate(x_data,y_data,lr,lamda=0.2,epoch=10,k=5):\n    if len(x_data)%k != 0:\n        print('cant vsplit',len(x_data),' by ',k)\n        return\n    \n    x_data_splitted = np.vsplit(x_data,k)\n    y_data_splitted = np.vsplit(y_data,k)\n    \n    aggrigate_result = []\n    for i in range(len(x_data_splitted)):\n        train = []\n        test = []\n        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n        x_test = x_data_splitted[i]\n        y_test = y_data_splitted[i]\n        for item in items:\n            if len(train) == 0:\n                x_train = x_data_splitted[item]\n                y_train = y_data_splitted[item]\n            else:\n                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n                \n        \n          \n        w = train(x_train,y_train)\n        \n        result = validation(x_test,y_test,w)\n        aggrigate_result.append(result)\n        \n        value = sum(aggrigate_result)/len(aggrigate_result)\n    return aggrigate_result ,value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_mat100.shape,labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_validate(train_op_data,labels,lr=0.01,lamda=0.2,epoch=100,k=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# soft margin SVM form scratch"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import linalg\nimport cvxopt\nimport cvxopt.solvers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define a set of kernels"},{"metadata":{"trusted":true},"cell_type":"code","source":"class kernels:\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n\n    def polynomial_kernel(x, y, p=3):\n        return (1 + np.dot(x, y)) ** p\n\n    def gaussian_kernel(x, y, sigma=5.0):\n        return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}