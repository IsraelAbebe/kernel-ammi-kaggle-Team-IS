{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Files\n",
    "This data challenge contains one dataset of 2000 training sequences. The main files available are the following ones\n",
    "\n",
    "* Xtr.csv - the training sequences.\n",
    "* \n",
    "* Xte.csv - the test sequences.\n",
    "* \n",
    "* Ytr.csv - the sequence labels of the training sequences indicating bound (1) or not (0).\n",
    "* \n",
    "Each row of Xtr.csv represents a sequence. Xte.csv contains 1000 test sequences, for which you need to predict. Ytr.csv contains the labels corresponding to the training data, in the same format as a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvxopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from numpy import linalg\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "import sklearn\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xtr.csv\", sep=',',index_col=0)\n",
    "\n",
    "labels=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Ytr.csv\", sep=',',index_col=0)\n",
    "\n",
    "\n",
    "test_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xte.csv\",sep=',', index_col=0)\n",
    "\n",
    "\n",
    "#optional data \n",
    "\n",
    "train_op_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xtr_mat100.csv\", sep=' ',header=None).values\n",
    "test_op_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xte_mat100.csv\", sep=' ',header=None).values\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_intercept(train_op_data):\n",
    "#     N=train_op_data.shape[0]\n",
    "   \n",
    "#     a = np.ones((N,1))\n",
    "    \n",
    "#     train_op_data=np.append(train_op_data,a, axis=1)\n",
    "#     return train_op_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the labels from 0,1 to -1 ,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= np.where(labels==0,-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the training dataset in train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mat100 = train_op_data\n",
    "# \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_mat100, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape,X_val.shape,y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spectrom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_ = pd.concat([train_data , pd.DataFrame(labels)],axis=1)\n",
    "\n",
    "# def getKmers(sequence, size=6):\n",
    "#     return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "# train_data['words'] = train_data.seq.apply(lambda x: ' '.join(getKmers(x)))\n",
    "# test_data['words'] = test_data.seq.apply(lambda x: ' '.join(getKmers(x)))\n",
    "# train_data.head(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# data = pd.DataFrame(pd.concat([train_data.words,test_data.words],axis=0))\n",
    "\n",
    "# train_text = data.words.values\n",
    "\n",
    "# cv = CountVectorizer(ngram_range=(2,2),max_features=1500,min_df=10,binary=True)\n",
    "# X = cv.fit_transform(train_text)\n",
    "# X = X.todense()\n",
    "# X=np.array(X)\n",
    "# train=np.array(X)[:2000,:]\n",
    "# train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_mat100 =train\n",
    "# # \n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train_mat100, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# print(X_train.shape,X_val.shape,y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2.T)\n",
    "\n",
    "def polynomial_kernel(x, y, power):\n",
    "    return (1 + np.dot(x, y.T)) ** power\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=5.0):\n",
    "    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n",
    "\n",
    "def kernelFuncTrigo(x1, x2, i):\n",
    "   \n",
    "    sigma = 0.5\n",
    "    \n",
    "    kxx = 1 +(np.dot(sin(k*sigma*x1), sin(k*sigma*x2))  + np.dot(cos(k*sigma*x1), cos(k*sigma*x2))  for k in range(1, i+1))\n",
    "\n",
    "    return kxx\n",
    "\n",
    "def rbf_kernel(X1, X2, sigma=10):\n",
    "    '''\n",
    "    Returns the kernel matrix K(X1_i, X2_j): size (n1, n2)\n",
    "    where K is the RBF kernel with parameter sigma\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    X1: an (n1, p) matrix\n",
    "    X2: an (n2, p) matrix\n",
    "    sigma: float\n",
    "    '''\n",
    "    # For loop with rbf_kernel_element works but is slow in python\n",
    "    # Use matrix operations!\n",
    "    \n",
    "    X2_norm = np.sum(X2 ** 2)\n",
    "    X1_norm = np.sum(X1 ** 2)\n",
    "    gamma = 1 / (2 * sigma ** 2)\n",
    "    K = np.exp(- gamma * (X1_norm+ X2_norm- 2 * np.dot(X1, X2.T)))\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soft margin svm with SGD from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, lr,lamb, epoch):\n",
    "        self.lr=lr\n",
    "        self.lamb=lamb\n",
    "        self.epoch=epoch\n",
    "    def compute_cost(self,W, X, Y):\n",
    "        # calculate hinge loss\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        distances = 1 - Y * (np.dot(X, W))\n",
    "        distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "        hinge_loss = self.lamb * (np.sum(distances) / N)\n",
    "\n",
    "        # calculate cost\n",
    "        cost = (1 / 2) * (np.dot(W, W)) + hinge_loss\n",
    "        return cost\n",
    "    \n",
    "    def calculate_cost_gradient(self,W, X_batch, Y_batch):\n",
    "        # if only one example is passed (eg. in case of SGD)\n",
    "        if type(Y_batch) == np.float64:\n",
    "            Y_batch = np.array([Y_batch])\n",
    "            X_batch = np.array([X_batch])  # gives multidimensional array\n",
    "\n",
    "        distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
    "        dw = np.zeros(len(W))\n",
    "\n",
    "        for ind, d in enumerate(distance):\n",
    "            if max(0, d) == 0:\n",
    "                di = W\n",
    "            else:\n",
    "                di = W -(self.lamb* Y_batch[ind] * X_batch[ind])\n",
    "            dw += di\n",
    "\n",
    "        dw = dw/len(Y_batch)  # average\n",
    "        return dw\n",
    "    \n",
    "    def sgd(self,features, outputs):\n",
    "        \n",
    "        weights = np.zeros(features.shape[1])\n",
    "        nth = 0\n",
    "        prev_cost = float(\"inf\")\n",
    "        cost_threshold = 1e-3  # in percent\n",
    "        # stochastic gradient descent\n",
    "        for epoc in range(1, self.epoch):\n",
    "            # shuffle to prevent repeating update cycles\n",
    "            X, Y = (features, outputs)\n",
    "            for ind, x in enumerate(X):\n",
    "                ascent = self.calculate_cost_gradient(weights, x, Y[ind])\n",
    "                weights = weights +(self.lr * ascent)\n",
    "\n",
    "\n",
    "            cost = self.compute_cost(weights, features, outputs)\n",
    "#             print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n",
    "            # stoppage criterion\n",
    "#             print(cost)\n",
    "#             if abs(prev_cost - cost) < cost_threshold:\n",
    "#                 return weights\n",
    "            prev_cost = cost\n",
    "            nth += 1\n",
    "        return weights\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_func(self,x_train,y_train):\n",
    "        W = self.sgd(x_train, y_train)\n",
    "#         print(\"training finished.\")\n",
    "        return W\n",
    "\n",
    "    def validation(self,x_val,y_val,w):\n",
    "        y_test_predicted = np.array([])\n",
    "        for i in range(x_val.shape[0]):\n",
    "            yp = np.sign(np.dot(w, x_val[i])) #model\n",
    "            y_test_predicted = np.append(y_test_predicted, yp)\n",
    "        return accuracy_score(y_val, y_test_predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train_op_data=insert_intercept(train_op_data)\n",
    "# svm_sgd=SVM(lr=0.2,lamb=50,epoch=100)\n",
    "# w=svm_sgd.train_func(X_train,y_train)\n",
    "# svm_sgd.validation(X_val,y_val,w)\n",
    "\n",
    "                   \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    " \n",
    "        lr = trial.suggest_float('lr',  1e-3, 1)\n",
    "        \n",
    "        epoch=trial.suggest_int('epoch',  20, 200)\n",
    "        lamb=trial.suggest_int('lamb',  1e-5,20)\n",
    "        C=trial.suggest_int('C',  1,20)\n",
    "        sigma=trial.suggest_int('sigma',  1e-7,20)\n",
    "        svmm=SVM(lr=lr,lamb=lamb, epoch=epoch)\n",
    "        w=svm_sgd.train_func(X_train,y_train)\n",
    "        acc=svmm.validation(X_val,y_val,w)\n",
    "            \n",
    "\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "study.optimize(func=objective, n_trials=100,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate(x_data,y_data,model,lr,lamda=0.2,epoch=10,k=5):\n",
    "    if len(x_data)%k != 0:\n",
    "        print('cant vsplit',len(x_data),' by ',k)\n",
    "        return\n",
    "    \n",
    "    x_data_splitted = np.vsplit(x_data,k)\n",
    "    y_data_splitted = np.vsplit(y_data,k)\n",
    "    \n",
    "    aggrigate_result = []\n",
    "    \n",
    "    for i in range(len(x_data_splitted)):\n",
    "        train = []\n",
    "        test = []\n",
    "        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n",
    "        x_test = x_data_splitted[i]\n",
    "        y_test = y_data_splitted[i]\n",
    "        \n",
    "        for item in items:\n",
    "            if len(train) == 0:\n",
    "                x_train = x_data_splitted[item]\n",
    "                y_train = y_data_splitted[item]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n",
    "                \n",
    "                \n",
    "                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n",
    "                \n",
    "                \n",
    "       \n",
    "        w=model.train_func(x_train,y_train)\n",
    "       \n",
    "       \n",
    "        \n",
    "        result = model.validation(x_test,y_test,w)\n",
    "        aggrigate_result.append(result)\n",
    "         \n",
    "        \n",
    "        value = sum(aggrigate_result)/len(aggrigate_result)\n",
    "        \n",
    "        \n",
    "    return value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(train_op_data,labels,svm_sgd,lr=0.01,lamda=0.2,epoch=100,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search using optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM dual problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install qpsolvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qpsolvers import solve_qp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # soft margin SVM with kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a set of kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM dual problem with kerner and solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=polynomial_kernel, sigma=5,C=10):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.sigma=sigma\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                #print(X[i], X[j])\n",
    "                K[i,j] = self.kernel(X[i], X[j],self.sigma)\n",
    "        y = y_train.reshape(-1,1) * 1.\n",
    "        X_dash = y_train * X_train\n",
    "        H = np.dot(X_dash , X_dash.T) * 1\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "            \n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        a = np.ravel(solution['x'])\n",
    "\n",
    "        sv = a > 1e-6\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        \n",
    "        #print(len(a))\n",
    "        if len(a)!=0:\n",
    "            self.b /= len(self.a)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == linear_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def project(self, X):\n",
    "        if self.w is not None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * self.kernel(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svmm=SVM(kernel=rbf_kernel,sigma=4,C=10)\n",
    "svmm.fit(X_train,y_train)\n",
    "y_predict = svmm.predict(X_val)\n",
    "print(accuracy_score(y_val, y_predict) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    " \n",
    "        C = trial.suggest_float('C',  1, 10)\n",
    "        #gamma = trial.suggest_float('gamma',  0.1, 100)\n",
    "        #kernels=['polynomial_kernel','gaussian_kernel','rbf_kernel','linear_kernel']\n",
    "        sigma=trial.suggest_int('sigma',  1, 20)\n",
    "        svmm=SVM(kernel=rbf_kernel,sigma=sigma,C=C)\n",
    "        svmm.fit(X_train,y_train)\n",
    "        y_predict = svmm.predict(X_val)\n",
    "            \n",
    "\n",
    "        return accuracy_score(y_val, y_predict)  \n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "study.optimize(func=objective, n_trials=100,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "df.sort_values(by=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate(x_data,y_data,model, testData, kernelFunc, powerI, lambdaPara,epoch=10,k=5):\n",
    "    if len(x_data)%k != 0:\n",
    "        print('cant vsplit',len(x_data),' by ',k)\n",
    "        return\n",
    "    \n",
    "    x_data_splitted = np.vsplit(x_data,k)\n",
    "    y_data_splitted = np.vsplit(y_data,k)\n",
    "    \n",
    "    aggrigate_result = []\n",
    "    \n",
    "    for i in range(len(x_data_splitted)):\n",
    "        train = []\n",
    "        test = []\n",
    "        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n",
    "        x_test = x_data_splitted[i]\n",
    "        y_test = y_data_splitted[i]\n",
    "        \n",
    "        for item in items:\n",
    "            if len(train) == 0:\n",
    "                x_train = x_data_splitted[item]\n",
    "                y_train = y_data_splitted[item]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n",
    "                \n",
    "                \n",
    "                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n",
    "                \n",
    "                \n",
    "       \n",
    "        w=model.train_func(x_train,y_train,testData, kernelFunc, powerI, lambdaPara)\n",
    "       \n",
    "       \n",
    "        \n",
    "        result = model.validation(x_test,y_test,w)\n",
    "        aggrigate_result.append(result)\n",
    "         \n",
    "        \n",
    "        value = sum(aggrigate_result)/len(aggrigate_result)\n",
    "        \n",
    "        \n",
    "    return value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernel logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(ypred, ytrue):\n",
    "    e = (ypred != ytrue).mean()\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelMethodBase(object):\n",
    "    '''\n",
    "    Base class for kernel methods models\n",
    "    \n",
    "    Methods\n",
    "    ----\n",
    "    fit\n",
    "    predict\n",
    "    '''\n",
    "    kernels_ = {\n",
    "        'linear': linear_kernel,\n",
    "        'polynomial': polynomial_kernel,\n",
    "        'rbf': rbf_kernel,\n",
    "        'gaussian':gaussian_kernel\n",
    "    }\n",
    "    def __init__(self, kernel='linear', **kwargs):\n",
    "        self.kernel_name = kernel\n",
    "        self.kernel_function_ = self.kernels_[kernel]\n",
    "        self.kernel_parameters = self.get_kernel_parameters(**kwargs)\n",
    "        \n",
    "    def get_kernel_parameters(self, **kwargs):\n",
    "        params = {}\n",
    "        if self.kernel_name == 'rbf':\n",
    "            params['sigma'] = kwargs.get('sigma', None)\n",
    "        if self.kernel_name == 'polynomial':\n",
    "            params['power'] = kwargs.get('power', None)\n",
    "        return params\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        return self\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRidgeRegression(KernelMethodBase):\n",
    "    '''\n",
    "    Kernel Ridge Regression\n",
    "    '''\n",
    "    def __init__(self, lambd=0.1, **kwargs):\n",
    "        self.lambd = lambd\n",
    "        # Python 3: replace the following line by\n",
    "        # super().__init__(**kwargs)\n",
    "        super(KernelRidgeRegression, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, sample_weights=None):\n",
    "        n, p = X.shape\n",
    "        assert (n == len(y))\n",
    "    \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        if sample_weights is not None:\n",
    "            w_sqrt = np.sqrt(sample_weights)\n",
    "            self.X_train = self.X_train * w_sqrt\n",
    "            self.y_train = self.y_train * w_sqrt\n",
    "        \n",
    "        A = self.kernel_function_(X,X,**self.kernel_parameters)\n",
    "        A[np.diag_indices_from(A)] = np.add(A[np.diag_indices_from(A)],n*self.lambd)\n",
    "        # self.alpha = (K + n lambda I)^-1 y\n",
    "        self.alpha = np.linalg.solve(A , self.y_train)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        K_x = self.kernel_function_(X,self.X_train, **self.kernel_parameters)\n",
    "        return K_x.dot(self.alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.decision_function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(x_data,y_data,kernel=None,lambd=0.2,sigma=0.5,k=5,power=2):\n",
    "    if len(x_data)%k != 0:\n",
    "        print('cant vsplit',len(x_data),' by ',k)\n",
    "        return\n",
    "    \n",
    "    x_data_splitted = np.vsplit(x_data,k)\n",
    "    y_data_splitted = np.vsplit(y_data.reshape(-1,1),k)\n",
    "    \n",
    "    aggrigate_result = []\n",
    "    for i in range(len(x_data_splitted)):\n",
    "        train = []\n",
    "        test = []\n",
    "        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n",
    "        x_test = x_data_splitted[i]\n",
    "        y_test = y_data_splitted[i]\n",
    "        for item in items:\n",
    "            if len(train) == 0:\n",
    "                x_train = x_data_splitted[item]\n",
    "                y_train = y_data_splitted[item]\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n",
    "                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n",
    "            \n",
    "            \n",
    "        model = KernelRidgeRegression(\n",
    "                kernel=kernel,\n",
    "                lambd=lambd,\n",
    "                sigma=sigma,\n",
    "                power=power\n",
    "            ).fit(x_train, y_train)\n",
    "        result = sum(np.sign(model.predict(x_test))==y_test)/len(y_test)\n",
    "        aggrigate_result.append(result)\n",
    "        \n",
    "        value = sum(aggrigate_result)/len(aggrigate_result)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(X_train,y_train,kernel='polynomial', lambd=4.023839198201892e-06,k=5,sigma=4.,power=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lambd = trial.suggest_loguniform('lambd', 1e-6, 5)\n",
    "    sigma = trial.suggest_loguniform('sigma', 1e-6, 6)\n",
    "    k =  trial.suggest_categorical('k', [2,4,5,8,10])\n",
    "    power =  trial.suggest_int('power', 2,6)\n",
    "    kernel =  trial.suggest_categorical('kernel', ['linear','rbf','polynomial'])\n",
    "    \n",
    "    return cross_validate(X_train,y_train,kernel=kernel,lambd=lambd,k=4,sigma=sigma,power=power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate(X_train_mat100, y,lamda=0.01,k=4)\n",
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "df = study.optimize(func=objective, n_trials=500,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class KernelLogisticRegression(KernelMethodBase):\n",
    "    '''\n",
    "    Kernel Logistic Regression\n",
    "    '''\n",
    "    def __init__(self, lambd=0.1, **kwargs):\n",
    "        self.lambd = lambd\n",
    "        # Python 3: replace the following line by\n",
    "        # super().__init__(**kwargs)\n",
    "        super(KernelLogisticRegression, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, max_iter=100, tol=1e-5):\n",
    "        n, p = X.shape\n",
    "        assert (n == len(y))\n",
    "    \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        K = self.kernel_function_(X, X, **self.kernel_parameters)\n",
    "        \n",
    "        # IRLS\n",
    "        KRR = KernelRidgeRegression(\n",
    "            lambd=2*self.lambd,\n",
    "            kernel=self.kernel_name,\n",
    "            **self.kernel_parameters\n",
    "        )\n",
    "        # Initialize\n",
    "        alpha = np.zeros(n)\n",
    "        # Iterate until convergence or max iterations\n",
    "        for n_iter in range(max_iter):\n",
    "            alpha_old = alpha\n",
    "            m = K.dot(alpha_old)\n",
    "            w = sigmoid(m) * sigmoid(-m)\n",
    "            z = m + self.y_train / sigmoid(self.y_train * m)\n",
    "            alpha = KRR.fit(self.X_train, z, sample_weights=w).alpha\n",
    "            # Break condition (achieved convergence)\n",
    "            if np.sum((alpha-alpha_old)**2) < tol:\n",
    "                break\n",
    "\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha = alpha\n",
    "\n",
    "        return self\n",
    "            \n",
    "    def decision_function(self, X_test):\n",
    "        K_x = self.kernel_function_(X_test, self.X_train, **self.kernel_parameters)\n",
    "        # Probability of y=1 (between 0 and 1)\n",
    "        return np.sign(K_x.dot(self.alpha))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probas = self.decision_function(X)\n",
    "        predicted_classes = np.where(probas < 0.5, -1, 1)\n",
    "        return predicted_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "sigma = .4\n",
    "lambd = 1.\n",
    "fig_title = 'Logistic Regression, {} Kernel'.format(kernel)\n",
    "\n",
    "model = KernelLogisticRegression(lambd=lambd, kernel=kernel, sigma=sigma)\n",
    "y_pred = model.fit(X_train, y_train).predict(X_val)\n",
    "print('Test error: {:.2%}'.format(error(y_pred, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
