{"cells":[{"metadata":{},"cell_type":"markdown","source":"Principal Files\nThis data challenge contains one dataset of 2000 training sequences. The main files available are the following ones\n\n* Xtr.csv - the training sequences.\n* \n* Xte.csv - the test sequences.\n* \n* Ytr.csv - the sequence labels of the training sequences indicating bound (1) or not (0).\n* \nEach row of Xtr.csv represents a sequence. Xte.csv contains 1000 test sequences, for which you need to predict. Ytr.csv contains the labels corresponding to the training data, in the same format as a submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install cvxopt\n","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting cvxopt\n  Downloading cvxopt-1.2.5-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n\u001b[K     |████████████████████████████████| 11.6 MB 2.6 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: cvxopt\nSuccessfully installed cvxopt-1.2.5\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nimport optuna\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score, precision_score\nfrom numpy import linalg\nimport cvxopt\nimport cvxopt.solvers\nimport sklearn\n\nimport os\n","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":3,"outputs":[{"output_type":"stream","text":"/kaggle/input/kernel-methods-ammi-2020/Xtr.csv\n/kaggle/input/kernel-methods-ammi-2020/Xte.csv\n/kaggle/input/kernel-methods-ammi-2020/Ytr.csv\n/kaggle/input/kernel-methods-ammi-2020/Xte_mat100.csv\n/kaggle/input/kernel-methods-ammi-2020/Xtr_mat100.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xtr.csv\", sep=',',index_col=0)\n\nlabels=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Ytr.csv\", sep=',',index_col=0)\n\n\ntest_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xte.csv\",sep=',', index_col=0)\n\n\n#optional data \n\ntrain_op_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xtr_mat100.csv\", sep=' ',header=None).values\ntest_op_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xte_mat100.csv\", sep=' ',header=None).values\n\n\n\n","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Insert bias"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def insert_intercept(train_op_data):\n#     N=train_op_data.shape[0]\n   \n#     a = np.ones((N,1))\n    \n#     train_op_data=np.append(train_op_data,a, axis=1)\n#     return train_op_data","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Change the labels from 0,1 to -1 ,1"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels= np.where(labels==0,-1,1)\n","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split the training dataset in train and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_mat100 = train_op_data\n# \nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_mat100, labels, test_size=0.33, random_state=42)\n\nprint(X_train.shape,X_val.shape,y_train.shape, y_val.shape)","execution_count":7,"outputs":[{"output_type":"stream","text":"(1340, 100) (660, 100) (1340, 1) (660, 1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n\ndef test_data(X,y):\n    results = {}\n    X=scale(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    classifier = SVC(kernel = 'linear', C = 0.1, gamma = 0.01)\n    classifier.fit(X_train, y_train)\n    results['svm'] = classifier.score(X_test,y_test)\n\n    clf = LogisticRegression(random_state=0,penalty='l2').fit(X, y)\n    results['lr'] = clf.score(X_test,y_test)\n    \n    return results\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data(X_train_mat100,labels)\n","execution_count":9,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n","name":"stderr"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"{'svm': 0.58, 'lr': 0.65}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# spectrom data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef getKmers(sequence, size=3):\n    return [sequence[x:x+size] for x in range(len(sequence) - size + 1)]\ndef base2int(c):\n    return {'A':0,'C':1,'G':2,'T':3}.get(c,0)\n\ndef index(kmer):\n    base_idx = np.array([base2int(base) for base in kmer])\n    multiplier = 4** np.arange(len(kmer))\n    kmer_idx = multiplier.dot(base_idx)\n    return kmer_idx\n    \n    \ndef spectral_embedding(sequence,kmer_size=3):\n    kmers = getKmers(sequence,kmer_size)\n    kmer_idxs = [index(kmer) for kmer in kmers]\n    one_hot_vector = np.zeros(4**kmer_size)\n    \n    for kmer_idx in kmer_idxs:\n        one_hot_vector[kmer_idx] += 1\n    return one_hot_vector\n\ntrain_data['kmers'] = train_data.seq.apply(lambda x:list(spectral_embedding(x,kmer_size=3)))\n\n\nkmer_data = []\nfor i in train_data.seq.values:\n    kmer_data.append(spectral_embedding(i,kmer_size=6))\n    \nnp.array(kmer_data).shape\n","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(2000, 4096)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data(kmer_data,labels)\n","execution_count":11,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n","name":"stderr"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"{'svm': 0.6216666666666667, 'lr': 1.0}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_mat100 =train\n# # \n# X_train, X_val, y_train, y_val = train_test_split(\n#     X_train_mat100, labels, test_size=0.33, random_state=42)\n\n# print(X_train.shape,X_val.shape,y_train.shape, y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define kernels"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef linear_kernel(x1, x2):\n    return np.dot(x1, x2.T)\n\ndef polynomial_kernel(x, y, power):\n    return (1 + np.dot(x, y.T)) ** power\n\ndef gaussian_kernel(x, y, sigma=5.0):\n    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n\ndef kernelFuncTrigo(x1, x2, i):\n   \n    sigma = 0.5\n    \n    kxx = 1 +(np.dot(sin(k*sigma*x1), sin(k*sigma*x2))  + np.dot(cos(k*sigma*x1), cos(k*sigma*x2))  for k in range(1, i+1))\n\n    return kxx\n\ndef rbf_kernel(X1, X2, sigma=10):\n    '''\n    Returns the kernel matrix K(X1_i, X2_j): size (n1, n2)\n    where K is the RBF kernel with parameter sigma\n    \n    Input:\n    ------\n    X1: an (n1, p) matrix\n    X2: an (n2, p) matrix\n    sigma: float\n    '''\n    # For loop with rbf_kernel_element works but is slow in python\n    # Use matrix operations!\n    \n    X2_norm = np.sum(X2 ** 2)\n    X1_norm = np.sum(X1 ** 2)\n    gamma = 1 / (2 * sigma ** 2)\n    K = np.exp(- gamma * (X1_norm+ X2_norm- 2 * np.dot(X1, X2.T)))\n    return K","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# soft margin svm with SGD from scratch"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SVM:\n    def __init__(self, lr,lamb, epoch):\n        self.lr=lr\n        self.lamb=lamb\n        self.epoch=epoch\n    def compute_cost(self,W, X, Y):\n        # calculate hinge loss\n        N = X.shape[0]\n        \n        distances = 1 - Y * (np.dot(X, W))\n        distances[distances < 0] = 0  # equivalent to max(0, distance)\n        hinge_loss = self.lamb * (np.sum(distances) / N)\n\n        # calculate cost\n        cost = (1 / 2) * (np.dot(W, W)) + hinge_loss\n        return cost\n    \n    def calculate_cost_gradient(self,W, X_batch, Y_batch):\n        # if only one example is passed (eg. in case of SGD)\n        if type(Y_batch) == np.float64:\n            Y_batch = np.array([Y_batch])\n            X_batch = np.array([X_batch])  # gives multidimensional array\n\n        distance = 1 - (Y_batch * np.dot(X_batch, W))\n        dw = np.zeros(len(W))\n\n        for ind, d in enumerate(distance):\n            if max(0, d) == 0:\n                di = W\n            else:\n                di = W -(self.lamb* Y_batch[ind] * X_batch[ind])\n            dw += di\n\n        dw = dw/len(Y_batch)  # average\n        return dw\n    \n    def sgd(self,features, outputs):\n        \n        weights = np.zeros(features.shape[1])\n        nth = 0\n        prev_cost = float(\"inf\")\n        cost_threshold = 1e-3  # in percent\n        # stochastic gradient descent\n        for epoc in range(1, self.epoch):\n            # shuffle to prevent repeating update cycles\n            X, Y = (features, outputs)\n            for ind, x in enumerate(X):\n                ascent = self.calculate_cost_gradient(weights, x, Y[ind])\n                weights = weights +(self.lr * ascent)\n\n\n            cost = self.compute_cost(weights, features, outputs)\n#             print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n            # stoppage criterion\n#             print(cost)\n#             if abs(prev_cost - cost) < cost_threshold:\n#                 return weights\n            prev_cost = cost\n            nth += 1\n        return weights\n    \n    \n    \n    def train_func(self,x_train,y_train):\n        W = self.sgd(x_train, y_train)\n#         print(\"training finished.\")\n        return W\n\n    def validation(self,x_val,y_val,w):\n        y_test_predicted = np.array([])\n        for i in range(x_val.shape[0]):\n            yp = np.sign(np.dot(w, x_val[i])) #model\n            y_test_predicted = np.append(y_test_predicted, yp)\n        return accuracy_score(y_val, y_test_predicted)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_op_data=insert_intercept(train_op_data)\nsvm_sgd=SVM(lr=0.2,lamb=50,epoch=100)\nw=svm_sgd.train_func(np.array(kmer_data)[:1340,:],labels[:1340])\n#svm_sgd.validation(np.array(kmer_data)[1340:2000,:],labels[1340:],w)\n\n                   \n                   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n \n        lr = trial.suggest_float('lr',  1e-3, 1)\n        \n        epoch=trial.suggest_int('epoch',  20, 200)\n        lamb=trial.suggest_int('lamb',  1e-5,20)\n        C=trial.suggest_int('C',  1,20)\n        sigma=trial.suggest_int('sigma',  1e-7,20)\n        svmm=SVM(lr=lr,lamb=lamb, epoch=epoch)\n        w=svm_sgd.train_func(X_train,y_train)\n        acc=svmm.validation(X_val,y_val,w)\n            \n\n        return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna\n\nsampler = optuna.samplers.TPESampler()\nstudy = optuna.create_study(sampler=sampler, direction='maximize')\nstudy.optimize(func=objective, n_trials=100,show_progress_bar=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef cross_validate(x_data,y_data,model,lr,lamda=0.2,epoch=10,k=5):\n    if len(x_data)%k != 0:\n        print('cant vsplit',len(x_data),' by ',k)\n        return\n    \n    x_data_splitted = np.vsplit(x_data,k)\n    y_data_splitted = np.vsplit(y_data,k)\n    \n    aggrigate_result = []\n    \n    for i in range(len(x_data_splitted)):\n        train = []\n        test = []\n        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n        x_test = x_data_splitted[i]\n        y_test = y_data_splitted[i]\n        \n        for item in items:\n            if len(train) == 0:\n                x_train = x_data_splitted[item]\n                y_train = y_data_splitted[item]\n                \n                \n            else:\n                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n                \n                \n                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n                \n                \n       \n        w=model.train_func(x_train,y_train)\n       \n       \n        \n        result = model.validation(x_test,y_test,w)\n        aggrigate_result.append(result)\n         \n        \n        value = sum(aggrigate_result)/len(aggrigate_result)\n        \n        \n    return value ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_validate(train_op_data,labels,svm_sgd,lr=0.01,lamda=0.2,epoch=100,k=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter search using optuna"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cvxopt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM dual problem"},{"metadata":{},"cell_type":"markdown","source":"# # soft margin SVM with kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install cvxopt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM dual problem with kerner and solver"},{"metadata":{"trusted":true},"cell_type":"code","source":"# class SVM(object):\n\n#     def __init__(self, kernel=polynomial_kernel, sigma=5,C=10):\n#         self.kernel = kernel\n#         self.C = C\n#         self.sigma=sigma\n#         if self.C is not None: self.C = float(self.C)\n\n#     def fit(self, X, y):\n#         n_samples, n_features = X.shape\n\n#         K = np.zeros((n_samples, n_samples))\n#         for i in range(n_samples):\n#             for j in range(n_samples):\n#                 #print(X[i], X[j])\n#                 K[i,j] = self.kernel(X[i], X[j],self.sigma)\n#         y = y_train.reshape(-1,1) * 1.\n#         X_dash = y_train * X_train\n#         H = np.dot(X_dash , X_dash.T) * 1\n\n#         P = cvxopt.matrix(np.outer(y,y) * K)\n#         q = cvxopt.matrix(np.ones(n_samples) * -1)\n#         A = cvxopt.matrix(y, (1,n_samples))\n#         b = cvxopt.matrix(0.0)\n#         if self.C is None:\n#             G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n#             h = cvxopt.matrix(np.zeros(n_samples))\n#         else:\n#             tmp1 = np.diag(np.ones(n_samples) * -1)\n#             tmp2 = np.identity(n_samples)\n#             G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n#             tmp1 = np.zeros(n_samples)\n#             tmp2 = np.ones(n_samples) * self.C\n#             h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n            \n#         solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n#         a = np.ravel(solution['x'])\n\n#         sv = a > 1e-6\n#         ind = np.arange(len(a))[sv]\n#         self.a = a[sv]\n#         self.sv = X[sv]\n#         self.sv_y = y[sv]\n#         print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n\n#         self.b = 0\n#         for n in range(len(self.a)):\n#             self.b += self.sv_y[n]\n#             self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n        \n#         #print(len(a))\n#         if len(a)!=0:\n#             self.b /= len(self.a)\n\n#         # Weight vector\n#         if self.kernel == linear_kernel:\n#             self.w = np.zeros(n_features)\n#             for n in range(len(self.a)):\n#                 self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n#         else:\n#             self.w = None\n\n#     def project(self, X):\n#         if self.w is not None:\n#             return np.dot(X, self.w) + self.b\n#         else:\n#             y_predict = np.zeros(len(X))\n#             for i in range(len(X)):\n#                 s = 0\n#                 for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n#                     s += a * sv_y * self.kernel(X[i], sv)\n#                 y_predict[i] = s\n#             return y_predict + self.b\n\n#     def predict(self, X):\n#         return np.sign(self.project(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SVM(object):\n\n    def __init__(self, kernel=polynomial_kernel, C=0.2, power=2):\n        self.kernel = polynomial_kernel\n        self.C = C\n        self.power=power\n        if self.C is not None: self.C = float(self.C)\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # Gram matrix\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                K[i,j] = self.kernel(X[i], X[j],self.power)\n        y=y.reshape(-1,1) * 1.\n        #print(y)\n\n        P = cvxopt.matrix(np.outer(y,y) * K)\n        q = cvxopt.matrix(np.ones(n_samples) * -1)\n        A = cvxopt.matrix(y, (1,n_samples))\n        \n        b = cvxopt.matrix(0.0)\n\n        if self.C is None:\n            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n            h = cvxopt.matrix(np.zeros(n_samples))\n        else:\n            tmp1 = np.diag(np.ones(n_samples) * -1)\n            tmp2 = np.identity(n_samples)\n            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n            tmp1 = np.zeros(n_samples)\n            tmp2 = np.ones(n_samples) * self.C\n            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n\n        # solve QP problem\n        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n\n        # Lagrange multipliers\n        a = np.ravel(solution['x'])\n\n        # Support vectors have non zero lagrange multipliers\n        sv = a > 1e-10\n        ind = np.arange(len(a))[sv]\n        self.a = a[sv]\n        self.sv = X[sv]\n        self.sv_y = y[sv]\n        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n\n        # Intercept\n        self.b = 0\n        for n in range(len(self.a)):\n            self.b += self.sv_y[n]\n            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n        self.b /= len(self.a)\n\n        # Weight vector\n        if self.kernel == polynomial_kernel:\n            self.w = np.zeros(n_features)\n            for n in range(len(self.a)):\n                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n        else:\n            self.w = None\n\n    def project(self, X):\n        if self.w is not None:\n            return np.dot(X, self.w) + self.b\n        else:\n            y_predict = np.zeros(len(X))\n            for i in range(len(X)):\n                s = 0\n                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n                    s += a * sv_y * self.kernel(X[i], sv, self.power)\n                y_predict[i] = s\n            return y_predict + self.b\n\n    def predict(self, X):\n        return np.sign(self.project(X))\n","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsvmm=SVM(kernel=polynomial_kernel,C=0,power=5)\nsvmm.fit(X_train,y_train)\ny_predict = svmm.predict(X_val)\nprint(accuracy_score(y_val, y_predict) )\n","execution_count":52,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"unsupported operand type(s) for *: 'builtin_function_or_method' and 'float'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-3c6dc0695a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msvmm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolynomial_kernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msvmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-51-2c384e63d388>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'builtin_function_or_method' and 'float'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_predict","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n \n        C = trial.suggest_float('C',  1e-3, 10)\n        p = trial.suggest_float('p',  1e-4, 10)\n        #gamma = trial.suggest_float('gamma',  0.1, 100)\n        #kernels=['polynomial_kernel','gaussian_kernel','rbf_kernel','linear_kernel']\n        #sigma=trial.suggest_int('sigma',  1, 20)\n        svmm=SVM(kernel=polynomial_kernel,C=C, power=p)\n        svmm.fit(np.array(kmer_data)[:1340,:],labels[:1340])\n        y_predict = svmm.predict(np.array(kmer_data)[1340:,:])\n            \n\n        return accuracy_score(labels[1340:], y_predict)  \n\n        \n            ","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna\n\nsampler = optuna.samplers.TPESampler()\nstudy = optuna.create_study(sampler=sampler, direction='maximize')\nstudy.optimize(func=objective, n_trials=100,show_progress_bar=True)","execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf620b98d5844e05b36affc47193392d"}},"metadata":{}},{"output_type":"stream","text":"     pcost       dcost       gap    pres   dres\n 0:  1.9565e-04 -4.8348e+03  1e+04  4e-01  7e-15\n 1:  3.2129e-04 -2.7670e+02  3e+02  4e-03  5e-15\n 2:  3.2972e-04 -2.7707e+00  3e+00  4e-05  5e-15\n 3:  3.2778e-04 -2.9297e-02  4e-02  4e-07  5e-15\n 4:  2.0537e-04 -9.3570e-04  1e-03  8e-09  5e-15\n 5:  1.0700e-05 -9.0785e-05  1e-04  6e-11  7e-15\n 6: -1.4994e-05 -2.7135e-05  1e-05  2e-16  3e-15\n 7: -1.6273e-05 -1.6847e-05  6e-07  2e-16  1e-15\n 8: -1.6279e-05 -1.6293e-05  1e-08  2e-16  1e-15\nOptimal solution found.\n1336 support vectors out of 1340 points\n[I 2020-05-29 20:36:13,157] Finished trial#0 with value: 0.5060606060606061 with parameters: {'C': 3.6080955873539926, 'p': 3.7718754720631953}. Best is trial#0 with value: 0.5060606060606061.\n     pcost       dcost       gap    pres   dres\n 0:  2.7259e-10 -1.0856e+04  2e+04  2e-01  2e-14\n 1:  3.4353e-10 -2.3956e+02  4e+02  2e-03  1e-14\n 2:  3.4521e-10 -2.3959e+00  4e+00  2e-05  9e-15\n 3:  3.4522e-10 -2.3959e-02  4e-02  2e-07  9e-15\n 4:  3.4522e-10 -2.3959e-04  4e-04  2e-09  7e-15\n 5:  3.4521e-10 -2.3964e-06  4e-06  2e-11  8e-15\n 6:  3.4421e-10 -2.4389e-08  4e-08  2e-13  1e-14\nOptimal solution found.\n0 support vectors out of 1340 points\n[W 2020-05-29 20:36:32,067] Setting status of trial#1 as TrialState.FAIL because of the following error: ZeroDivisionError('division by zero')\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/optuna/study.py\", line 699, in _run_trial\n    result = func(trial)\n  File \"<ipython-input-38-b4c9111a4fec>\", line 9, in objective\n    svmm.fit(np.array(kmer_data)[:1340,:],labels[:1340])\n  File \"<ipython-input-35-813535605f73>\", line 55, in fit\n    self.b /= len(self.a)\nZeroDivisionError: division by zero\n\n","name":"stdout"},{"output_type":"error","ename":"ZeroDivisionError","evalue":"division by zero","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-193fc584ef75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 334\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                 )\n\u001b[1;32m    336\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    646\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             message = \"Setting status of trial#{} as {}. {}\".format(\n","\u001b[0;32m<ipython-input-38-b4c9111a4fec>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#sigma=trial.suggest_int('sigma',  1, 20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msvmm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolynomial_kernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msvmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1340\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1340\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1340\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-813535605f73>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msv_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msv_y\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Weight vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\ndf.sort_values(by=['value'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef cross_validate(x_data,y_data,model, testData, kernelFunc, powerI, lambdaPara,epoch=10,k=5):\n    if len(x_data)%k != 0:\n        print('cant vsplit',len(x_data),' by ',k)\n        return\n    \n    x_data_splitted = np.vsplit(x_data,k)\n    y_data_splitted = np.vsplit(y_data,k)\n    \n    aggrigate_result = []\n    \n    for i in range(len(x_data_splitted)):\n        train = []\n        test = []\n        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n        x_test = x_data_splitted[i]\n        y_test = y_data_splitted[i]\n        \n        for item in items:\n            if len(train) == 0:\n                x_train = x_data_splitted[item]\n                y_train = y_data_splitted[item]\n                \n                \n            else:\n                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n                \n                \n                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n                \n                \n       \n        w=model.train_func(x_train,y_train,testData, kernelFunc, powerI, lambdaPara)\n       \n       \n        \n        result = model.validation(x_test,y_test,w)\n        aggrigate_result.append(result)\n         \n        \n        value = sum(aggrigate_result)/len(aggrigate_result)\n        \n        \n    return value ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## kernel logistic regression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def error(ypred, ytrue):\n    e = (ypred != ytrue).mean()\n    return e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KernelMethodBase(object):\n    '''\n    Base class for kernel methods models\n    \n    Methods\n    ----\n    fit\n    predict\n    '''\n    kernels_ = {\n        'linear': linear_kernel,\n        'polynomial': polynomial_kernel,\n        'rbf': rbf_kernel,\n        'gaussian':gaussian_kernel\n    }\n    def __init__(self, kernel='linear', **kwargs):\n        self.kernel_name = kernel\n        self.kernel_function_ = self.kernels_[kernel]\n        self.kernel_parameters = self.get_kernel_parameters(**kwargs)\n        \n    def get_kernel_parameters(self, **kwargs):\n        params = {}\n        if self.kernel_name == 'rbf':\n            params['sigma'] = kwargs.get('sigma', None)\n        if self.kernel_name == 'polynomial':\n            params['power'] = kwargs.get('power', None)\n        return params\n\n    def fit(self, X, y, **kwargs):\n        return self\n        \n    def decision_function(self, X):\n        pass\n\n    def predict(self, X):\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KernelRidgeRegression(KernelMethodBase):\n    '''\n    Kernel Ridge Regression\n    '''\n    def __init__(self, lambd=0.1, **kwargs):\n        self.lambd = lambd\n        # Python 3: replace the following line by\n        # super().__init__(**kwargs)\n        super(KernelRidgeRegression, self).__init__(**kwargs)\n\n    def fit(self, X, y, sample_weights=None):\n        n, p = X.shape\n        assert (n == len(y))\n    \n        self.X_train = X\n        self.y_train = y\n        \n        if sample_weights is not None:\n            w_sqrt = np.sqrt(sample_weights)\n            self.X_train = self.X_train * w_sqrt\n            self.y_train = self.y_train * w_sqrt\n        \n        A = self.kernel_function_(X,X,**self.kernel_parameters)\n        A[np.diag_indices_from(A)] = np.add(A[np.diag_indices_from(A)],n*self.lambd)\n        # self.alpha = (K + n lambda I)^-1 y\n        self.alpha = np.linalg.solve(A , self.y_train)\n\n        return self\n    \n    def decision_function(self, X):\n        K_x = self.kernel_function_(X,self.X_train, **self.kernel_parameters)\n        return K_x.dot(self.alpha)\n    \n    def predict(self, X):\n        return self.decision_function(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_validate(x_data,y_data,kernel=None,lambd=0.2,sigma=0.5,k=5,power=2):\n    if len(x_data)%k != 0:\n        print('cant vsplit',len(x_data),' by ',k)\n        return\n    \n    x_data_splitted = np.vsplit(x_data,k)\n    y_data_splitted = np.vsplit(y_data.reshape(-1,1),k)\n    \n    aggrigate_result = []\n    for i in range(len(x_data_splitted)):\n        train = []\n        test = []\n        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n        x_test = x_data_splitted[i]\n        y_test = y_data_splitted[i]\n        for item in items:\n            if len(train) == 0:\n                x_train = x_data_splitted[item]\n                y_train = y_data_splitted[item]\n            else:\n                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n            \n            \n        model = KernelRidgeRegression(\n                kernel=kernel,\n                lambd=lambd,\n                sigma=sigma,\n                power=power\n            ).fit(x_train, y_train)\n        result = sum(np.sign(model.predict(x_test))==y_test)/len(y_test)\n        aggrigate_result.append(result)\n        \n        value = sum(aggrigate_result)/len(aggrigate_result)\n    return value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_validate(X_train,y_train,kernel='polynomial', lambd=4.023839198201892e-06,k=5,sigma=4.,power=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    lambd = trial.suggest_loguniform('lambd', 1e-6, 5)\n    sigma = trial.suggest_loguniform('sigma', 1e-6, 6)\n    k =  trial.suggest_categorical('k', [2,4,5,8,10])\n    power =  trial.suggest_int('power', 2,6)\n    kernel =  trial.suggest_categorical('kernel', ['linear','rbf','polynomial'])\n    \n    return cross_validate(X_train,y_train,kernel=kernel,lambd=lambd,k=4,sigma=sigma,power=power)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross_validate(X_train_mat100, y,lamda=0.01,k=4)\nimport optuna\n\nsampler = optuna.samplers.TPESampler()\nstudy = optuna.create_study(sampler=sampler, direction='maximize')\ndf = study.optimize(func=objective, n_trials=500,show_progress_bar=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass KernelLogisticRegression(KernelMethodBase):\n    '''\n    Kernel Logistic Regression\n    '''\n    def __init__(self, lambd=0.1, **kwargs):\n        self.lambd = lambd\n        # Python 3: replace the following line by\n        # super().__init__(**kwargs)\n        super(KernelLogisticRegression, self).__init__(**kwargs)\n\n    def fit(self, X, y, max_iter=100, tol=1e-5):\n        n, p = X.shape\n        assert (n == len(y))\n    \n        self.X_train = X\n        self.y_train = y\n        \n        K = self.kernel_function_(X, X, **self.kernel_parameters)\n        \n        # IRLS\n        KRR = KernelRidgeRegression(\n            lambd=2*self.lambd,\n            kernel=self.kernel_name,\n            **self.kernel_parameters\n        )\n        # Initialize\n        alpha = np.zeros(n)\n        # Iterate until convergence or max iterations\n        for n_iter in range(max_iter):\n            alpha_old = alpha\n            m = K.dot(alpha_old)\n            w = sigmoid(m) * sigmoid(-m)\n            z = m + self.y_train / sigmoid(self.y_train * m)\n            alpha = KRR.fit(self.X_train, z, sample_weights=w).alpha\n            # Break condition (achieved convergence)\n            if np.sum((alpha-alpha_old)**2) < tol:\n                break\n\n        self.n_iter = n_iter\n        self.alpha = alpha\n\n        return self\n            \n    def decision_function(self, X_test):\n        K_x = self.kernel_function_(X_test, self.X_train, **self.kernel_parameters)\n        # Probability of y=1 (between 0 and 1)\n        return np.sign(K_x.dot(self.alpha))\n\n    def predict(self, X):\n        probas = self.decision_function(X)\n        predicted_classes = np.where(probas < 0.5, -1, 1)\n        return predicted_classes ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel = 'rbf'\nsigma = .4\nlambd = 1.\nfig_title = 'Logistic Regression, {} Kernel'.format(kernel)\n\nmodel = KernelLogisticRegression(lambd=lambd, kernel=kernel, sigma=sigma)\ny_pred = model.fit(X_train, y_train).predict(X_val)\nprint('Test error: {:.2%}'.format(error(y_pred, y_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}