{"cells":[{"metadata":{},"cell_type":"markdown","source":"Principal Files\nThis data challenge contains one dataset of 2000 training sequences. The main files available are the following ones\n\n* Xtr.csv - the training sequences.\n* \n* Xte.csv - the test sequences.\n* \n* Ytr.csv - the sequence labels of the training sequences indicating bound (1) or not (0).\n* \nEach row of Xtr.csv represents a sequence. Xte.csv contains 1000 test sequences, for which you need to predict. Ytr.csv contains the labels corresponding to the training data, in the same format as a submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install cvxopt\n","execution_count":63,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: cvxopt in /opt/conda/lib/python3.7/site-packages (1.2.5)\nWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nimport optuna\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score, precision_score\nfrom numpy import linalg\nimport cvxopt\nimport cvxopt.solvers\nimport sklearn\n\nimport os\n","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":65,"outputs":[{"output_type":"stream","text":"/kaggle/input/kernel-methods-ammi-2020/Ytr.csv\n/kaggle/input/kernel-methods-ammi-2020/Xte.csv\n/kaggle/input/kernel-methods-ammi-2020/Xte_mat100.csv\n/kaggle/input/kernel-methods-ammi-2020/Xtr_mat100.csv\n/kaggle/input/kernel-methods-ammi-2020/Xtr.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xtr.csv\", sep=',',index_col=0)\n\nlabels=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Ytr.csv\", sep=',',index_col=0)\n\n\ntest_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xte.csv\",sep=',', index_col=0)\n\n\n#optional data \n\ntrain_op_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xtr_mat100.csv\", sep=' ',header=None).values\ntest_op_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xte_mat100.csv\", sep=' ',header=None).values\n\n\n\n","execution_count":66,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Insert bias"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def insert_intercept(train_op_data):\n#     N=train_op_data.shape[0]\n   \n#     a = np.ones((N,1))\n    \n#     train_op_data=np.append(train_op_data,a, axis=1)\n#     return train_op_data","execution_count":67,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Change the labels from 0,1 to -1 ,1"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels= np.where(labels==0,-1,1)\n","execution_count":68,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split the training dataset in train and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_mat100 = train_op_data\n# \nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_mat100, labels, test_size=0.33, random_state=42)\n\nprint(X_train.shape,X_val.shape,y_train.shape, y_val.shape)","execution_count":69,"outputs":[{"output_type":"stream","text":"(1340, 100) (660, 100) (1340, 1) (660, 1)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# spectrom data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef getKmers(sequence, size=3):\n    return [sequence[x:x+size] for x in range(len(sequence) - size + 1)]\ndef base2int(c):\n    return {'A':0,'C':1,'G':2,'T':3}.get(c,0)\n\ndef index(kmer):\n    base_idx = np.array([base2int(base) for base in kmer])\n    multiplier = 4** np.arange(len(kmer))\n    kmer_idx = multiplier.dot(base_idx)\n    return kmer_idx\n    \n    \ndef spectral_embedding(sequence,kmer_size=3):\n    kmers = getKmers(sequence,kmer_size)\n    kmer_idxs = [index(kmer) for kmer in kmers]\n    one_hot_vector = np.zeros(4**kmer_size)\n    \n    for kmer_idx in kmer_idxs:\n        one_hot_vector[kmer_idx] += 1\n    return one_hot_vector\n\ntrain_data['kmers'] = train_data.seq.apply(lambda x:list(spectral_embedding(x,kmer_size=3)))\n\n\nkmer_data = []\nfor i in train_data.seq.values:\n    kmer_data.append(spectral_embedding(i,kmer_size=6))\n    \nnp.array(kmer_data).shape\n","execution_count":72,"outputs":[{"output_type":"execute_result","execution_count":72,"data":{"text/plain":"(2000, 4096)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Define kernels"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef linear_kernel(x1, x2):\n    return np.dot(x1, x2.T)\n\ndef polynomial_kernel(x, y, power):\n    return (1 + np.dot(x, y.T)) ** power\n\ndef gaussian_kernel(x, y, sigma=5.0):\n    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n\ndef kernelFuncTrigo(x1, x2, i):\n   \n    sigma = 0.5\n    \n    kxx = 1 +(np.dot(sin(k*sigma*x1), sin(k*sigma*x2))  + np.dot(cos(k*sigma*x1), cos(k*sigma*x2))  for k in range(1, i+1))\n\n    return kxx\n\ndef rbf_kernel(X1, X2, sigma=10):\n    '''\n    Returns the kernel matrix K(X1_i, X2_j): size (n1, n2)\n    where K is the RBF kernel with parameter sigma\n    \n    Input:\n    ------\n    X1: an (n1, p) matrix\n    X2: an (n2, p) matrix\n    sigma: float\n    '''\n    # For loop with rbf_kernel_element works but is slow in python\n    # Use matrix operations!\n    \n    X2_norm = np.sum(X2 ** 2)\n    X1_norm = np.sum(X1 ** 2)\n    gamma = 1 / (2 * sigma ** 2)\n    K = np.exp(- gamma * (X1_norm+ X2_norm- 2 * np.dot(X1, X2.T)))\n    return K","execution_count":74,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# soft margin svm with SGD from scratch"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SVM:\n    def __init__(self, lr,lamb, epoch):\n        self.lr=lr\n        self.lamb=lamb\n        self.epoch=epoch\n    def compute_cost(self,W, X, Y):\n        # calculate hinge loss\n        N = X.shape[0]\n        \n        distances = 1 - Y * (np.dot(X, W))\n        distances[distances < 0] = 0  # equivalent to max(0, distance)\n        hinge_loss = self.lamb * (np.sum(distances) / N)\n\n        # calculate cost\n        cost = (1 / 2) * (np.dot(W, W)) + hinge_loss\n        return cost\n    \n    def calculate_cost_gradient(self,W, X_batch, Y_batch):\n        # if only one example is passed (eg. in case of SGD)\n        if type(Y_batch) == np.float64:\n            Y_batch = np.array([Y_batch])\n            X_batch = np.array([X_batch])  # gives multidimensional array\n\n        distance = 1 - (Y_batch * np.dot(X_batch, W))\n        dw = np.zeros(len(W))\n\n        for ind, d in enumerate(distance):\n            if max(0, d) == 0:\n                di = W\n            else:\n                di = W -(self.lamb* Y_batch[ind] * X_batch[ind])\n            dw += di\n\n        dw = dw/len(Y_batch)  # average\n        return dw\n    \n    def sgd(self,features, outputs):\n        \n        weights = np.zeros(features.shape[1])\n        nth = 0\n        prev_cost = float(\"inf\")\n        cost_threshold = 1e-3  # in percent\n        # stochastic gradient descent\n        for epoc in range(1, self.epoch):\n            # shuffle to prevent repeating update cycles\n            X, Y = (features, outputs)\n            for ind, x in enumerate(X):\n                ascent = self.calculate_cost_gradient(weights, x, Y[ind])\n                weights = weights +(self.lr * ascent)\n\n\n            cost = self.compute_cost(weights, features, outputs)\n#             print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n            # stoppage criterion\n#             print(cost)\n#             if abs(prev_cost - cost) < cost_threshold:\n#                 return weights\n            prev_cost = cost\n            nth += 1\n        return weights\n    \n    \n    \n    def train_func(self,x_train,y_train):\n        W = self.sgd(x_train, y_train)\n#         print(\"training finished.\")\n        return W\n\n    def validation(self,x_val,y_val,w):\n        y_test_predicted = np.array([])\n        for i in range(x_val.shape[0]):\n            yp = np.sign(np.dot(w, x_val[i])) #model\n            y_test_predicted = np.append(y_test_predicted, yp)\n        return accuracy_score(y_val, y_test_predicted)\n\n","execution_count":75,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_op_data=insert_intercept(train_op_data)\nsvm_sgd=SVM(lr=0.2,lamb=50,epoch=100)\nw=svm_sgd.train_func(np.array(kmer_data)[:1340,:],labels[:1340])\n#svm_sgd.validation(np.array(kmer_data)[1340:2000,:],labels[1340:],w)\n\n                   \n                   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n \n        lr = trial.suggest_float('lr',  1e-3, 1)\n        \n        epoch=trial.suggest_int('epoch',  20, 200)\n        lamb=trial.suggest_int('lamb',  1e-5,20)\n        C=trial.suggest_int('C',  1,20)\n        sigma=trial.suggest_int('sigma',  1e-7,20)\n        svmm=SVM(lr=lr,lamb=lamb, epoch=epoch)\n        w=svm_sgd.train_func(X_train,y_train)\n        acc=svmm.validation(X_val,y_val,w)\n            \n\n        return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna\n\nsampler = optuna.samplers.TPESampler()\nstudy = optuna.create_study(sampler=sampler, direction='maximize')\nstudy.optimize(func=objective, n_trials=100,show_progress_bar=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef cross_validate(x_data,y_data,model,lr,lamda=0.2,epoch=10,k=5):\n    if len(x_data)%k != 0:\n        print('cant vsplit',len(x_data),' by ',k)\n        return\n    \n    x_data_splitted = np.vsplit(x_data,k)\n    y_data_splitted = np.vsplit(y_data,k)\n    \n    aggrigate_result = []\n    \n    for i in range(len(x_data_splitted)):\n        train = []\n        test = []\n        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n        x_test = x_data_splitted[i]\n        y_test = y_data_splitted[i]\n        \n        for item in items:\n            if len(train) == 0:\n                x_train = x_data_splitted[item]\n                y_train = y_data_splitted[item]\n                \n                \n            else:\n                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n                \n                \n                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n                \n                \n       \n        w=model.train_func(x_train,y_train)\n       \n       \n        \n        result = model.validation(x_test,y_test,w)\n        aggrigate_result.append(result)\n         \n        \n        value = sum(aggrigate_result)/len(aggrigate_result)\n        \n        \n    return value ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SVM(object):\n\n    def __init__(self, kernel=polynomial_kernel, C=0.2, power=2):\n        self.kernel = polynomial_kernel\n        self.C = C\n        self.power=power\n        if self.C is not None: self.C = float(self.C)\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # Gram matrix\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                K[i,j] = self.kernel(X[i], X[j],self.power)\n        #y=y.reshape(-1,1) * 1.\n        #print(y)\n\n        P = cvxopt.matrix(np.outer(y,y) * K)\n        q = cvxopt.matrix(np.ones(n_samples) * -1)\n        \n        A = y.reshape(1,n_samples)\n        A=A.astype('float')\n        A=cvxopt.matrix(A)\n        \n        \n        b = cvxopt.matrix(0.0)\n\n        if self.C is None:\n            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n            h = cvxopt.matrix(np.zeros(n_samples))\n        else:\n            tmp1 = np.diag(np.ones(n_samples) * -1)\n            tmp2 = np.identity(n_samples)\n            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n            tmp1 = np.zeros(n_samples)\n            tmp2 = np.ones(n_samples) * self.C\n            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n\n        # solve QP problem\n        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n\n        # Lagrange multipliers\n        a = np.ravel(solution['x'])\n\n        # Support vectors have non zero lagrange multipliers\n        sv = a > 1e-12\n        ind = np.arange(len(a))[sv]\n        self.a = a[sv]\n        self.sv = X[sv]\n        self.sv_y = y[sv]\n        print(self.sv_y)\n        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n\n        # Intercept\n        self.b = 0\n        for n in range(len(self.a)):\n            self.b =self.b+ self.sv_y[n]\n            self.b =self.b- np.sum(self.a * self.sv_y * K[ind[n],sv])\n        self.b /= len(self.a)\n\n        # Weight vector\n        if self.kernel == polynomial_kernel:\n            self.w = np.zeros(n_features)\n            for n in range(len(self.a)):\n                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n        else:\n            self.w = None\n\n    def project(self, X):\n        if self.w is not None:\n            return np.dot(X, self.w) + self.b\n        else:\n            y_predict = np.zeros(len(X))\n            for i in range(len(X)):\n                s = 0\n                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n                    s += a * sv_y * self.kernel(X[i], sv, self.power)\n                y_predict[i] = s\n            return y_predict + self.b\n\n    def predict(self, X):\n        return np.sign(self.project(X))\n","execution_count":96,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsvmm=SVM(kernel=polynomial_kernel,C=0,power=5)\nsvmm.fit(X_train,y_train)\ny_predict = svmm.predict(X_val)\nprint(accuracy_score(y_val, y_predict) )\n","execution_count":97,"outputs":[{"output_type":"stream","text":"     pcost       dcost       gap    pres   dres\n 0: -6.1643e+02 -1.1816e+01  7e+03  9e+01  2e-14\n 1: -4.1629e+01 -5.5610e-01  4e+02  5e+00  1e-14\n 2: -6.3957e-01 -3.2151e-04  6e+00  8e-02  2e-15\n 3: -6.4041e-03 -3.2274e-08  6e-02  8e-04  2e-15\n 4: -6.4041e-05 -3.2274e-12  6e-04  8e-06  2e-15\n 5: -6.4041e-07 -3.2274e-16  6e-06  8e-08  2e-15\n 6: -6.4041e-09 -3.2267e-20  6e-08  8e-10  2e-15\nOptimal solution found.\n[[ 1]\n [-1]\n [-1]\n ...\n [-1]\n [ 1]\n [ 1]]\n1263 support vectors out of 1340 points\n0.4712121212121212\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n \n        C = trial.suggest_float('C',  1e-3, 10)\n        p = trial.suggest_float('p',  1e-4, 10)\n        #gamma = trial.suggest_float('gamma',  0.1, 100)\n        #kernels=['polynomial_kernel','gaussian_kernel','rbf_kernel','linear_kernel']\n        #sigma=trial.suggest_int('sigma',  1, 20)\n        svmm=SVM(kernel=polynomial_kernel,C=C, power=p)\n        svmm.fit(np.array(kmer_data)[:1340,:],labels[:1340])\n        y_predict = svmm.predict(np.array(kmer_data)[1340:,:])\n            \n\n        return accuracy_score(labels[1340:], y_predict)  \n\n        \n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna\n\nsampler = optuna.samplers.TPESampler()\nstudy = optuna.create_study(sampler=sampler, direction='maximize')\nstudy.optimize(func=objective, n_trials=100,show_progress_bar=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\ndf.sort_values(by=['value'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef cross_validate(x_data,y_data,model, testData, kernelFunc, powerI, lambdaPara,epoch=10,k=5):\n    if len(x_data)%k != 0:\n        print('cant vsplit',len(x_data),' by ',k)\n        return\n    \n    x_data_splitted = np.vsplit(x_data,k)\n    y_data_splitted = np.vsplit(y_data,k)\n    \n    aggrigate_result = []\n    \n    for i in range(len(x_data_splitted)):\n        train = []\n        test = []\n        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n        x_test = x_data_splitted[i]\n        y_test = y_data_splitted[i]\n        \n        for item in items:\n            if len(train) == 0:\n                x_train = x_data_splitted[item]\n                y_train = y_data_splitted[item]\n                \n                \n            else:\n                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n                \n                \n                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n                \n                \n       \n        w=model.train_func(x_train,y_train,testData, kernelFunc, powerI, lambdaPara)\n       \n       \n        \n        result = model.validation(x_test,y_test,w)\n        aggrigate_result.append(result)\n         \n        \n        value = sum(aggrigate_result)/len(aggrigate_result)\n        \n        \n    return value ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## kernel logistic regression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def error(ypred, ytrue):\n    e = (ypred != ytrue).mean()\n    return e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KernelMethodBase(object):\n    '''\n    Base class for kernel methods models\n    \n    Methods\n    ----\n    fit\n    predict\n    '''\n    kernels_ = {\n        'linear': linear_kernel,\n        'polynomial': polynomial_kernel,\n        'rbf': rbf_kernel,\n        'gaussian':gaussian_kernel\n    }\n    def __init__(self, kernel='linear', **kwargs):\n        self.kernel_name = kernel\n        self.kernel_function_ = self.kernels_[kernel]\n        self.kernel_parameters = self.get_kernel_parameters(**kwargs)\n        \n    def get_kernel_parameters(self, **kwargs):\n        params = {}\n        if self.kernel_name == 'rbf':\n            params['sigma'] = kwargs.get('sigma', None)\n        if self.kernel_name == 'polynomial':\n            params['power'] = kwargs.get('power', None)\n        return params\n\n    def fit(self, X, y, **kwargs):\n        return self\n        \n    def decision_function(self, X):\n        pass\n\n    def predict(self, X):\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KernelRidgeRegression(KernelMethodBase):\n    '''\n    Kernel Ridge Regression\n    '''\n    def __init__(self, lambd=0.1, **kwargs):\n        self.lambd = lambd\n        # Python 3: replace the following line by\n        # super().__init__(**kwargs)\n        super(KernelRidgeRegression, self).__init__(**kwargs)\n\n    def fit(self, X, y, sample_weights=None):\n        n, p = X.shape\n        assert (n == len(y))\n    \n        self.X_train = X\n        self.y_train = y\n        \n        if sample_weights is not None:\n            w_sqrt = np.sqrt(sample_weights)\n            self.X_train = self.X_train * w_sqrt\n            self.y_train = self.y_train * w_sqrt\n        \n        A = self.kernel_function_(X,X,**self.kernel_parameters)\n        A[np.diag_indices_from(A)] = np.add(A[np.diag_indices_from(A)],n*self.lambd)\n        # self.alpha = (K + n lambda I)^-1 y\n        self.alpha = np.linalg.solve(A , self.y_train)\n\n        return self\n    \n    def decision_function(self, X):\n        K_x = self.kernel_function_(X,self.X_train, **self.kernel_parameters)\n        return K_x.dot(self.alpha)\n    \n    def predict(self, X):\n        return self.decision_function(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_validate(x_data,y_data,kernel=None,lambd=0.2,sigma=0.5,k=5,power=2):\n    if len(x_data)%k != 0:\n        print('cant vsplit',len(x_data),' by ',k)\n        return\n    \n    x_data_splitted = np.vsplit(x_data,k)\n    y_data_splitted = np.vsplit(y_data.reshape(-1,1),k)\n    \n    aggrigate_result = []\n    for i in range(len(x_data_splitted)):\n        train = []\n        test = []\n        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n        x_test = x_data_splitted[i]\n        y_test = y_data_splitted[i]\n        for item in items:\n            if len(train) == 0:\n                x_train = x_data_splitted[item]\n                y_train = y_data_splitted[item]\n            else:\n                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n            \n            \n        model = KernelRidgeRegression(\n                kernel=kernel,\n                lambd=lambd,\n                sigma=sigma,\n                power=power\n            ).fit(x_train, y_train)\n        result = sum(np.sign(model.predict(x_test))==y_test)/len(y_test)\n        aggrigate_result.append(result)\n        \n        value = sum(aggrigate_result)/len(aggrigate_result)\n    return value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_validate(X_train,y_train,kernel='polynomial', lambd=4.023839198201892e-06,k=5,sigma=4.,power=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    lambd = trial.suggest_loguniform('lambd', 1e-6, 5)\n    sigma = trial.suggest_loguniform('sigma', 1e-6, 6)\n    k =  trial.suggest_categorical('k', [2,4,5,8,10])\n    power =  trial.suggest_int('power', 2,6)\n    kernel =  trial.suggest_categorical('kernel', ['linear','rbf','polynomial'])\n    \n    return cross_validate(X_train,y_train,kernel=kernel,lambd=lambd,k=4,sigma=sigma,power=power)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross_validate(X_train_mat100, y,lamda=0.01,k=4)\nimport optuna\n\nsampler = optuna.samplers.TPESampler()\nstudy = optuna.create_study(sampler=sampler, direction='maximize')\ndf = study.optimize(func=objective, n_trials=500,show_progress_bar=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass KernelLogisticRegression(KernelMethodBase):\n    '''\n    Kernel Logistic Regression\n    '''\n    def __init__(self, lambd=0.1, **kwargs):\n        self.lambd = lambd\n        # Python 3: replace the following line by\n        # super().__init__(**kwargs)\n        super(KernelLogisticRegression, self).__init__(**kwargs)\n\n    def fit(self, X, y, max_iter=100, tol=1e-5):\n        n, p = X.shape\n        assert (n == len(y))\n    \n        self.X_train = X\n        self.y_train = y\n        \n        K = self.kernel_function_(X, X, **self.kernel_parameters)\n        \n        # IRLS\n        KRR = KernelRidgeRegression(\n            lambd=2*self.lambd,\n            kernel=self.kernel_name,\n            **self.kernel_parameters\n        )\n        # Initialize\n        alpha = np.zeros(n)\n        # Iterate until convergence or max iterations\n        for n_iter in range(max_iter):\n            alpha_old = alpha\n            m = K.dot(alpha_old)\n            w = sigmoid(m) * sigmoid(-m)\n            z = m + self.y_train / sigmoid(self.y_train * m)\n            alpha = KRR.fit(self.X_train, z, sample_weights=w).alpha\n            # Break condition (achieved convergence)\n            if np.sum((alpha-alpha_old)**2) < tol:\n                break\n\n        self.n_iter = n_iter\n        self.alpha = alpha\n\n        return self\n            \n    def decision_function(self, X_test):\n        K_x = self.kernel_function_(X_test, self.X_train, **self.kernel_parameters)\n        # Probability of y=1 (between 0 and 1)\n        return np.sign(K_x.dot(self.alpha))\n\n    def predict(self, X):\n        probas = self.decision_function(X)\n        predicted_classes = np.where(probas < 0.5, -1, 1)\n        return predicted_classes ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel = 'rbf'\nsigma = .4\nlambd = 1.\nfig_title = 'Logistic Regression, {} Kernel'.format(kernel)\n\nmodel = KernelLogisticRegression(lambd=lambd, kernel=kernel, sigma=sigma)\ny_pred = model.fit(X_train, y_train).predict(X_val)\nprint('Test error: {:.2%}'.format(error(y_pred, y_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}