{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting whether a DNA sequence region is binding site to a specifictranscription factor.\n",
    "\n",
    "\n",
    "\n",
    "In this Kaggle Competition we were working on binary classification task of predicting whether a DNA sequence region is binding site to a specifictranscription factor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## [Optuna](https://optuna.org/) \n",
    "- A hyperparameter optimization framework\n",
    "\n",
    "## [cvxopt](https://cvxopt.org/) \n",
    "- A convex optimization package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvxopt -q\n",
    "!pip install optuna -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import os\n",
    "import optuna\n",
    "import random\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "import sklearn\n",
    "\n",
    "cvxopt.solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ = pd.read_csv('data/Xte.csv',sep=',',index_col=0)\n",
    "X_train_ = pd.read_csv('data/Xtr.csv',sep=',',index_col=0)\n",
    "\n",
    "X_test_mat100 = pd.read_csv('data/Xte_mat100.csv',sep=' ',header=None).values\n",
    "X_train_mat100 = pd.read_csv('data/Xtr_mat100.csv',sep=' ',header=None).values\n",
    "\n",
    "y = pd.read_csv('data/Ytr.csv',sep=',',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuction will help us get the label \n",
    "\n",
    "- Label will be  0 or 1 if we use type=0\n",
    "- label will be -1 or 1 if we use type=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_label(type=0):\n",
    "    y = pd.read_csv('data/Ytr.csv',sep=',',index_col=0)\n",
    "    if type == 0:\n",
    "        y = y.Bound.values\n",
    "        return y\n",
    "    else:\n",
    "        y['Bound'] = y.Bound.apply(lambda x: -1 if x == 0 else 1)\n",
    "        y = y.Bound.values\n",
    "        return y\n",
    "    \n",
    "get_label(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function will return train-val split data using sklearns built in train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(X,y,p):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=p, random_state=42)\n",
    "    print(X_train.shape,X_test.shape,y_train.shape, y_test.shape)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>GAGGGGCTGGGGAGGGGGCTGGCCCAGAGGCACCAGACTCTGCAGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CGGCCTGGGGGCCACATGTGAGTGCTTACCTGTGTGGGGATGAGGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>GACAACGCCGCTGTCAGCCGCCTTCGACTCACCTGGGAGGTGATGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>GCCTCCCTTGGCACCACGGGAGACCAGTTTTGGAGGGGCGGGGCTG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GCACTACTACACCCATTGCTGTAATAGTAAGTGCCGGTGCCTTCAC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  seq\n",
       "Id                                                   \n",
       "0   GAGGGGCTGGGGAGGGGGCTGGCCCAGAGGCACCAGACTCTGCAGA...\n",
       "1   CGGCCTGGGGGCCACATGTGAGTGCTTACCTGTGTGGGGATGAGGG...\n",
       "2   GACAACGCCGCTGTCAGCCGCCTTCGACTCACCTGGGAGGTGATGA...\n",
       "3   GCCTCCCTTGGCACCACGGGAGACCAGTTTTGGAGGGGCGGGGCTG...\n",
       "4   GCACTACTACACCCATTGCTGTAATAGTAAGTGCCGGTGCCTTCAC..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE DATA \n",
    "X_train_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5e678609d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THE DATA IS PROPORTIONAL DATA \n",
    "y['Bound'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5e67c3a190>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ADDITIONALLY ALL DATA CONTAINS 101 LENGTH SEQUENCES SO THERE WILL BE NO NEED OF PADDING \n",
    "X_train_['Count'] = X_train_.seq.apply(lambda x:len(x))\n",
    "X_train_['Count'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Related Experiments\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence =  GAGGGGCTGGGGAGGGGGCTGGCCCAGAGGCACCAGACTCTGCAGAACCACCCAGGCATTGTGGGGCTGCCCTGCCACCTGCTGGCCGCTCCTGGTGGCAG\n",
      "scaled version of One Hot Representation = [0.01086957 0.01086957 0.01086957 0.0326087  0.01086957 0.0326087\n",
      " 0.01086957 0.         0.         0.01086957 0.0326087  0.\n",
      " 0.         0.01086957 0.02173913 0.01086957 0.01086957 0.01086957\n",
      " 0.         0.0326087  0.02173913 0.         0.         0.01086957\n",
      " 0.01086957 0.01086957 0.         0.02173913 0.         0.01086957\n",
      " 0.01086957 0.04347826 0.         0.01086957 0.01086957 0.01086957\n",
      " 0.         0.01086957 0.04347826 0.01086957 0.02173913 0.02173913\n",
      " 0.         0.         0.01086957 0.02173913 0.02173913 0.\n",
      " 0.         0.         0.         0.01086957 0.         0.0326087\n",
      " 0.01086957 0.         0.01086957 0.         0.01086957 0.02173913\n",
      " 0.         0.01086957 0.01086957 0.02173913 0.01086957 0.\n",
      " 0.01086957 0.01086957 0.01086957 0.         0.         0.\n",
      " 0.         0.         0.01086957 0.01086957 0.         0.02173913\n",
      " 0.01086957 0.         0.         0.         0.         0.\n",
      " 0.04347826 0.01086957 0.01086957 0.01086957 0.01086957 0.02173913\n",
      " 0.02173913 0.         0.         0.         0.01086957 0.\n",
      " 0.         0.02173913 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('sequence = ', X_train_.seq.values[0])\n",
    "print('scaled version of One Hot Representation =',X_train_mat100[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal encoding DNA\n",
    "-------------------------------------------------------\n",
    "\n",
    "we assign \n",
    "\n",
    "    A - 0.25\n",
    "    C - 0.50\n",
    "    G - 0.71\n",
    "    T - 1.0\n",
    "    \n",
    "    \n",
    "and convert data to matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.array(['A','C','G','T']))\n",
    "\n",
    "def ordinal_encoder(sequence):\n",
    "    integer_encoded = label_encoder.transform(np.array(list(sequence)))\n",
    "    float_encoded = integer_encoded.astype(float)\n",
    "    float_encoded[float_encoded == 0] = 0.25\n",
    "    float_encoded[float_encoded == 1] = 0.50\n",
    "    float_encoded[float_encoded == 2] = 0.75\n",
    "    float_encoded[float_encoded == 3] = 1.00 \n",
    "    return float_encoded\n",
    "\n",
    "def get_ordinal_data(x_train):\n",
    "    X_list = []\n",
    "    for i in x_train:\n",
    "        X_list.append(ordinal_encoder(i))\n",
    "    return np.array(X_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence =  GAGGGGCTGGGGAGGGGGCTGGCCCAGAGGCACCAGACTCTGCAGAACCACCCAGGCATTGTGGGGCTGCCCTGCCACCTGCTGGCCGCTCCTGGTGGCAG\n",
      "scaled version of One Hot Representation = [0.75 0.25 0.75 0.75 0.75 0.75 0.5  1.   0.75 0.75 0.75 0.75 0.25 0.75\n",
      " 0.75 0.75 0.75 0.75 0.5  1.   0.75 0.75 0.5  0.5  0.5  0.25 0.75 0.25\n",
      " 0.75 0.75 0.5  0.25 0.5  0.5  0.25 0.75 0.25 0.5  1.   0.5  1.   0.75\n",
      " 0.5  0.25 0.75 0.25 0.25 0.5  0.5  0.25 0.5  0.5  0.5  0.25 0.75 0.75\n",
      " 0.5  0.25 1.   1.   0.75 1.   0.75 0.75 0.75 0.75 0.5  1.   0.75 0.5\n",
      " 0.5  0.5  1.   0.75 0.5  0.5  0.25 0.5  0.5  1.   0.75 0.5  1.   0.75\n",
      " 0.75 0.5  0.5  0.75 0.5  1.   0.5  0.5  1.   0.75 0.75 1.   0.75 0.75\n",
      " 0.5  0.25 0.75]\n"
     ]
    }
   ],
   "source": [
    "print('sequence = ', X_train_.seq.values[0])\n",
    "print('scaled version of One Hot Representation =',get_ordinal_data(X_train_.seq.values)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-mer counting/Spectral Embedding\n",
    "--------------------------------------------------------\n",
    "\n",
    "\n",
    "K-mer of size 3 \n",
    "\n",
    "    'ACAAT' = ['aca', 'caa', 'aat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKmers(sequence, size=3):\n",
    "    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base2int(c):\n",
    "    return {'a':0,'c':1,'g':2,'t':3}.get(c,0)\n",
    "\n",
    "def index(kmer):\n",
    "    base_idx = np.array([base2int(base) for base in kmer])\n",
    "    multiplier = 4** np.arange(len(kmer))\n",
    "    kmer_idx = multiplier.dot(base_idx)\n",
    "    return kmer_idx\n",
    "\n",
    "def spectral_embedding(sequence,kmer_size=3):\n",
    "    kmers = getKmers(sequence,kmer_size)\n",
    "    kmer_idxs = [index(kmer) for kmer in kmers]\n",
    "    one_hot_vector = np.zeros(4**kmer_size)\n",
    "    \n",
    "    for kmer_idx in kmer_idxs:\n",
    "        one_hot_vector[kmer_idx] += 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def get_data(kmer_size):\n",
    "    data = pd.DataFrame(pd.concat([X_train_.seq,X_test_.seq],axis=0))\n",
    "    train_text = data.seq.values\n",
    "    kmer_data = []\n",
    "    for i in train_text:\n",
    "        kmer_data.append(spectral_embedding(i,kmer_size=kmer_size))\n",
    "\n",
    "    return np.array(kmer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer / TfidfTransformer\n",
    "-------------------------------------------------------\n",
    "\n",
    "\n",
    "#### Count Vectorizer \n",
    "    \n",
    "   - ngram_range = 2,analyzer='char'\n",
    "    \n",
    "    'ACAAATTTGGGGAAA' - [4, 1, 1, 1, 1, 3, 1, 2]\n",
    "    \n",
    " \n",
    "#### TfidfVectorizer \n",
    "\n",
    "   - ngram_range = 2,analyzer='char'\n",
    "\n",
    "    'ACAAATTTGGGGAAA' - [0.68599434, 0.17149859, 0.17149859, 0.17149859, 0.17149859,\n",
    "        0.51449576, 0.17149859, 0.34299717]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_grams(data,n=6):\n",
    "    cv = CountVectorizer(analyzer='char',ngram_range=(n,n))\n",
    "    X = cv.fit_transform(data).toarray()\n",
    "    return X\n",
    "\n",
    "def get_tf_idf_grams(data,n=6):\n",
    "    cv = TfidfVectorizer(analyzer='char',ngram_range=(n,n))\n",
    "    X = cv.fit_transform(data).toarray()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Related Experiments\n",
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started this experiment with a simple logistic regression with a batch gradient descent. \n",
    "This algorithm was train and validated with the  data processings we did above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression \n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticregression():\n",
    "    def __init__(self,train_data,train_labels,lamda=0.2,lr=0.01,decay=10,batch_size=64,epoch=10,print_every = 10):\n",
    "        dummy_once = np.ones((len(train_data),1))\n",
    "        self.train_data = np.hstack((dummy_once,train_data))\n",
    "        self.train_labels = train_labels\n",
    "        \n",
    "        self.params = np.zeros((len(self.train_data[0]),1))\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.print_every = print_every\n",
    "        self._lambda = lamda\n",
    "        self.decay = decay\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def cost(self,y,y_pred):\n",
    "        return -np.mean(y*np.log(y_pred)+(1-y)*np.log(1-y_pred))\n",
    "    \n",
    "    def gradient(self,y,y_pred,x):\n",
    "        return np.dot(x.T,(y_pred-y))+(2*(self._lambda/len(self.train_labels ))*self.params)\n",
    "    \n",
    "    def train(self):\n",
    "        for i in range(self.epoch):\n",
    "            for j in range(len(self.train_labels)//self.batch_size):\n",
    "                idx = list(np.random.choice(np.arange(len(self.train_labels)),self.batch_size,replace=False))\n",
    "                data = self.train_data[idx]\n",
    "                label = self.train_labels[idx]\n",
    "\n",
    "                y_pred = self.sigmoid(np.dot(data,self.params))\n",
    "                loss = self.cost(label,y_pred)\n",
    "\n",
    "                gra = self.gradient(label,y_pred,data)\n",
    "                self.params -= self.lr*gra\n",
    "\n",
    "                self.lr *= (1. / (1. + self.decay * i))\n",
    "            \n",
    "            if self.print_every:\n",
    "                if i%self.print_every == 0 or i == self.epoch-1:\n",
    "                    print('Epoch : {}  Loss: {}'.format(i,loss))\n",
    "    def predict(self,test_data):\n",
    "        result = self.sigmoid(np.dot(test_data,self.params[1:])+self.params[0])\n",
    "        result[result > 0.5 ] = 1\n",
    "        result[result <= 0.5 ] = 0\n",
    "        return result\n",
    "    \n",
    "    def evaluate(self,test_data,labels):\n",
    "        accuracy = accuracy_score(self.predict(test_data),labels)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(x_data,y_data,lr,lamda=0.2,epoch=10,k=4,batch_size=64,decay=10):\n",
    "    if len(x_data)%k != 0:\n",
    "        print('cant vsplit',len(x_data),' by ',k)\n",
    "        return\n",
    "    \n",
    "    x_data_splitted = np.vsplit(x_data,k)\n",
    "    y_data_splitted = np.vsplit(y_data,k)\n",
    "    \n",
    "    aggrigate_result = []\n",
    "    for i in range(len(x_data_splitted)):\n",
    "        train = []\n",
    "        test = []\n",
    "        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n",
    "        x_test = x_data_splitted[i]\n",
    "        y_test = y_data_splitted[i]\n",
    "        for item in items:\n",
    "            if len(train) == 0:\n",
    "                x_train = x_data_splitted[item]\n",
    "                y_train = y_data_splitted[item]\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n",
    "                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n",
    "        \n",
    "        logistic = logisticregression(x_train,y_train,batch_size=batch_size,lamda=lamda,lr=lr,decay=decay,epoch=epoch,print_every=None)\n",
    "        logistic.train()\n",
    "        \n",
    "        result = logistic.evaluate(x_test,y_test)\n",
    "        aggrigate_result.append(result)\n",
    "        \n",
    "        value = sum(aggrigate_result)/len(aggrigate_result)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_result = cross_validate(X_train_mat100,get_label(type=0).reshape(-1,1),k=5,lr=0.01,batch_size=32,lamda=5,epoch=100)\n",
    "ordinal_result = cross_validate(get_ordinal_data(X_train_.seq.values),get_label(type=0).reshape(-1,1),k=5,lr=0.01,batch_size=32,lamda=5,epoch=100)\n",
    "kmer_result = cross_validate(get_data(6)[:2000,:],get_label(type=0).reshape(-1,1),k=5,lr=0.01,batch_size=32,lamda=5,epoch=100)\n",
    "count_result = cross_validate(get_count_grams(X_train_.seq.values,6),get_label(type=0).reshape(-1,1),k=5,lr=0.01,batch_size=32,lamda=5,epoch=100)\n",
    "tfidf_result = cross_validate(get_tf_idf_grams(X_train_.seq.values,6),get_label(type=0).reshape(-1,1),k=5,lr=0.01,batch_size=32,lamda=5,epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test which data is better representation\n",
    "----------------------------------------------\n",
    "\n",
    "\n",
    " The First Result Shows The Quality of the data on a simple logistic regression without any hyper-parameters search. Meaning that this is not the best value this method can achive but it can show us the quality of the data preprocessing\n",
    " \n",
    "- The parameters of logistic regression was not optimized for each data so this result might not show the real value.\n",
    " \n",
    "- Count Vectorizer is the best representation so far "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot === 0.483\n",
      "Ordinal === 0.49499999999999994\n",
      "K-mer === 0.607\n",
      "Count-Vectorizer === 0.624\n",
      "Tf-idf === 0.5189999999999999\n"
     ]
    }
   ],
   "source": [
    "final_value = {'One Hot':one_hot_result,'Ordinal':ordinal_result,'K-mer':kmer_result,'Count-Vectorizer':count_result,'Tf-idf':tfidf_result}\n",
    "final_value\n",
    "\n",
    "for k,v in final_value.items():\n",
    "    print('{} === {}'.format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Related Experiments +  Kernel Related Experiments\n",
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already experimented with Logistic regression now we will add \n",
    "\n",
    "- Ridge Regression\n",
    "- SVM\n",
    "- Kernelized Method of the Variants\n",
    "\n",
    "In the folowing lines of code  define a set of kernels that we will applied with ridge regression and SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_from_median(X):\n",
    "    pairwise_diff = X[:, :, None] - X[:, :, None].T\n",
    "    pairwise_diff *= pairwise_diff\n",
    "    euclidean_dist = np.sqrt(pairwise_diff.sum(axis=1))\n",
    "    return np.median(euclidean_dist)\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=5.0):\n",
    "    return np.exp(-np.linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n",
    "\n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2.T)\n",
    "\n",
    "def polynomial_kernel(X1, X2, power=2):\n",
    "    return np.power((1 + linear_kernel(X1, X2)),power)\n",
    "\n",
    "def rbf_kernel(X1, X2, sigma=10):\n",
    "    X2_norm = np.sum(X2 ** 2, axis = -1)\n",
    "    X1_norm = np.sum(X1 ** 2, axis = -1)\n",
    "    gamma = 1 / (2 * sigma ** 2)\n",
    "    K = np.exp(- gamma * (X1_norm[:, None] + X2_norm[None, :] - 2 * np.dot(X1, X2.T)))\n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is Base class for all Kernelized method we saw in class \n",
    "     \n",
    "All kernelized methods are derived from this class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelMethodBase(object):\n",
    "    kernels_ = {\n",
    "        'linear': linear_kernel,\n",
    "        'polynomial': polynomial_kernel,\n",
    "        'rbf': rbf_kernel,\n",
    "        'gaussian':gaussian_kernel\n",
    "    }\n",
    "    def __init__(self, kernel='linear', **kwargs):\n",
    "        self.kernel_name = kernel\n",
    "        self.kernel_function_ = self.kernels_[kernel]\n",
    "        self.kernel_parameters = self.get_kernel_parameters(**kwargs)\n",
    "        \n",
    "    def get_kernel_parameters(self, **kwargs):\n",
    "        params = {}\n",
    "        if self.kernel_name == 'rbf' or self.kernel_name == 'gaussian':\n",
    "            params['sigma'] = kwargs.get('sigma', None)\n",
    "        if self.kernel_name == 'polynomial':\n",
    "            params['power'] = kwargs.get('power', None)\n",
    "        return params\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        return self\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge Regression\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is  implementation of ridge regression with kernel.\n",
    "- We train this model with alll the kernels above.\n",
    "- It gives the best performance in the leaderboard with linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRidgeRegression(KernelMethodBase):\n",
    "    '''\n",
    "    Kernel Ridge Regression\n",
    "    '''\n",
    "    def __init__(self, lambd=0.1, **kwargs):\n",
    "        self.lambd = lambd\n",
    "        # Python 3: replace the following line by\n",
    "        # super().__init__(**kwargs)\n",
    "        super(KernelRidgeRegression, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, sample_weights=None):\n",
    "        n, p = X.shape\n",
    "        assert (n == len(y))\n",
    "    \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        if sample_weights is not None:\n",
    "            w_sqrt = np.sqrt(sample_weights)\n",
    "            self.X_train = self.X_train * w_sqrt[:, None]\n",
    "            self.y_train = self.y_train * w_sqrt\n",
    "        \n",
    "        A = self.kernel_function_(X,X,**self.kernel_parameters)\n",
    "        A[np.diag_indices_from(A)] = np.add(A[np.diag_indices_from(A)],n*self.lambd)\n",
    "        # self.alpha = (K + n lambda I)^-1 y\n",
    "        self.alpha = np.linalg.solve(A , self.y_train)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        K_x = self.kernel_function_(X,self.X_train, **self.kernel_parameters)\n",
    "        return K_x.dot(self.alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.decision_function(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with all the kernels defined above.\n",
    "-----------------------------------------------------\n",
    "\n",
    "- After training, we realized that the linear kernel with the k-mer data was giving the best result(65% on cross validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelSVM(KernelMethodBase):\n",
    "    def __init__(self, C=0.1, **kwargs):\n",
    "        self.C = C\n",
    "        # Python 3: replace the following line by\n",
    "        # super().__init__(**kwargs)\n",
    "        super(KernelSVM, self).__init__(**kwargs)\n",
    "        \n",
    "    def cvxopt_qp(self,P, q, G, h, A, b):\n",
    "        P = .5 * (P + P.T)\n",
    "        cvx_matrices = [\n",
    "            cvxopt.matrix(M) if M is not None else None for M in [P, q, G, h, A, b] \n",
    "        ]\n",
    "        #cvxopt.solvers.options['show_progress'] = False\n",
    "        solution = cvxopt.solvers.qp(*cvx_matrices, options={'show_progress': False})\n",
    "        return np.array(solution['x']).flatten()\n",
    "    \n",
    "    def svm_dual_soft_to_qp_kernel(self,K, y, C=1):\n",
    "        n = K.shape[0]\n",
    "        assert (len(y) == n)\n",
    "\n",
    "        # Dual formulation, soft margin\n",
    "        # P = np.diag(y) @ K @ np.diag(y)\n",
    "        P = np.diag(y).dot(K).dot(np.diag(y))\n",
    "        # As a regularization, we add epsilon * identity to P\n",
    "        eps = 1e-12\n",
    "        P += eps * np.eye(n)\n",
    "        q = - np.ones(n)\n",
    "        G = np.vstack([-np.eye(n), np.eye(n)])\n",
    "        h = np.hstack([np.zeros(n), C * np.ones(n)])\n",
    "        A = y[np.newaxis, :]\n",
    "        b = np.array([0.])\n",
    "        return P, q, G, h, A.astype(float), b\n",
    "\n",
    "\n",
    "    def fit(self, X, y, tol=1e-8):\n",
    "        n, p = X.shape\n",
    "        assert (n == len(y))\n",
    "    \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        # Kernel matrix\n",
    "        K = self.kernel_function_(\n",
    "            self.X_train, self.X_train, **self.kernel_parameters)\n",
    "        \n",
    "        # Solve dual problem\n",
    "        self.alpha = self.cvxopt_qp(*self.svm_dual_soft_to_qp_kernel(K, y, C=self.C))\n",
    "        \n",
    "        # Compute support vectors and bias b\n",
    "        sv = np.logical_and((self.alpha > tol), (self.C - self.alpha > tol))\n",
    "        self.bias = y[sv] - K[sv].dot(self.alpha * y)\n",
    "        self.bias = self.bias.mean()\n",
    "\n",
    "        self.support_vector_indices = np.nonzero(sv)[0]\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        K_x = self.kernel_function_(X, self.X_train, **self.kernel_parameters)\n",
    "        return K_x.dot(self.alpha * self.y_train) + self.bias\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.decision_function(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We cross validated all of our submissions to avoud overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(x_data,y_data,model_name,lr=None,kernel=None,lambd=0.2,C=3,sigma=0.5,k=5,power=2):\n",
    "    if len(x_data)%k != 0:\n",
    "        print('cant vsplit',len(x_data),' by ',k)\n",
    "        return\n",
    "    \n",
    "    x_data_splitted = np.vsplit(x_data,k)\n",
    "    y_data_splitted = np.vsplit(y_data.reshape(-1,1),k)\n",
    "    \n",
    "    aggrigate_result = []\n",
    "    for i in range(len(x_data_splitted)):\n",
    "        train = []\n",
    "        test = []\n",
    "        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n",
    "        x_test = x_data_splitted[i]\n",
    "        y_test = y_data_splitted[i]\n",
    "        for item in items:\n",
    "            if len(train) == 0:\n",
    "                x_train = x_data_splitted[item]\n",
    "                y_train = y_data_splitted[item]\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n",
    "                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n",
    "        \n",
    "        if model_name == 'KernelRidgeRegression':\n",
    "            model = KernelRidgeRegression(\n",
    "                    kernel=kernel,\n",
    "                    lambd=lambd,\n",
    "                    sigma=sigma,\n",
    "                    power=power\n",
    "                ).fit(x_train, y_train)\n",
    "            result =sum(np.sign(model.predict(x_test))==y_test)/len(y_test)#roc_auc_score(np.sign(model.predict(x_test)),y_test) #\n",
    "            \n",
    "        elif model_name == 'KernelSVM':\n",
    "\n",
    "            model = KernelSVM(C=C,\n",
    "                              kernel=kernel,\n",
    "                              lambd=lambd,\n",
    "                              sigma=sigma,\n",
    "                              power=power)\n",
    "            model.fit(x_train, y_train.flatten())\n",
    "            y_pred = model.predict(x_test)\n",
    "            \n",
    "            result = sum((y_pred.flatten()==y_test.flatten()))/len(y_test)\n",
    "            \n",
    "        else:\n",
    "            print('wrong model_name')\n",
    "            return 0\n",
    "            \n",
    "        aggrigate_result.append(result)\n",
    "        \n",
    "        value = sum(aggrigate_result)/len(aggrigate_result)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optuna](https://optuna.org/) \n",
    "\n",
    "- A hyperparameter optimization framework\n",
    "- we tried to get the best hyperparameters for the models we worked on using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lambd = trial.suggest_float('lambd', 1e-5, 100.0)\n",
    "    sigma = trial.suggest_float('sigma', 1e-5, 150)\n",
    "    k =  trial.suggest_categorical('k', [4,5,8])\n",
    "    C =  trial.suggest_float('C', 0.1,50)\n",
    "    power =  trial.suggest_int('power', 2,5)\n",
    "    kmer_size =  trial.suggest_int('kmer_size', 3,8)\n",
    "#     kernel =  trial.suggest_categorical('kernel', ['linear','rbf','gaussian_kernel','polynomial'])\n",
    "#     model_name\n",
    "    \n",
    "    return cross_validate(get_data(kmer_size)[:2000,:],get_label(type=-1),model_name='KernelRidgeRegression',C=C,kernel='linear',lambd=lambd,k=k,sigma=sigma,power=power)\n",
    "\n",
    "# cross_validate(X_train_mat100, y,lamda=0.01,k=4)\n",
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "df = study.optimize(func=objective, n_trials=100,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "df.sort_values(by=['value']).tail(7)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "              value\t\tparams_C\tparams_k\tparams_kmer_size\tparams_lambd\tparams_power\tparams_sigma\n",
    "\n",
    "\n",
    "linear  -     0.6520\t   47.102628\t    4\t     7\t    0.756909\t    3\t       15.369069\n",
    "rbf           0.6470\t00:00:04.427092\t9.071834\t5\t7\t77.976361\t4\t3.775633\n",
    "polinomial    0.6585\t00:00:03.992723\t27.187947\t4\t7\t1.418356\t2\n",
    "\n",
    "\n",
    "svm\n",
    "\n",
    "linear        0.6525\t00:00:08.438371\t10.791498\t5\t8\t52.239587\t4\t74.078190\n",
    "rbf           0.6530\t00:00:05.553817\t14.611283\t4\t7\t32.915987\t2\t45.600422\n",
    "pol           0.6315\t00:00:03.624395\t18.056135\t4\t5\t53.476413\t3\t9.218339"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally we use our best parameters and train our model and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980, 65536) (20, 65536) (1980, 1) (20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test  = get_train_test(get_data(8)[:2000,:],get_label(type=-1).reshape(-1,1),p=0.01)\n",
    "\n",
    "\n",
    "kernel = 'rbf'\n",
    "power = 3\n",
    "sigma = 12.217523\n",
    "C = 9.839876\n",
    "lambd = 38.202739\n",
    "model = KernelSVM(C=C, kernel=kernel, sigma=sigma,lambd=lambd, power=power)\n",
    "model.fit(X_train, y_train.flatten())\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "sum(y_pred.flatten()==y_test.flatten())/len(y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.656"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(get_data(8)[:2000,:],get_label(type=-1),\n",
    "               model_name='KernelSVM',\n",
    "               kernel = 'rbf',\n",
    "               k=4,\n",
    "                power = 3,\n",
    "                sigma = 12.217523,\n",
    "                C = 9.839876,\n",
    "                lambd = 38.202739)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final  = model.predict(get_data(8)[2000:,:])\n",
    "sumbission = []\n",
    "for i in range(len(X_test_final)):\n",
    "    r1 = X_test_final[i]\n",
    "    if r1 == 1:\n",
    "        sumbission.append([i,int(r1)])\n",
    "    elif r1 == -1:\n",
    "        sumbission.append([i,0])\n",
    "    else:\n",
    "        print('problem')\n",
    "        \n",
    "    \n",
    "# sumbission\n",
    "df = pd.DataFrame(sumbission)\n",
    "df.columns = ['Id','Bound']\n",
    "df.to_csv('cv_65.75_rbf-svm.csv',index=False)\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- we tried and tested and found that Ridge Regression,SVM are good models to work on when we have small data set coupled with special kernels.\n",
    "- Resources on writing kernels were very scarce and some additional kernels we tried to implement was not succsessful because of time constraint. For example Mismatch kernel and String simillarity kernels\n",
    "- Optimizing parameters for kernels is very difficult task. we took more than half of the time doing parameter search.\n",
    "- new data represetnations are the best way to improve our model looking forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional\n",
    "\n",
    "- Please take a look at the code we worked on to see detail implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
