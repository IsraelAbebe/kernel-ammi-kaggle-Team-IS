{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Files\n",
    "This data challenge contains one dataset of 2000 training sequences. The main files available are the following ones\n",
    "\n",
    "* Xtr.csv - the training sequences.\n",
    "* \n",
    "* Xte.csv - the test sequences.\n",
    "* \n",
    "* Ytr.csv - the sequence labels of the training sequences indicating bound (1) or not (0).\n",
    "* \n",
    "Each row of Xtr.csv represents a sequence. Xte.csv contains 1000 test sequences, for which you need to predict. Ytr.csv contains the labels corresponding to the training data, in the same format as a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cvxopt in /usr/local/lib/python3.6/dist-packages (1.2.5)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
      "Requirement already satisfied: cliff in /usr/local/lib/python3.6/dist-packages (from optuna) (3.1.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.17)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.15.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (from optuna) (4.1.0)\n",
      "Requirement already satisfied: cmaes>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from optuna) (0.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.4)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
      "Requirement already satisfied: stevedore>=1.20.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.32.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.12.0)\n",
      "Requirement already satisfied: cmd2!=0.8.3,<0.9.0,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.8.9)\n",
      "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (5.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.1.3)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.0.4)\n",
      "Requirement already satisfied: pyperclip in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,<0.9.0,>=0.8.0->cliff->optuna) (1.8.0)\n",
      "Requirement already satisfied: wcwidth; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,<0.9.0,>=0.8.0->cliff->optuna) (0.1.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install cvxopt\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from numpy import linalg\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "import sklearn\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xtr.csv\", sep=',',index_col=0)\n",
    "\n",
    "labels=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Ytr.csv\", sep=',',index_col=0)\n",
    "\n",
    "\n",
    "test_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xte.csv\",sep=',', index_col=0)\n",
    "\n",
    "\n",
    "#optional data \n",
    "\n",
    "train_op_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xtr_mat100.csv\", sep=' ',header=None).values\n",
    "test_op_data=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2020/Xte_mat100.csv\", sep=' ',header=None).values\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_intercept(train_op_data):\n",
    "#     N=train_op_data.shape[0]\n",
    "   \n",
    "#     a = np.ones((N,1))\n",
    "    \n",
    "#     train_op_data=np.append(train_op_data,a, axis=1)\n",
    "#     return train_op_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the labels from 0,1 to -1 ,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= np.where(labels==0,-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the training dataset in train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 100) (660, 100) (1340, 1) (660, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_mat100 = train_op_data\n",
    "# \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_mat100, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape,X_val.shape,y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def test_data(X,y):\n",
    "    results = {}\n",
    "    X=scale(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    classifier = SVC(kernel = 'linear', C = 0.1, gamma = 0.01)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    results['svm'] = classifier.score(X_test,y_test)\n",
    "\n",
    "    clf = LogisticRegression(random_state=0,penalty='l2').fit(X, y)\n",
    "    results['lr'] = clf.score(X_test,y_test)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'svm': 0.58, 'lr': 0.65}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data(X_train_mat100,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spectrom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4096)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def getKmers(sequence, size=3):\n",
    "    return [sequence[x:x+size] for x in range(len(sequence) - size + 1)]\n",
    "def base2int(c):\n",
    "    return {'A':0,'C':1,'G':2,'T':3}.get(c,0)\n",
    "\n",
    "def index(kmer):\n",
    "    base_idx = np.array([base2int(base) for base in kmer])\n",
    "    multiplier = 4** np.arange(len(kmer))\n",
    "    kmer_idx = multiplier.dot(base_idx)\n",
    "    return kmer_idx\n",
    "    \n",
    "    \n",
    "def spectral_embedding(sequence,kmer_size=3):\n",
    "    kmers = getKmers(sequence,kmer_size)\n",
    "    kmer_idxs = [index(kmer) for kmer in kmers]\n",
    "    one_hot_vector = np.zeros(4**kmer_size)\n",
    "    \n",
    "    for kmer_idx in kmer_idxs:\n",
    "        one_hot_vector[kmer_idx] += 1\n",
    "    return one_hot_vector\n",
    "\n",
    "train_data['kmers'] = train_data.seq.apply(lambda x:list(spectral_embedding(x,kmer_size=3)))\n",
    "\n",
    "\n",
    "kmer_data = []\n",
    "for i in train_data.seq.values:\n",
    "    kmer_data.append(spectral_embedding(i,kmer_size=6))\n",
    "    \n",
    "np.array(kmer_data).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'svm': 0.6216666666666667, 'lr': 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data(kmer_data,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_mat100 =train\n",
    "# # \n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train_mat100, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# print(X_train.shape,X_val.shape,y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2.T)\n",
    "\n",
    "def polynomial_kernel(x, y, power):\n",
    "    return (1 + np.dot(x, y.T)) ** power\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=5.0):\n",
    "    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n",
    "\n",
    "def kernelFuncTrigo(x1, x2, i):\n",
    "   \n",
    "    sigma = 0.5\n",
    "    \n",
    "    kxx = 1 +(np.dot(sin(k*sigma*x1), sin(k*sigma*x2))  + np.dot(cos(k*sigma*x1), cos(k*sigma*x2))  for k in range(1, i+1))\n",
    "\n",
    "    return kxx\n",
    "\n",
    "def rbf_kernel(X1, X2, sigma=10):\n",
    "    '''\n",
    "    Returns the kernel matrix K(X1_i, X2_j): size (n1, n2)\n",
    "    where K is the RBF kernel with parameter sigma\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    X1: an (n1, p) matrix\n",
    "    X2: an (n2, p) matrix\n",
    "    sigma: float\n",
    "    '''\n",
    "    # For loop with rbf_kernel_element works but is slow in python\n",
    "    # Use matrix operations!\n",
    "    \n",
    "    X2_norm = np.sum(X2 ** 2)\n",
    "    X1_norm = np.sum(X1 ** 2)\n",
    "    gamma = 1 / (2 * sigma ** 2)\n",
    "    K = np.exp(- gamma * (X1_norm+ X2_norm- 2 * np.dot(X1, X2.T)))\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soft margin svm with SGD from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, lr,lamb, epoch):\n",
    "        self.lr=lr\n",
    "        self.lamb=lamb\n",
    "        self.epoch=epoch\n",
    "    def compute_cost(self,W, X, Y):\n",
    "        # calculate hinge loss\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        distances = 1 - Y * (np.dot(X, W))\n",
    "        distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "        hinge_loss = self.lamb * (np.sum(distances) / N)\n",
    "\n",
    "        # calculate cost\n",
    "        cost = (1 / 2) * (np.dot(W, W)) + hinge_loss\n",
    "        return cost\n",
    "    \n",
    "    def calculate_cost_gradient(self,W, X_batch, Y_batch):\n",
    "        # if only one example is passed (eg. in case of SGD)\n",
    "        if type(Y_batch) == np.float64:\n",
    "            Y_batch = np.array([Y_batch])\n",
    "            X_batch = np.array([X_batch])  # gives multidimensional array\n",
    "\n",
    "        distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
    "        dw = np.zeros(len(W))\n",
    "\n",
    "        for ind, d in enumerate(distance):\n",
    "            if max(0, d) == 0:\n",
    "                di = W\n",
    "            else:\n",
    "                di = W -(self.lamb* Y_batch[ind] * X_batch[ind])\n",
    "            dw += di\n",
    "\n",
    "        dw = dw/len(Y_batch)  # average\n",
    "        return dw\n",
    "    \n",
    "    def sgd(self,features, outputs):\n",
    "        \n",
    "        weights = np.zeros(features.shape[1])\n",
    "        nth = 0\n",
    "        prev_cost = float(\"inf\")\n",
    "        cost_threshold = 1e-3  # in percent\n",
    "        # stochastic gradient descent\n",
    "        for epoc in range(1, self.epoch):\n",
    "            # shuffle to prevent repeating update cycles\n",
    "            X, Y = (features, outputs)\n",
    "            for ind, x in enumerate(X):\n",
    "                ascent = self.calculate_cost_gradient(weights, x, Y[ind])\n",
    "                weights = weights +(self.lr * ascent)\n",
    "\n",
    "\n",
    "            cost = self.compute_cost(weights, features, outputs)\n",
    "#             print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n",
    "            # stoppage criterion\n",
    "#             print(cost)\n",
    "#             if abs(prev_cost - cost) < cost_threshold:\n",
    "#                 return weights\n",
    "            prev_cost = cost\n",
    "            nth += 1\n",
    "        return weights\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_func(self,x_train,y_train):\n",
    "        W = self.sgd(x_train, y_train)\n",
    "#         print(\"training finished.\")\n",
    "        return W\n",
    "\n",
    "    def validation(self,x_val,y_val,w):\n",
    "        y_test_predicted = np.array([])\n",
    "        for i in range(x_val.shape[0]):\n",
    "            yp = np.sign(np.dot(w, x_val[i])) #model\n",
    "            y_test_predicted = np.append(y_test_predicted, yp)\n",
    "        return accuracy_score(y_val, y_test_predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_op_data=insert_intercept(train_op_data)\n",
    "svm_sgd=SVM(lr=0.2,lamb=50,epoch=100)\n",
    "w=svm_sgd.train_func(np.array(kmer_data)[:1340,:],labels[:1340])\n",
    "#svm_sgd.validation(np.array(kmer_data)[1340:2000,:],labels[1340:],w)\n",
    "\n",
    "                   \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    " \n",
    "        lr = trial.suggest_float('lr',  1e-3, 1)\n",
    "        \n",
    "        epoch=trial.suggest_int('epoch',  20, 200)\n",
    "        lamb=trial.suggest_int('lamb',  1e-5,20)\n",
    "        C=trial.suggest_int('C',  1,20)\n",
    "        sigma=trial.suggest_int('sigma',  1e-7,20)\n",
    "        svmm=SVM(lr=lr,lamb=lamb, epoch=epoch)\n",
    "        w=svm_sgd.train_func(X_train,y_train)\n",
    "        acc=svmm.validation(X_val,y_val,w)\n",
    "            \n",
    "\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "study.optimize(func=objective, n_trials=100,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate(x_data,y_data,model,lr,lamda=0.2,epoch=10,k=5):\n",
    "    if len(x_data)%k != 0:\n",
    "        print('cant vsplit',len(x_data),' by ',k)\n",
    "        return\n",
    "    \n",
    "    x_data_splitted = np.vsplit(x_data,k)\n",
    "    y_data_splitted = np.vsplit(y_data,k)\n",
    "    \n",
    "    aggrigate_result = []\n",
    "    \n",
    "    for i in range(len(x_data_splitted)):\n",
    "        train = []\n",
    "        test = []\n",
    "        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n",
    "        x_test = x_data_splitted[i]\n",
    "        y_test = y_data_splitted[i]\n",
    "        \n",
    "        for item in items:\n",
    "            if len(train) == 0:\n",
    "                x_train = x_data_splitted[item]\n",
    "                y_train = y_data_splitted[item]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n",
    "                \n",
    "                \n",
    "                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n",
    "                \n",
    "                \n",
    "       \n",
    "        w=model.train_func(x_train,y_train)\n",
    "       \n",
    "       \n",
    "        \n",
    "        result = model.validation(x_test,y_test,w)\n",
    "        aggrigate_result.append(result)\n",
    "         \n",
    "        \n",
    "        value = sum(aggrigate_result)/len(aggrigate_result)\n",
    "        \n",
    "        \n",
    "    return value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(train_op_data,labels,svm_sgd,lr=0.01,lamda=0.2,epoch=100,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search using optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # soft margin SVM with kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM dual problem with kerner and solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SVM(object):\n",
    "\n",
    "#     def __init__(self, kernel=polynomial_kernel, sigma=5,C=10):\n",
    "#         self.kernel = kernel\n",
    "#         self.C = C\n",
    "#         self.sigma=sigma\n",
    "#         if self.C is not None: self.C = float(self.C)\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         n_samples, n_features = X.shape\n",
    "\n",
    "#         K = np.zeros((n_samples, n_samples))\n",
    "#         for i in range(n_samples):\n",
    "#             for j in range(n_samples):\n",
    "#                 #print(X[i], X[j])\n",
    "#                 K[i,j] = self.kernel(X[i], X[j],self.sigma)\n",
    "#         y = y_train.reshape(-1,1) * 1.\n",
    "#         X_dash = y_train * X_train\n",
    "#         H = np.dot(X_dash , X_dash.T) * 1\n",
    "\n",
    "#         P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "#         q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "#         A = cvxopt.matrix(y, (1,n_samples))\n",
    "#         b = cvxopt.matrix(0.0)\n",
    "#         if self.C is None:\n",
    "#             G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "#             h = cvxopt.matrix(np.zeros(n_samples))\n",
    "#         else:\n",
    "#             tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "#             tmp2 = np.identity(n_samples)\n",
    "#             G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "#             tmp1 = np.zeros(n_samples)\n",
    "#             tmp2 = np.ones(n_samples) * self.C\n",
    "#             h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "            \n",
    "#         solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "#         a = np.ravel(solution['x'])\n",
    "\n",
    "#         sv = a > 1e-6\n",
    "#         ind = np.arange(len(a))[sv]\n",
    "#         self.a = a[sv]\n",
    "#         self.sv = X[sv]\n",
    "#         self.sv_y = y[sv]\n",
    "#         print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "#         self.b = 0\n",
    "#         for n in range(len(self.a)):\n",
    "#             self.b += self.sv_y[n]\n",
    "#             self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        \n",
    "#         #print(len(a))\n",
    "#         if len(a)!=0:\n",
    "#             self.b /= len(self.a)\n",
    "\n",
    "#         # Weight vector\n",
    "#         if self.kernel == linear_kernel:\n",
    "#             self.w = np.zeros(n_features)\n",
    "#             for n in range(len(self.a)):\n",
    "#                 self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "#         else:\n",
    "#             self.w = None\n",
    "\n",
    "#     def project(self, X):\n",
    "#         if self.w is not None:\n",
    "#             return np.dot(X, self.w) + self.b\n",
    "#         else:\n",
    "#             y_predict = np.zeros(len(X))\n",
    "#             for i in range(len(X)):\n",
    "#                 s = 0\n",
    "#                 for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "#                     s += a * sv_y * self.kernel(X[i], sv)\n",
    "#                 y_predict[i] = s\n",
    "#             return y_predict + self.b\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=polynomial_kernel, C=0.2, power=2):\n",
    "        self.kernel = polynomial_kernel\n",
    "        self.C = C\n",
    "        self.power=power\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = self.kernel(X[i], X[j],self.power)\n",
    "        y=y.reshape(-1,1) * 1.\n",
    "        #print(y)\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        \n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-10\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == polynomial_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def project(self, X):\n",
    "        if self.w is not None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * self.kernel(X[i], sv, self.power)\n",
    "                y_predict[i] = s\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'builtin_function_or_method' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-3c6dc0695a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msvmm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolynomial_kernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msvmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-2c384e63d388>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'builtin_function_or_method' and 'float'"
     ]
    }
   ],
   "source": [
    "\n",
    "svmm=SVM(kernel=polynomial_kernel,C=0,power=5)\n",
    "svmm.fit(X_train,y_train)\n",
    "y_predict = svmm.predict(X_val)\n",
    "print(accuracy_score(y_val, y_predict) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    " \n",
    "        C = trial.suggest_float('C',  1e-3, 10)\n",
    "        p = trial.suggest_float('p',  1e-4, 10)\n",
    "        #gamma = trial.suggest_float('gamma',  0.1, 100)\n",
    "        #kernels=['polynomial_kernel','gaussian_kernel','rbf_kernel','linear_kernel']\n",
    "        #sigma=trial.suggest_int('sigma',  1, 20)\n",
    "        svmm=SVM(kernel=polynomial_kernel,C=C, power=p)\n",
    "        svmm.fit(np.array(kmer_data)[:1340,:],labels[:1340])\n",
    "        y_predict = svmm.predict(np.array(kmer_data)[1340:,:])\n",
    "            \n",
    "\n",
    "        return accuracy_score(labels[1340:], y_predict)  \n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf620b98d5844e05b36affc47193392d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.9565e-04 -4.8348e+03  1e+04  4e-01  7e-15\n",
      " 1:  3.2129e-04 -2.7670e+02  3e+02  4e-03  5e-15\n",
      " 2:  3.2972e-04 -2.7707e+00  3e+00  4e-05  5e-15\n",
      " 3:  3.2778e-04 -2.9297e-02  4e-02  4e-07  5e-15\n",
      " 4:  2.0537e-04 -9.3570e-04  1e-03  8e-09  5e-15\n",
      " 5:  1.0700e-05 -9.0785e-05  1e-04  6e-11  7e-15\n",
      " 6: -1.4994e-05 -2.7135e-05  1e-05  2e-16  3e-15\n",
      " 7: -1.6273e-05 -1.6847e-05  6e-07  2e-16  1e-15\n",
      " 8: -1.6279e-05 -1.6293e-05  1e-08  2e-16  1e-15\n",
      "Optimal solution found.\n",
      "1336 support vectors out of 1340 points\n",
      "[I 2020-05-29 20:36:13,157] Finished trial#0 with value: 0.5060606060606061 with parameters: {'C': 3.6080955873539926, 'p': 3.7718754720631953}. Best is trial#0 with value: 0.5060606060606061.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.7259e-10 -1.0856e+04  2e+04  2e-01  2e-14\n",
      " 1:  3.4353e-10 -2.3956e+02  4e+02  2e-03  1e-14\n",
      " 2:  3.4521e-10 -2.3959e+00  4e+00  2e-05  9e-15\n",
      " 3:  3.4522e-10 -2.3959e-02  4e-02  2e-07  9e-15\n",
      " 4:  3.4522e-10 -2.3959e-04  4e-04  2e-09  7e-15\n",
      " 5:  3.4521e-10 -2.3964e-06  4e-06  2e-11  8e-15\n",
      " 6:  3.4421e-10 -2.4389e-08  4e-08  2e-13  1e-14\n",
      "Optimal solution found.\n",
      "0 support vectors out of 1340 points\n",
      "[W 2020-05-29 20:36:32,067] Setting status of trial#1 as TrialState.FAIL because of the following error: ZeroDivisionError('division by zero')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/optuna/study.py\", line 699, in _run_trial\n",
      "    result = func(trial)\n",
      "  File \"<ipython-input-38-b4c9111a4fec>\", line 9, in objective\n",
      "    svmm.fit(np.array(kmer_data)[:1340,:],labels[:1340])\n",
      "  File \"<ipython-input-35-813535605f73>\", line 55, in fit\n",
      "    self.b /= len(self.a)\n",
      "ZeroDivisionError: division by zero\n",
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-193fc584ef75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 334\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                 )\n\u001b[1;32m    336\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    646\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             message = \"Setting status of trial#{} as {}. {}\".format(\n",
      "\u001b[0;32m<ipython-input-38-b4c9111a4fec>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#sigma=trial.suggest_int('sigma',  1, 20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msvmm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolynomial_kernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msvmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1340\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1340\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1340\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-813535605f73>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msv_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msv_y\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Weight vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "study.optimize(func=objective, n_trials=100,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "df.sort_values(by=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate(x_data,y_data,model, testData, kernelFunc, powerI, lambdaPara,epoch=10,k=5):\n",
    "    if len(x_data)%k != 0:\n",
    "        print('cant vsplit',len(x_data),' by ',k)\n",
    "        return\n",
    "    \n",
    "    x_data_splitted = np.vsplit(x_data,k)\n",
    "    y_data_splitted = np.vsplit(y_data,k)\n",
    "    \n",
    "    aggrigate_result = []\n",
    "    \n",
    "    for i in range(len(x_data_splitted)):\n",
    "        train = []\n",
    "        test = []\n",
    "        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n",
    "        x_test = x_data_splitted[i]\n",
    "        y_test = y_data_splitted[i]\n",
    "        \n",
    "        for item in items:\n",
    "            if len(train) == 0:\n",
    "                x_train = x_data_splitted[item]\n",
    "                y_train = y_data_splitted[item]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n",
    "                \n",
    "                \n",
    "                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n",
    "                \n",
    "                \n",
    "       \n",
    "        w=model.train_func(x_train,y_train,testData, kernelFunc, powerI, lambdaPara)\n",
    "       \n",
    "       \n",
    "        \n",
    "        result = model.validation(x_test,y_test,w)\n",
    "        aggrigate_result.append(result)\n",
    "         \n",
    "        \n",
    "        value = sum(aggrigate_result)/len(aggrigate_result)\n",
    "        \n",
    "        \n",
    "    return value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernel logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(ypred, ytrue):\n",
    "    e = (ypred != ytrue).mean()\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelMethodBase(object):\n",
    "    '''\n",
    "    Base class for kernel methods models\n",
    "    \n",
    "    Methods\n",
    "    ----\n",
    "    fit\n",
    "    predict\n",
    "    '''\n",
    "    kernels_ = {\n",
    "        'linear': linear_kernel,\n",
    "        'polynomial': polynomial_kernel,\n",
    "        'rbf': rbf_kernel,\n",
    "        'gaussian':gaussian_kernel\n",
    "    }\n",
    "    def __init__(self, kernel='linear', **kwargs):\n",
    "        self.kernel_name = kernel\n",
    "        self.kernel_function_ = self.kernels_[kernel]\n",
    "        self.kernel_parameters = self.get_kernel_parameters(**kwargs)\n",
    "        \n",
    "    def get_kernel_parameters(self, **kwargs):\n",
    "        params = {}\n",
    "        if self.kernel_name == 'rbf':\n",
    "            params['sigma'] = kwargs.get('sigma', None)\n",
    "        if self.kernel_name == 'polynomial':\n",
    "            params['power'] = kwargs.get('power', None)\n",
    "        return params\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        return self\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRidgeRegression(KernelMethodBase):\n",
    "    '''\n",
    "    Kernel Ridge Regression\n",
    "    '''\n",
    "    def __init__(self, lambd=0.1, **kwargs):\n",
    "        self.lambd = lambd\n",
    "        # Python 3: replace the following line by\n",
    "        # super().__init__(**kwargs)\n",
    "        super(KernelRidgeRegression, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, sample_weights=None):\n",
    "        n, p = X.shape\n",
    "        assert (n == len(y))\n",
    "    \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        if sample_weights is not None:\n",
    "            w_sqrt = np.sqrt(sample_weights)\n",
    "            self.X_train = self.X_train * w_sqrt\n",
    "            self.y_train = self.y_train * w_sqrt\n",
    "        \n",
    "        A = self.kernel_function_(X,X,**self.kernel_parameters)\n",
    "        A[np.diag_indices_from(A)] = np.add(A[np.diag_indices_from(A)],n*self.lambd)\n",
    "        # self.alpha = (K + n lambda I)^-1 y\n",
    "        self.alpha = np.linalg.solve(A , self.y_train)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        K_x = self.kernel_function_(X,self.X_train, **self.kernel_parameters)\n",
    "        return K_x.dot(self.alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.decision_function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(x_data,y_data,kernel=None,lambd=0.2,sigma=0.5,k=5,power=2):\n",
    "    if len(x_data)%k != 0:\n",
    "        print('cant vsplit',len(x_data),' by ',k)\n",
    "        return\n",
    "    \n",
    "    x_data_splitted = np.vsplit(x_data,k)\n",
    "    y_data_splitted = np.vsplit(y_data.reshape(-1,1),k)\n",
    "    \n",
    "    aggrigate_result = []\n",
    "    for i in range(len(x_data_splitted)):\n",
    "        train = []\n",
    "        test = []\n",
    "        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n",
    "        x_test = x_data_splitted[i]\n",
    "        y_test = y_data_splitted[i]\n",
    "        for item in items:\n",
    "            if len(train) == 0:\n",
    "                x_train = x_data_splitted[item]\n",
    "                y_train = y_data_splitted[item]\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n",
    "                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n",
    "            \n",
    "            \n",
    "        model = KernelRidgeRegression(\n",
    "                kernel=kernel,\n",
    "                lambd=lambd,\n",
    "                sigma=sigma,\n",
    "                power=power\n",
    "            ).fit(x_train, y_train)\n",
    "        result = sum(np.sign(model.predict(x_test))==y_test)/len(y_test)\n",
    "        aggrigate_result.append(result)\n",
    "        \n",
    "        value = sum(aggrigate_result)/len(aggrigate_result)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(X_train,y_train,kernel='polynomial', lambd=4.023839198201892e-06,k=5,sigma=4.,power=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lambd = trial.suggest_loguniform('lambd', 1e-6, 5)\n",
    "    sigma = trial.suggest_loguniform('sigma', 1e-6, 6)\n",
    "    k =  trial.suggest_categorical('k', [2,4,5,8,10])\n",
    "    power =  trial.suggest_int('power', 2,6)\n",
    "    kernel =  trial.suggest_categorical('kernel', ['linear','rbf','polynomial'])\n",
    "    \n",
    "    return cross_validate(X_train,y_train,kernel=kernel,lambd=lambd,k=4,sigma=sigma,power=power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate(X_train_mat100, y,lamda=0.01,k=4)\n",
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "df = study.optimize(func=objective, n_trials=500,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class KernelLogisticRegression(KernelMethodBase):\n",
    "    '''\n",
    "    Kernel Logistic Regression\n",
    "    '''\n",
    "    def __init__(self, lambd=0.1, **kwargs):\n",
    "        self.lambd = lambd\n",
    "        # Python 3: replace the following line by\n",
    "        # super().__init__(**kwargs)\n",
    "        super(KernelLogisticRegression, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, max_iter=100, tol=1e-5):\n",
    "        n, p = X.shape\n",
    "        assert (n == len(y))\n",
    "    \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        K = self.kernel_function_(X, X, **self.kernel_parameters)\n",
    "        \n",
    "        # IRLS\n",
    "        KRR = KernelRidgeRegression(\n",
    "            lambd=2*self.lambd,\n",
    "            kernel=self.kernel_name,\n",
    "            **self.kernel_parameters\n",
    "        )\n",
    "        # Initialize\n",
    "        alpha = np.zeros(n)\n",
    "        # Iterate until convergence or max iterations\n",
    "        for n_iter in range(max_iter):\n",
    "            alpha_old = alpha\n",
    "            m = K.dot(alpha_old)\n",
    "            w = sigmoid(m) * sigmoid(-m)\n",
    "            z = m + self.y_train / sigmoid(self.y_train * m)\n",
    "            alpha = KRR.fit(self.X_train, z, sample_weights=w).alpha\n",
    "            # Break condition (achieved convergence)\n",
    "            if np.sum((alpha-alpha_old)**2) < tol:\n",
    "                break\n",
    "\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha = alpha\n",
    "\n",
    "        return self\n",
    "            \n",
    "    def decision_function(self, X_test):\n",
    "        K_x = self.kernel_function_(X_test, self.X_train, **self.kernel_parameters)\n",
    "        # Probability of y=1 (between 0 and 1)\n",
    "        return np.sign(K_x.dot(self.alpha))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probas = self.decision_function(X)\n",
    "        predicted_classes = np.where(probas < 0.5, -1, 1)\n",
    "        return predicted_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "sigma = .4\n",
    "lambd = 1.\n",
    "fig_title = 'Logistic Regression, {} Kernel'.format(kernel)\n",
    "\n",
    "model = KernelLogisticRegression(lambd=lambd, kernel=kernel, sigma=sigma)\n",
    "y_pred = model.fit(X_train, y_train).predict(X_val)\n",
    "print('Test error: {:.2%}'.format(error(y_pred, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
