{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 184kB 2.9MB/s eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 11.7MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 81kB 7.3MB/s  eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 81kB 6.4MB/s  eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s  eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 112kB 16.2MB/s eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s  eta 0:00:01\n",
      "\u001b[?25h  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mat100 = pd.read_csv('../data/Xte_mat100.csv',sep=' ',header=None).values\n",
    "X_train_mat100 = pd.read_csv('../data/Xtr_mat100.csv',sep=' ',header=None).values\n",
    "\n",
    "\n",
    "# X_test = pd.read_csv('../data/Xte.csv',sep=',',index_col=0).values\n",
    "# X_train = pd.read_csv('../data/Xtr.csv',sep=',',index_col=0).values\n",
    "\n",
    "y = pd.read_csv('../data/Ytr.csv',sep=',',index_col=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (2000, 100) y_train (2000, 1)\n",
      "x_test: (1000, 100)\n"
     ]
    }
   ],
   "source": [
    "print('x_train: {} y_train {}'.format(X_train_mat100.shape,y.shape))\n",
    "# print('x_train: {} y_train {}'.format(X_train.shape,y.shape))\n",
    "print('x_test: {}'.format(X_test_mat100.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 100) (200, 100) (1800, 1) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_mat100 = scale(X_train_mat100)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_mat100, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print(X_train.shape,X_val.shape,y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticregression():\n",
    "    def __init__(self,train_data,train_labels,lamda=0.2,lr=0.01,decay=10,batch_size=None,epoch=10,print_every = 10):\n",
    "        dummy_once = np.ones((len(train_data),1))\n",
    "        self.train_data = np.hstack((dummy_once,train_data))\n",
    "        self.train_labels = train_labels\n",
    "        \n",
    "        self.params = np.zeros((len(self.train_data[0]),1))\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.print_every = print_every\n",
    "        self._lambda = lamda\n",
    "        self.decay = decay\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def cost(self,y,y_pred):\n",
    "        return -np.mean(y*np.log(y_pred)+(1-y)*np.log(1-y_pred))\n",
    "    \n",
    "    def gradient(self,y,y_pred,x):\n",
    "        hassien = np.dot(y_pred.T,(1-y_pred))*np.linalg.inv(np.dot(x.T,x))\n",
    "        return np.dot(hassien,np.dot(x.T,(y_pred-y)))+(2*self._lambda*self.params)\n",
    "    \n",
    "    def train(self):\n",
    "        for i in range(self.epoch):\n",
    "            y_pred = self.sigmoid(np.dot(self.train_data,self.params))\n",
    "            loss = self.cost(self.train_labels,y_pred)\n",
    "            \n",
    "            gra = self.gradient(self.train_labels,y_pred,self.train_data)\n",
    "            \n",
    "            self.params -= self.lr*gra\n",
    "            \n",
    "            self.lr *= (1. / (1. + self.decay * i))\n",
    "            \n",
    "            if self.print_every:\n",
    "                if i%self.print_every == 0 or i == self.epoch-1:\n",
    "                    print('Epoch : {}  Loss: {}'.format(i,loss))\n",
    "    def predict(self,test_data):\n",
    "        result = self.sigmoid(np.dot(test_data,self.params[1:])+self.params[0])\n",
    "        result[result > 0.5 ] = 1\n",
    "        result[result <= 0.5 ] = 0\n",
    "        return result\n",
    "    \n",
    "    def evaluate(self,test_data,labels):\n",
    "        accuracy = accuracy_score(self.predict(test_data),labels)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(x_data,y_data,lr,lamda=0.2,epoch=10,k=5,decay=10):\n",
    "    if len(x_data)%k != 0:\n",
    "        print('cant vsplit',len(x_data),' by ',k)\n",
    "        return\n",
    "    \n",
    "    x_data_splitted = np.vsplit(x_data,k)\n",
    "    y_data_splitted = np.vsplit(y_data,k)\n",
    "    \n",
    "    aggrigate_result = []\n",
    "    for i in range(len(x_data_splitted)):\n",
    "        train = []\n",
    "        test = []\n",
    "        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n",
    "        x_test = x_data_splitted[i]\n",
    "        y_test = y_data_splitted[i]\n",
    "        for item in items:\n",
    "            if len(train) == 0:\n",
    "                x_train = x_data_splitted[item]\n",
    "                y_train = y_data_splitted[item]\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n",
    "                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n",
    "            \n",
    "        logistic = logisticregression(x_train,y_train,lamda=lamda,lr=lr,decay=decay,epoch=epoch,print_every=None)\n",
    "        logistic.train()\n",
    "        \n",
    "        result = logistic.evaluate(x_test,y_test)\n",
    "        aggrigate_result.append(result)\n",
    "        \n",
    "        value = sum(aggrigate_result)/len(aggrigate_result)\n",
    "    return value if value!= None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    lamda = trial.suggest_loguniform('lamda', 0.01, 0.5)\n",
    "    k =  trial.suggest_categorical('k', [4,5,8,10])\n",
    "    epoch =  trial.suggest_int('epoch', 10, 20)\n",
    "    decay = trial.suggest_int('decay', 3, 10)\n",
    "    return cross_validate(X_train_mat100, y,lr=lr,lamda=lamda,k=k,epoch=epoch,decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:90: ExperimentalWarning:\n",
      "\n",
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8181c3ed4215428ebcf9954fc05857d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-05-25 21:13:41,553]\u001b[0m Finished trial#0 with value: 0.549 with parameters: {'lr': 0.01799227840699826, 'lamda': 0.039554372946756323, 'k': 8, 'epoch': 11, 'decay': 6}. Best is trial#0 with value: 0.549.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:41,787]\u001b[0m Finished trial#1 with value: 0.5625 with parameters: {'lr': 6.295925826193235e-05, 'lamda': 0.06005922386526236, 'k': 4, 'epoch': 20, 'decay': 9}. Best is trial#1 with value: 0.5625.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:42,022]\u001b[0m Finished trial#2 with value: 0.5685 with parameters: {'lr': 0.0002100871699434587, 'lamda': 0.17148645254518677, 'k': 5, 'epoch': 16, 'decay': 10}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:42,259]\u001b[0m Finished trial#3 with value: 0.5675000000000001 with parameters: {'lr': 0.007266199858132005, 'lamda': 0.2919960688968443, 'k': 5, 'epoch': 18, 'decay': 3}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:42,563]\u001b[0m Finished trial#4 with value: 0.5495000000000001 with parameters: {'lr': 0.0005604992222853867, 'lamda': 0.37210607528766415, 'k': 8, 'epoch': 19, 'decay': 10}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:42,865]\u001b[0m Finished trial#5 with value: 0.528 with parameters: {'lr': 0.00031536332066069205, 'lamda': 0.2155070282094314, 'k': 10, 'epoch': 19, 'decay': 8}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:43,124]\u001b[0m Finished trial#6 with value: 0.5295 with parameters: {'lr': 0.07675977767641334, 'lamda': 0.21183197969991055, 'k': 10, 'epoch': 10, 'decay': 6}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:43,352]\u001b[0m Finished trial#7 with value: 0.5665 with parameters: {'lr': 0.00293881226061249, 'lamda': 0.16284152236290467, 'k': 5, 'epoch': 16, 'decay': 3}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:43,626]\u001b[0m Finished trial#8 with value: 0.5485 with parameters: {'lr': 0.01640218120396753, 'lamda': 0.060352181970578976, 'k': 8, 'epoch': 14, 'decay': 6}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:43,835]\u001b[0m Finished trial#9 with value: 0.5645 with parameters: {'lr': 0.005742864480268022, 'lamda': 0.02772028024935754, 'k': 4, 'epoch': 16, 'decay': 9}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:44,060]\u001b[0m Finished trial#10 with value: 0.5665 with parameters: {'lr': 1.620886311575509e-05, 'lamda': 0.1201415685015607, 'k': 5, 'epoch': 13, 'decay': 10}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:44,288]\u001b[0m Finished trial#11 with value: 0.5654999999999999 with parameters: {'lr': 0.00011180787592554936, 'lamda': 0.47678321475069896, 'k': 5, 'epoch': 17, 'decay': 3}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:44,530]\u001b[0m Finished trial#12 with value: 0.5675 with parameters: {'lr': 0.0019243953514140966, 'lamda': 0.36164526878985803, 'k': 5, 'epoch': 18, 'decay': 4}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:44,746]\u001b[0m Finished trial#13 with value: 0.5660000000000001 with parameters: {'lr': 1.1986982037913745e-05, 'lamda': 0.011328751339886525, 'k': 5, 'epoch': 15, 'decay': 4}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:44,985]\u001b[0m Finished trial#14 with value: 0.5685 with parameters: {'lr': 0.07432713579995072, 'lamda': 0.11662927489963469, 'k': 5, 'epoch': 17, 'decay': 8}. Best is trial#2 with value: 0.5685.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:45,214]\u001b[0m Finished trial#15 with value: 0.569 with parameters: {'lr': 0.09052008053692323, 'lamda': 0.11654707667449393, 'k': 5, 'epoch': 13, 'decay': 8}. Best is trial#15 with value: 0.569.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:45,417]\u001b[0m Finished trial#16 with value: 0.5660000000000001 with parameters: {'lr': 6.765623295371807e-05, 'lamda': 0.10696080476071707, 'k': 5, 'epoch': 12, 'decay': 8}. Best is trial#15 with value: 0.569.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:45,722]\u001b[0m Finished trial#17 with value: 0.5315000000000001 with parameters: {'lr': 0.00025189995255826754, 'lamda': 0.0744174136278261, 'k': 10, 'epoch': 14, 'decay': 7}. Best is trial#15 with value: 0.569.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:45,939]\u001b[0m Finished trial#18 with value: 0.5670000000000001 with parameters: {'lr': 0.0008048235421249596, 'lamda': 0.029297686146043143, 'k': 5, 'epoch': 13, 'decay': 9}. Best is trial#15 with value: 0.569.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:46,157]\u001b[0m Finished trial#19 with value: 0.56 with parameters: {'lr': 4.3214925382762134e-05, 'lamda': 0.17421449001581635, 'k': 4, 'epoch': 10, 'decay': 10}. Best is trial#15 with value: 0.569.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:46,385]\u001b[0m Finished trial#20 with value: 0.5655000000000001 with parameters: {'lr': 2.462451643007305e-05, 'lamda': 0.08422420392076933, 'k': 5, 'epoch': 15, 'decay': 7}. Best is trial#15 with value: 0.569.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:46,635]\u001b[0m Finished trial#21 with value: 0.5690000000000001 with parameters: {'lr': 0.09109877864435428, 'lamda': 0.12550062204770787, 'k': 5, 'epoch': 16, 'decay': 8}. Best is trial#21 with value: 0.5690000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:46,869]\u001b[0m Finished trial#22 with value: 0.564 with parameters: {'lr': 0.03593871445041203, 'lamda': 0.14274953187815687, 'k': 5, 'epoch': 16, 'decay': 8}. Best is trial#21 with value: 0.5690000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:47,094]\u001b[0m Finished trial#23 with value: 0.5640000000000001 with parameters: {'lr': 0.03454476048303687, 'lamda': 0.23066984327319848, 'k': 5, 'epoch': 14, 'decay': 9}. Best is trial#21 with value: 0.5690000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:47,291]\u001b[0m Finished trial#24 with value: 0.5660000000000001 with parameters: {'lr': 0.00013052253553982174, 'lamda': 0.09889604899876271, 'k': 5, 'epoch': 12, 'decay': 7}. Best is trial#21 with value: 0.5690000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:47,522]\u001b[0m Finished trial#25 with value: 0.5700000000000001 with parameters: {'lr': 0.09122957809763829, 'lamda': 0.04709050179986035, 'k': 5, 'epoch': 17, 'decay': 5}. Best is trial#25 with value: 0.5700000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:47,778]\u001b[0m Finished trial#26 with value: 0.5700000000000001 with parameters: {'lr': 0.0723344994223796, 'lamda': 0.04195083623001984, 'k': 5, 'epoch': 17, 'decay': 5}. Best is trial#25 with value: 0.5700000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:48,008]\u001b[0m Finished trial#27 with value: 0.563 with parameters: {'lr': 0.03673870799980159, 'lamda': 0.01609931445814757, 'k': 4, 'epoch': 18, 'decay': 5}. Best is trial#25 with value: 0.5700000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:48,260]\u001b[0m Finished trial#28 with value: 0.5715000000000001 with parameters: {'lr': 0.012816281673183577, 'lamda': 0.046028957583431944, 'k': 5, 'epoch': 17, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:48,522]\u001b[0m Finished trial#29 with value: 0.546 with parameters: {'lr': 0.014795641854892965, 'lamda': 0.04344850344180145, 'k': 8, 'epoch': 17, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:48,851]\u001b[0m Finished trial#30 with value: 0.5295 with parameters: {'lr': 0.008777840694770564, 'lamda': 0.039558458729411536, 'k': 10, 'epoch': 20, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:49,091]\u001b[0m Finished trial#31 with value: 0.5650000000000001 with parameters: {'lr': 0.05513521595703318, 'lamda': 0.02520986689514454, 'k': 5, 'epoch': 17, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:49,332]\u001b[0m Finished trial#32 with value: 0.5660000000000001 with parameters: {'lr': 0.023995058562335667, 'lamda': 0.04472171088179488, 'k': 5, 'epoch': 19, 'decay': 6}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:49,562]\u001b[0m Finished trial#33 with value: 0.5685 with parameters: {'lr': 0.0956751800028021, 'lamda': 0.057550122750491615, 'k': 5, 'epoch': 18, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:49,781]\u001b[0m Finished trial#34 with value: 0.5660000000000001 with parameters: {'lr': 0.045675953346344375, 'lamda': 0.05169121426310489, 'k': 5, 'epoch': 16, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:50,043]\u001b[0m Finished trial#35 with value: 0.5665000000000001 with parameters: {'lr': 0.011127236030740336, 'lamda': 0.03295593133881792, 'k': 5, 'epoch': 17, 'decay': 6}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:50,315]\u001b[0m Finished trial#36 with value: 0.548 with parameters: {'lr': 0.004669303831625278, 'lamda': 0.020711408822946364, 'k': 8, 'epoch': 15, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:50,557]\u001b[0m Finished trial#37 with value: 0.5650000000000001 with parameters: {'lr': 0.020642601797817985, 'lamda': 0.07310336194232005, 'k': 5, 'epoch': 19, 'decay': 6}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:50,789]\u001b[0m Finished trial#38 with value: 0.5690000000000001 with parameters: {'lr': 0.09778097390147639, 'lamda': 0.035972289349803496, 'k': 5, 'epoch': 18, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:51,038]\u001b[0m Finished trial#39 with value: 0.5595000000000001 with parameters: {'lr': 0.0613881053541421, 'lamda': 0.017824438213232187, 'k': 4, 'epoch': 16, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:51,380]\u001b[0m Finished trial#40 with value: 0.5325 with parameters: {'lr': 0.0029553868025528826, 'lamda': 0.08603996076831776, 'k': 10, 'epoch': 19, 'decay': 7}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:51,620]\u001b[0m Finished trial#41 with value: 0.5675 with parameters: {'lr': 0.027002627262370914, 'lamda': 0.05023020900230653, 'k': 5, 'epoch': 18, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:51,850]\u001b[0m Finished trial#42 with value: 0.5700000000000001 with parameters: {'lr': 0.09690484007731717, 'lamda': 0.03524047018650506, 'k': 5, 'epoch': 17, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:52,137]\u001b[0m Finished trial#43 with value: 0.5650000000000001 with parameters: {'lr': 0.053674462227964306, 'lamda': 0.034739736824807486, 'k': 5, 'epoch': 18, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:52,365]\u001b[0m Finished trial#44 with value: 0.5690000000000001 with parameters: {'lr': 0.0923430218661092, 'lamda': 0.06672480745808901, 'k': 5, 'epoch': 17, 'decay': 6}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:52,598]\u001b[0m Finished trial#45 with value: 0.568 with parameters: {'lr': 0.012433265825073389, 'lamda': 0.02316402796729631, 'k': 5, 'epoch': 17, 'decay': 6}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:52,887]\u001b[0m Finished trial#46 with value: 0.552 with parameters: {'lr': 0.09969367928932636, 'lamda': 0.03160104328820851, 'k': 8, 'epoch': 19, 'decay': 3}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:53,142]\u001b[0m Finished trial#47 with value: 0.5660000000000001 with parameters: {'lr': 0.026858272831991284, 'lamda': 0.0384927898273816, 'k': 5, 'epoch': 18, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:53,370]\u001b[0m Finished trial#48 with value: 0.5695 with parameters: {'lr': 0.06687105163644837, 'lamda': 0.048837547950710915, 'k': 5, 'epoch': 16, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:53,598]\u001b[0m Finished trial#49 with value: 0.5695 with parameters: {'lr': 0.0014116890094419206, 'lamda': 0.052154735761148305, 'k': 5, 'epoch': 15, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:53,827]\u001b[0m Finished trial#50 with value: 0.567 with parameters: {'lr': 0.0013961749474111793, 'lamda': 0.060435248309187804, 'k': 5, 'epoch': 15, 'decay': 3}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:54,061]\u001b[0m Finished trial#51 with value: 0.569 with parameters: {'lr': 0.0005364612678665305, 'lamda': 0.045304091098288916, 'k': 5, 'epoch': 16, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:54,341]\u001b[0m Finished trial#52 with value: 0.5685 with parameters: {'lr': 0.005457616558662503, 'lamda': 0.048878997045475124, 'k': 5, 'epoch': 15, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:54,570]\u001b[0m Finished trial#53 with value: 0.5640000000000001 with parameters: {'lr': 0.003112491842223772, 'lamda': 0.05661681960262647, 'k': 5, 'epoch': 17, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:54,797]\u001b[0m Finished trial#54 with value: 0.5665 with parameters: {'lr': 0.06433611057114198, 'lamda': 0.04101879942188599, 'k': 5, 'epoch': 16, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:55,020]\u001b[0m Finished trial#55 with value: 0.5645 with parameters: {'lr': 0.0004865680751354013, 'lamda': 0.027108081693398848, 'k': 5, 'epoch': 16, 'decay': 3}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:55,237]\u001b[0m Finished trial#56 with value: 0.5700000000000001 with parameters: {'lr': 0.01622263212621308, 'lamda': 0.06569536674206795, 'k': 5, 'epoch': 14, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:55,493]\u001b[0m Finished trial#57 with value: 0.5265000000000001 with parameters: {'lr': 0.01830546811739075, 'lamda': 0.08219736715831855, 'k': 10, 'epoch': 14, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:55,704]\u001b[0m Finished trial#58 with value: 0.567 with parameters: {'lr': 0.002020204576342697, 'lamda': 0.06675150921905446, 'k': 5, 'epoch': 13, 'decay': 3}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:55,927]\u001b[0m Finished trial#59 with value: 0.5645 with parameters: {'lr': 0.0421297338723387, 'lamda': 0.030411107513987973, 'k': 4, 'epoch': 17, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:56,151]\u001b[0m Finished trial#60 with value: 0.5695 with parameters: {'lr': 0.008726591900590145, 'lamda': 0.061796311471694246, 'k': 5, 'epoch': 14, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:56,440]\u001b[0m Finished trial#61 with value: 0.5705 with parameters: {'lr': 0.07053277308095697, 'lamda': 0.0521530107069668, 'k': 5, 'epoch': 15, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:56,660]\u001b[0m Finished trial#62 with value: 0.5660000000000001 with parameters: {'lr': 0.004239816654361241, 'lamda': 0.05413947221394241, 'k': 5, 'epoch': 15, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:56,874]\u001b[0m Finished trial#63 with value: 0.568 with parameters: {'lr': 0.0008442614172956937, 'lamda': 0.038108814332868786, 'k': 5, 'epoch': 14, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:57,073]\u001b[0m Finished trial#64 with value: 0.5645 with parameters: {'lr': 0.029915014558223346, 'lamda': 0.09028222397475034, 'k': 5, 'epoch': 12, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:57,300]\u001b[0m Finished trial#65 with value: 0.5685 with parameters: {'lr': 0.007133896281267064, 'lamda': 0.07308262385381098, 'k': 5, 'epoch': 15, 'decay': 3}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:57,532]\u001b[0m Finished trial#66 with value: 0.5665000000000001 with parameters: {'lr': 0.009985181301078996, 'lamda': 0.06309593103388969, 'k': 5, 'epoch': 14, 'decay': 6}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:57,760]\u001b[0m Finished trial#67 with value: 0.5495000000000001 with parameters: {'lr': 0.04195957122810096, 'lamda': 0.0433779226814461, 'k': 8, 'epoch': 13, 'decay': 4}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:57,965]\u001b[0m Finished trial#68 with value: 0.5685 with parameters: {'lr': 0.016756396925113815, 'lamda': 0.09654247483926633, 'k': 5, 'epoch': 13, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:58,183]\u001b[0m Finished trial#69 with value: 0.5685 with parameters: {'lr': 0.06625843854754053, 'lamda': 0.0461718496640195, 'k': 5, 'epoch': 16, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:58,435]\u001b[0m Finished trial#70 with value: 0.5695 with parameters: {'lr': 0.014638071071097018, 'lamda': 0.06005677642881086, 'k': 5, 'epoch': 14, 'decay': 6}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:58,657]\u001b[0m Finished trial#71 with value: 0.5665000000000001 with parameters: {'lr': 0.007513119259765603, 'lamda': 0.07929219797998349, 'k': 5, 'epoch': 14, 'decay': 6}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:58,886]\u001b[0m Finished trial#72 with value: 0.5695 with parameters: {'lr': 0.07426465482699533, 'lamda': 0.03574364343769128, 'k': 5, 'epoch': 17, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:59,120]\u001b[0m Finished trial#73 with value: 0.571 with parameters: {'lr': 0.08038732118711786, 'lamda': 0.035483561741721376, 'k': 5, 'epoch': 17, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:59,376]\u001b[0m Finished trial#74 with value: 0.567 with parameters: {'lr': 0.04867070010306593, 'lamda': 0.026053750967085492, 'k': 5, 'epoch': 17, 'decay': 6}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:59,647]\u001b[0m Finished trial#75 with value: 0.571 with parameters: {'lr': 0.07348282514858365, 'lamda': 0.03369280559916668, 'k': 5, 'epoch': 17, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:13:59,881]\u001b[0m Finished trial#76 with value: 0.5705 with parameters: {'lr': 0.08123147612360847, 'lamda': 0.029922700807963904, 'k': 5, 'epoch': 18, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:00,121]\u001b[0m Finished trial#77 with value: 0.565 with parameters: {'lr': 0.034484694538110754, 'lamda': 0.02267576094854431, 'k': 4, 'epoch': 20, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:00,455]\u001b[0m Finished trial#78 with value: 0.5285 with parameters: {'lr': 0.08009525651211542, 'lamda': 0.018499665025555175, 'k': 10, 'epoch': 18, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:00,720]\u001b[0m Finished trial#79 with value: 0.567 with parameters: {'lr': 0.052045146562352956, 'lamda': 0.029080641868503544, 'k': 5, 'epoch': 18, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:00,954]\u001b[0m Finished trial#80 with value: 0.571 with parameters: {'lr': 0.09643528723016141, 'lamda': 0.03285023110777481, 'k': 5, 'epoch': 17, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:01,193]\u001b[0m Finished trial#81 with value: 0.5690000000000001 with parameters: {'lr': 0.09487648340285527, 'lamda': 0.03340630212840734, 'k': 5, 'epoch': 17, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:01,467]\u001b[0m Finished trial#82 with value: 0.5660000000000001 with parameters: {'lr': 0.023494577660654246, 'lamda': 0.02410225184956038, 'k': 5, 'epoch': 18, 'decay': 5}. Best is trial#28 with value: 0.5715000000000001.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:01,744]\u001b[0m Finished trial#83 with value: 0.572 with parameters: {'lr': 0.07439722099766218, 'lamda': 0.04159502539549111, 'k': 5, 'epoch': 17, 'decay': 4}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:01,976]\u001b[0m Finished trial#84 with value: 0.5645 with parameters: {'lr': 0.037110314256925375, 'lamda': 0.02908783213788585, 'k': 5, 'epoch': 17, 'decay': 4}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:02,218]\u001b[0m Finished trial#85 with value: 0.5650000000000001 with parameters: {'lr': 0.0580920143273545, 'lamda': 0.037247954175236846, 'k': 5, 'epoch': 19, 'decay': 4}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:02,480]\u001b[0m Finished trial#86 with value: 0.5705 with parameters: {'lr': 0.08406213790022259, 'lamda': 0.013991648031121157, 'k': 5, 'epoch': 16, 'decay': 6}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:02,750]\u001b[0m Finished trial#87 with value: 0.5705 with parameters: {'lr': 0.07855068069842422, 'lamda': 0.020924702974884777, 'k': 5, 'epoch': 16, 'decay': 6}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:02,979]\u001b[0m Finished trial#88 with value: 0.5695 with parameters: {'lr': 0.0847184884998345, 'lamda': 0.013351470401799017, 'k': 5, 'epoch': 16, 'decay': 7}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:03,240]\u001b[0m Finished trial#89 with value: 0.55 with parameters: {'lr': 0.051285157900669995, 'lamda': 0.01099880945168058, 'k': 8, 'epoch': 16, 'decay': 6}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:03,500]\u001b[0m Finished trial#90 with value: 0.5690000000000001 with parameters: {'lr': 0.07170017427477353, 'lamda': 0.012673783328553349, 'k': 5, 'epoch': 16, 'decay': 6}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:03,733]\u001b[0m Finished trial#91 with value: 0.5685 with parameters: {'lr': 0.09956815820785737, 'lamda': 0.02109174611213762, 'k': 5, 'epoch': 17, 'decay': 6}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:03,962]\u001b[0m Finished trial#92 with value: 0.5685 with parameters: {'lr': 0.04285378371137981, 'lamda': 0.014576782459352658, 'k': 5, 'epoch': 17, 'decay': 6}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:04,200]\u001b[0m Finished trial#93 with value: 0.5680000000000001 with parameters: {'lr': 0.08128856920398388, 'lamda': 0.01802799306480616, 'k': 5, 'epoch': 18, 'decay': 7}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:04,453]\u001b[0m Finished trial#94 with value: 0.5645 with parameters: {'lr': 0.03153156612356625, 'lamda': 0.026540326079536873, 'k': 5, 'epoch': 16, 'decay': 4}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:04,674]\u001b[0m Finished trial#95 with value: 0.5675000000000001 with parameters: {'lr': 0.02172476164710878, 'lamda': 0.020612021200830947, 'k': 5, 'epoch': 15, 'decay': 4}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:04,941]\u001b[0m Finished trial#96 with value: 0.5690000000000001 with parameters: {'lr': 0.0656847664745304, 'lamda': 0.031852461485104844, 'k': 5, 'epoch': 17, 'decay': 5}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:05,173]\u001b[0m Finished trial#97 with value: 0.5690000000000001 with parameters: {'lr': 0.09871352210751118, 'lamda': 0.040175781156460644, 'k': 5, 'epoch': 18, 'decay': 5}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:05,416]\u001b[0m Finished trial#98 with value: 0.5665 with parameters: {'lr': 0.060460318579941594, 'lamda': 0.046386394664571555, 'k': 5, 'epoch': 17, 'decay': 6}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\u001b[32m[I 2020-05-25 21:14:05,644]\u001b[0m Finished trial#99 with value: 0.5660000000000001 with parameters: {'lr': 0.043111902696165535, 'lamda': 0.04287169942872284, 'k': 5, 'epoch': 16, 'decay': 5}. Best is trial#83 with value: 0.572.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross_validate(X_train_mat100, y,0.001,10)\n",
    "\n",
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "study.optimize(func=objective, n_trials=100,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_decay</th>\n",
       "      <th>params_epoch</th>\n",
       "      <th>params_k</th>\n",
       "      <th>params_lamda</th>\n",
       "      <th>params_lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>00:00:00.160833</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0.161073</td>\n",
       "      <td>0.093368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>00:00:00.138507</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010428</td>\n",
       "      <td>0.049403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>00:00:00.129669</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.151531</td>\n",
       "      <td>0.041347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.5055</td>\n",
       "      <td>00:00:00.130993</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0.494667</td>\n",
       "      <td>0.034418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>00:00:00.135779</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0.027088</td>\n",
       "      <td>0.012938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>102</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>00:00:00.137702</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0.105406</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>00:00:00.128045</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0.304025</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>117</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>00:00:00.134048</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0.299451</td>\n",
       "      <td>0.000401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>165</td>\n",
       "      <td>0.5960</td>\n",
       "      <td>00:00:00.149376</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0.405698</td>\n",
       "      <td>0.000422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>171</td>\n",
       "      <td>0.5960</td>\n",
       "      <td>00:00:00.129486</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0.455265</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     number   value        duration  ...  params_k  params_lamda  params_lr\n",
       "88       88  0.5000 00:00:00.160833  ...         5      0.161073   0.093368\n",
       "3         3  0.5005 00:00:00.138507  ...         5      0.010428   0.049403\n",
       "6         6  0.5005 00:00:00.129669  ...         4      0.151531   0.041347\n",
       "7         7  0.5055 00:00:00.130993  ...         4      0.494667   0.034418\n",
       "26       26  0.5070 00:00:00.135779  ...         5      0.027088   0.012938\n",
       "..      ...     ...             ...  ...       ...           ...        ...\n",
       "102     102  0.5955 00:00:00.137702  ...         5      0.105406   0.000414\n",
       "93       93  0.5955 00:00:00.128045  ...         5      0.304025   0.000414\n",
       "117     117  0.5955 00:00:00.134048  ...         5      0.299451   0.000401\n",
       "165     165  0.5960 00:00:00.149376  ...         5      0.405698   0.000422\n",
       "171     171  0.5960 00:00:00.129486  ...         5      0.455265   0.000407\n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "\n",
    "df.sort_values(by=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.595"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(X_train_mat100,y,lamda=0.455265,epoch=50,lr=0.000407,decay=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0  Loss: 0.6931471805599452\n",
      "Epoch : 1  Loss: 0.6867708235098253\n",
      "Epoch : 2  Loss: 0.6809385491742812\n",
      "Epoch : 3  Loss: 0.6804877037751971\n",
      "Epoch : 4  Loss: 0.6804687383928599\n",
      "Epoch : 5  Loss: 0.6804681813613453\n",
      "Epoch : 6  Loss: 0.6804681690827427\n",
      "Epoch : 7  Loss: 0.68046816886251\n",
      "Epoch : 8  Loss: 0.6804681688590899\n",
      "Epoch : 9  Loss: 0.6804681688590469\n",
      "Epoch : 10  Loss: 0.6804681688590464\n",
      "Epoch : 11  Loss: 0.6804681688590466\n",
      "Epoch : 12  Loss: 0.6804681688590466\n",
      "Epoch : 13  Loss: 0.6804681688590466\n",
      "Epoch : 14  Loss: 0.6804681688590466\n",
      "Epoch : 15  Loss: 0.6804681688590466\n",
      "Epoch : 16  Loss: 0.6804681688590466\n",
      "Epoch : 17  Loss: 0.6804681688590466\n",
      "Epoch : 18  Loss: 0.6804681688590466\n",
      "Epoch : 19  Loss: 0.6804681688590466\n",
      "Epoch : 20  Loss: 0.6804681688590466\n",
      "Epoch : 21  Loss: 0.6804681688590466\n",
      "Epoch : 22  Loss: 0.6804681688590466\n",
      "Epoch : 23  Loss: 0.6804681688590466\n",
      "Epoch : 24  Loss: 0.6804681688590466\n",
      "Epoch : 25  Loss: 0.6804681688590466\n",
      "Epoch : 26  Loss: 0.6804681688590466\n",
      "Epoch : 27  Loss: 0.6804681688590466\n",
      "Epoch : 28  Loss: 0.6804681688590466\n",
      "Epoch : 29  Loss: 0.6804681688590466\n",
      "Epoch : 30  Loss: 0.6804681688590466\n",
      "Epoch : 31  Loss: 0.6804681688590466\n",
      "Epoch : 32  Loss: 0.6804681688590466\n",
      "Epoch : 33  Loss: 0.6804681688590466\n",
      "Epoch : 34  Loss: 0.6804681688590466\n",
      "Epoch : 35  Loss: 0.6804681688590466\n",
      "Epoch : 36  Loss: 0.6804681688590466\n",
      "Epoch : 37  Loss: 0.6804681688590466\n",
      "Epoch : 38  Loss: 0.6804681688590466\n",
      "Epoch : 39  Loss: 0.6804681688590466\n",
      "Epoch : 40  Loss: 0.6804681688590466\n",
      "Epoch : 41  Loss: 0.6804681688590466\n",
      "Epoch : 42  Loss: 0.6804681688590466\n",
      "Epoch : 43  Loss: 0.6804681688590466\n",
      "Epoch : 44  Loss: 0.6804681688590466\n",
      "Epoch : 45  Loss: 0.6804681688590466\n",
      "Epoch : 46  Loss: 0.6804681688590466\n",
      "Epoch : 47  Loss: 0.6804681688590466\n",
      "Epoch : 48  Loss: 0.6804681688590466\n",
      "Epoch : 49  Loss: 0.6804681688590466\n",
      "0.6461111111111111\n",
      "0.565\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test\n",
    "# logistic = logisticregression(X_train,y_train,lamda=0.362124,epoch=15,print_every=1,lr=0.000254)\n",
    "#\n",
    "logistic = logisticregression(X_train,y_train,lamda=0.455265,epoch=50,print_every=1,lr=0.000407,decay=11)\n",
    "logistic.train()\n",
    "        \n",
    "print(logistic.evaluate(X_train,y_train))\n",
    "print(logistic.evaluate(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mat100 = pd.read_csv('../data/Xte_mat100.csv',sep=' ',header=None).values\n",
    "\n",
    "X_test_mat100[0,:]\n",
    "\n",
    "X_test_mat100 = scale(X_test_mat100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sumbission = []\n",
    "for i in range(len(X_test_mat100)):\n",
    "    result = logistic.predict(X_test_mat100[i])\n",
    "    sumbission.append([i,int(result)])\n",
    "    result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumbission\n",
    "df = pd.DataFrame(sumbission)\n",
    "df.columns = ['Id','Bound']\n",
    "df.to_csv('test_59.5_cross_validated.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  Bound\n",
       "0    0      1\n",
       "1    1      0\n",
       "2    2      0\n",
       "3    3      0\n",
       "4    4      0\n",
       "5    5      1\n",
       "6    6      0\n",
       "7    7      1\n",
       "8    8      1\n",
       "9    9      0\n",
       "10  10      1\n",
       "11  11      1\n",
       "12  12      1\n",
       "13  13      0\n",
       "14  14      0\n",
       "15  15      1\n",
       "16  16      1\n",
       "17  17      1\n",
       "18  18      0\n",
       "19  19      1\n",
       "20  20      1\n",
       "21  21      0\n",
       "22  22      1\n",
       "23  23      0\n",
       "24  24      1\n",
       "25  25      0\n",
       "26  26      1\n",
       "27  27      1\n",
       "28  28      0\n",
       "29  29      0\n",
       "30  30      1\n",
       "31  31      0\n",
       "32  32      1\n",
       "33  33      0\n",
       "34  34      0\n",
       "35  35      0\n",
       "36  36      1\n",
       "37  37      0\n",
       "38  38      0\n",
       "39  39      0\n",
       "40  40      0\n",
       "41  41      1\n",
       "42  42      0\n",
       "43  43      1\n",
       "44  44      1\n",
       "45  45      1\n",
       "46  46      0\n",
       "47  47      0\n",
       "48  48      1\n",
       "49  49      0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.585"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(penalty = 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
