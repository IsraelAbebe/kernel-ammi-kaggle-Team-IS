{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 184kB 3.5MB/s eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 14.2MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s  eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 81kB 8.4MB/s  eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s  eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 112kB 18.5MB/s eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.0MB/s  eta 0:00:01\n",
      "\u001b[?25h  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "import optuna\n",
    "import random\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mat100 = pd.read_csv('../data/Xte_mat100.csv',sep=' ',header=None).values\n",
    "X_train_mat100 = pd.read_csv('../data/Xtr_mat100.csv',sep=' ',header=None).values\n",
    "\n",
    "X_test_ = pd.read_csv('../data/Xte.csv',sep=',',index_col=0)\n",
    "X_train_ = pd.read_csv('../data/Xtr.csv',sep=',',index_col=0)\n",
    "\n",
    "y = pd.read_csv('../data/Ytr.csv',sep=',',index_col=0)\n",
    "\n",
    "train_data = pd.concat([X_train_ , y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (2000, 100) y_train (2000, 1)\n",
      "x_test: (1000, 100)\n"
     ]
    }
   ],
   "source": [
    "print('x_train: {} y_train {}'.format(X_train_mat100.shape,y.shape))\n",
    "print('x_test: {}'.format(X_test_mat100.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(X,y,p):\n",
    "    X = scale(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=p, random_state=42)\n",
    "    print(X_train.shape,X_test.shape,y_train.shape, y_test.shape)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticregression():\n",
    "    def __init__(self,train_data,train_labels,lamda=0.2,lr=0.01,decay=10,batch_size=64,epoch=10,print_every = 10):\n",
    "        dummy_once = np.ones((len(train_data),1))\n",
    "        self.train_data = np.hstack((dummy_once,train_data))\n",
    "        self.train_labels = train_labels\n",
    "        \n",
    "        self.params = np.zeros((len(self.train_data[0]),1))\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.print_every = print_every\n",
    "        self._lambda = lamda\n",
    "        self.decay = decay\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def cost(self,y,y_pred):\n",
    "        return -np.mean(y*np.log(y_pred)+(1-y)*np.log(1-y_pred))\n",
    "    \n",
    "    def gradient(self,y,y_pred,x):\n",
    "#         hassien = np.dot(y_pred.T,(1-y_pred))*np.linalg.pinv(np.dot(x.T,x))\n",
    "#         return np.dot(hassien,np.dot(x.T,(y_pred-y)))+(2*(self._lambda/len(y_pred))*self.params)\n",
    "        return np.dot(x.T,(y_pred-y))+(2*(self._lambda/len(self.train_labels ))*self.params)\n",
    "    \n",
    "    def train(self):\n",
    "        for i in range(self.epoch):\n",
    "            for j in range(len(self.train_labels)//self.batch_size):\n",
    "                idx = list(np.random.choice(np.arange(len(self.train_labels)),self.batch_size,replace=False))\n",
    "                data = self.train_data[idx]\n",
    "                label = self.train_labels[idx]\n",
    "\n",
    "                y_pred = self.sigmoid(np.dot(data,self.params))\n",
    "                loss = self.cost(label,y_pred)\n",
    "\n",
    "                gra = self.gradient(label,y_pred,data)\n",
    "                self.params -= self.lr*gra\n",
    "\n",
    "                self.lr *= (1. / (1. + self.decay * i))\n",
    "            \n",
    "            if self.print_every:\n",
    "                if i%self.print_every == 0 or i == self.epoch-1:\n",
    "                    print('Epoch : {}  Loss: {}'.format(i,loss))\n",
    "    def predict(self,test_data):\n",
    "        result = self.sigmoid(np.dot(test_data,self.params[1:])+self.params[0])\n",
    "        result[result > 0.5 ] = 1\n",
    "        result[result <= 0.5 ] = 0\n",
    "        return result\n",
    "    \n",
    "    def evaluate(self,test_data,labels):\n",
    "        accuracy = accuracy_score(self.predict(test_data),labels)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(x_data,y_data,lr,lamda=0.2,epoch=10,k=4,batch_size=64,decay=10):\n",
    "    if len(x_data)%k != 0:\n",
    "        print('cant vsplit',len(x_data),' by ',k)\n",
    "        return\n",
    "    \n",
    "    x_data_splitted = np.vsplit(x_data,k)\n",
    "    y_data_splitted = np.vsplit(y_data,k)\n",
    "    \n",
    "    aggrigate_result = []\n",
    "    for i in range(len(x_data_splitted)):\n",
    "        train = []\n",
    "        test = []\n",
    "        items = [j for j in range(len(x_data_splitted)) if j !=i ]\n",
    "        x_test = x_data_splitted[i]\n",
    "        y_test = y_data_splitted[i]\n",
    "        for item in items:\n",
    "            if len(train) == 0:\n",
    "                x_train = x_data_splitted[item]\n",
    "                y_train = y_data_splitted[item]\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train,x_data_splitted[item]), axis=0)\n",
    "                y_train = np.concatenate((y_train,y_data_splitted[item]), axis=0)\n",
    "        \n",
    "        logistic = logisticregression(x_train,y_train,batch_size=batch_size,lamda=lamda,lr=lr,decay=decay,epoch=epoch,print_every=None)\n",
    "        logistic.train()\n",
    "        \n",
    "        result = logistic.evaluate(x_test,y_test)\n",
    "        aggrigate_result.append(result)\n",
    "        \n",
    "        value = sum(aggrigate_result)/len(aggrigate_result)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>Bound</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAGGGGCTGGGGAGGGGGCTGGCCCAGAGGCACCAGACTCTGCAGA...</td>\n",
       "      <td>1</td>\n",
       "      <td>gagggg aggggc ggggct gggctg ggctgg gctggg ctgg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CGGCCTGGGGGCCACATGTGAGTGCTTACCTGTGTGGGGATGAGGG...</td>\n",
       "      <td>0</td>\n",
       "      <td>cggcct ggcctg gcctgg cctggg ctgggg tggggg gggg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  seq  ...                                              words\n",
       "Id                                                     ...                                                   \n",
       "0   GAGGGGCTGGGGAGGGGGCTGGCCCAGAGGCACCAGACTCTGCAGA...  ...  gagggg aggggc ggggct gggctg ggctgg gctggg ctgg...\n",
       "1   CGGCCTGGGGGCCACATGTGAGTGCTTACCTGTGTGGGGATGAGGG...  ...  cggcct ggcctg gcctgg cctggg ctgggg tggggg gggg...\n",
       "\n",
       "[2 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getKmers(sequence, size=6):\n",
    "    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "train_data['words'] = train_data.seq.apply(lambda x: ' '.join(getKmers(x)))\n",
    "X_test_['words'] = X_test_.seq.apply(lambda x: ' '.join(getKmers(x)))\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1500)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "data = pd.DataFrame(pd.concat([train_data.words,X_test_.words],axis=0))\n",
    "\n",
    "train_text = data.words.values\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(2,2),max_features=1500,min_df=10)\n",
    "X = cv.fit_transform(train_text)\n",
    "X = X.todense()\n",
    "\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(np.array(X)[:2000,:],y.values,k=5,lr=0.001,batch_size=128,lamda=0.003,epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:90: ExperimentalWarning:\n",
      "\n",
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24ec1d970744d45bef45ee8e2c31684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-05-28 11:36:30,678]\u001b[0m Finished trial#0 with value: 0.6344999999999998 with parameters: {'lr': 0.09820908412853177, 'lamda': 2.030629914157752e-06, 'batch_size': 64, 'epoch': 370, 'decay': 3}. Best is trial#0 with value: 0.6344999999999998.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:36:37,137]\u001b[0m Finished trial#1 with value: 0.615 with parameters: {'lr': 0.002155667947797433, 'lamda': 8.846932713488824, 'batch_size': 32, 'epoch': 324, 'decay': 9}. Best is trial#0 with value: 0.6344999999999998.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:36:46,823]\u001b[0m Finished trial#2 with value: 0.6185 with parameters: {'lr': 0.011256244630345356, 'lamda': 7.206613329583028e-07, 'batch_size': 32, 'epoch': 486, 'decay': 5}. Best is trial#0 with value: 0.6344999999999998.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:36:52,839]\u001b[0m Finished trial#3 with value: 0.6345 with parameters: {'lr': 0.03449098601101696, 'lamda': 0.0031069453670833943, 'batch_size': 64, 'epoch': 468, 'decay': 7}. Best is trial#3 with value: 0.6345.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:36:54,310]\u001b[0m Finished trial#4 with value: 0.5900000000000001 with parameters: {'lr': 0.0002288163678336849, 'lamda': 0.0003217725954835587, 'batch_size': 64, 'epoch': 105, 'decay': 9}. Best is trial#3 with value: 0.6345.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:36:57,409]\u001b[0m Finished trial#5 with value: 0.6239999999999999 with parameters: {'lr': 0.024996754107298593, 'lamda': 0.06439020736784004, 'batch_size': 128, 'epoch': 326, 'decay': 3}. Best is trial#3 with value: 0.6345.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:00,966]\u001b[0m Finished trial#6 with value: 0.627 with parameters: {'lr': 0.025954099165351834, 'lamda': 0.02626642161448242, 'batch_size': 128, 'epoch': 379, 'decay': 10}. Best is trial#3 with value: 0.6345.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:05,749]\u001b[0m Finished trial#7 with value: 0.64 with parameters: {'lr': 0.032753629755188925, 'lamda': 1.1447264672068044e-07, 'batch_size': 128, 'epoch': 500, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:11,508]\u001b[0m Finished trial#8 with value: 0.6085 with parameters: {'lr': 0.0011078530442218517, 'lamda': 0.009835745425138112, 'batch_size': 32, 'epoch': 288, 'decay': 9}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:16,547]\u001b[0m Finished trial#9 with value: 0.619 with parameters: {'lr': 0.005501326452831073, 'lamda': 0.007531389176412949, 'batch_size': 32, 'epoch': 249, 'decay': 9}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:18,297]\u001b[0m Finished trial#10 with value: 0.598 with parameters: {'lr': 3.0115599764598007e-05, 'lamda': 2.6252318888926813e-05, 'batch_size': 128, 'epoch': 176, 'decay': 5}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:24,505]\u001b[0m Finished trial#11 with value: 0.6305 with parameters: {'lr': 0.0873033007064038, 'lamda': 0.7096014020532101, 'batch_size': 64, 'epoch': 488, 'decay': 7}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:28,489]\u001b[0m Finished trial#12 with value: 0.602 with parameters: {'lr': 0.09572771974790759, 'lamda': 0.0001700339179172046, 'batch_size': 128, 'epoch': 430, 'decay': 7}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:34,826]\u001b[0m Finished trial#13 with value: 0.604 with parameters: {'lr': 0.00018175386650145137, 'lamda': 9.561794158336064e-06, 'batch_size': 64, 'epoch': 495, 'decay': 5}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:40,407]\u001b[0m Finished trial#14 with value: 0.6305 with parameters: {'lr': 0.0276822286669408, 'lamda': 0.0013104419893789766, 'batch_size': 64, 'epoch': 436, 'decay': 6}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:44,447]\u001b[0m Finished trial#15 with value: 0.6125 with parameters: {'lr': 0.005271046388724207, 'lamda': 1.2954332442087817e-07, 'batch_size': 128, 'epoch': 435, 'decay': 6}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:48,178]\u001b[0m Finished trial#16 with value: 0.6365000000000001 with parameters: {'lr': 0.04538584415722637, 'lamda': 0.3645213125437948, 'batch_size': 128, 'epoch': 398, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:51,866]\u001b[0m Finished trial#17 with value: 0.606 with parameters: {'lr': 0.0002497735820320874, 'lamda': 1.7578628729790478, 'batch_size': 128, 'epoch': 398, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:55,254]\u001b[0m Finished trial#18 with value: 0.631 with parameters: {'lr': 0.00890139516612108, 'lamda': 0.20039059362792913, 'batch_size': 128, 'epoch': 355, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:37:59,056]\u001b[0m Finished trial#19 with value: 0.5905 with parameters: {'lr': 1.4730545621299684e-05, 'lamda': 5.595203591533896, 'batch_size': 128, 'epoch': 408, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:01,473]\u001b[0m Finished trial#20 with value: 0.6145 with parameters: {'lr': 0.002260320458300867, 'lamda': 1.252716889905685e-07, 'batch_size': 128, 'epoch': 252, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:07,446]\u001b[0m Finished trial#21 with value: 0.626 with parameters: {'lr': 0.054616213585836106, 'lamda': 0.0024189924665675875, 'batch_size': 64, 'epoch': 462, 'decay': 7}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:11,764]\u001b[0m Finished trial#22 with value: 0.6275000000000001 with parameters: {'lr': 0.03992403123378238, 'lamda': 0.3252056526779171, 'batch_size': 128, 'epoch': 462, 'decay': 8}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:18,218]\u001b[0m Finished trial#23 with value: 0.6244999999999999 with parameters: {'lr': 0.015698375942723473, 'lamda': 0.0001012495419138837, 'batch_size': 64, 'epoch': 500, 'decay': 8}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:22,468]\u001b[0m Finished trial#24 with value: 0.595 with parameters: {'lr': 0.06807814402119149, 'lamda': 0.06739716954694998, 'batch_size': 128, 'epoch': 459, 'decay': 6}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:26,241]\u001b[0m Finished trial#25 with value: 0.628 with parameters: {'lr': 0.004853130587494103, 'lamda': 0.0057863224653109824, 'batch_size': 128, 'epoch': 409, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:32,536]\u001b[0m Finished trial#26 with value: 0.6375 with parameters: {'lr': 0.015286755624204893, 'lamda': 1.9065416223364944, 'batch_size': 64, 'epoch': 498, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:35,856]\u001b[0m Finished trial#27 with value: 0.625 with parameters: {'lr': 0.015548264594704509, 'lamda': 2.231501135521526, 'batch_size': 128, 'epoch': 352, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:42,155]\u001b[0m Finished trial#28 with value: 0.6085 with parameters: {'lr': 0.002416307726095629, 'lamda': 1.7914515007785514, 'batch_size': 64, 'epoch': 498, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:47,015]\u001b[0m Finished trial#29 with value: 0.614 with parameters: {'lr': 0.09830147006957818, 'lamda': 2.0588374388192415e-06, 'batch_size': 64, 'epoch': 375, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:51,024]\u001b[0m Finished trial#30 with value: 0.623 with parameters: {'lr': 0.0006298941147321399, 'lamda': 8.959910933378515, 'batch_size': 128, 'epoch': 437, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:38:57,072]\u001b[0m Finished trial#31 with value: 0.6345000000000001 with parameters: {'lr': 0.04106199100726039, 'lamda': 0.17061522343323587, 'batch_size': 64, 'epoch': 472, 'decay': 5}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:39:03,056]\u001b[0m Finished trial#32 with value: 0.6194999999999999 with parameters: {'lr': 0.052946376784402345, 'lamda': 0.12985833173514183, 'batch_size': 64, 'epoch': 468, 'decay': 5}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:39:09,280]\u001b[0m Finished trial#33 with value: 0.6239999999999999 with parameters: {'lr': 0.01598138446862416, 'lamda': 0.8037776333274793, 'batch_size': 64, 'epoch': 485, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:39:17,619]\u001b[0m Finished trial#34 with value: 0.604 with parameters: {'lr': 0.010161207336648171, 'lamda': 0.0359686868280775, 'batch_size': 32, 'epoch': 421, 'decay': 5}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:39:24,020]\u001b[0m Finished trial#35 with value: 0.637 with parameters: {'lr': 0.040395702307799854, 'lamda': 9.686251159340898, 'batch_size': 64, 'epoch': 500, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:39:28,113]\u001b[0m Finished trial#36 with value: 0.6279999999999999 with parameters: {'lr': 0.021401302588433056, 'lamda': 9.551088306112401, 'batch_size': 64, 'epoch': 316, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:39:33,068]\u001b[0m Finished trial#37 with value: 0.622 with parameters: {'lr': 0.007218982180932111, 'lamda': 3.6677889624832716, 'batch_size': 64, 'epoch': 385, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:39:42,756]\u001b[0m Finished trial#38 with value: 0.6140000000000001 with parameters: {'lr': 0.037527901846232566, 'lamda': 0.540748623409, 'batch_size': 32, 'epoch': 497, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:39:48,526]\u001b[0m Finished trial#39 with value: 0.6305 with parameters: {'lr': 0.0032484074108562544, 'lamda': 8.193097782822534, 'batch_size': 64, 'epoch': 449, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:39:51,345]\u001b[0m Finished trial#40 with value: 0.6024999999999999 with parameters: {'lr': 0.001093550696967475, 'lamda': 1.1840732378318368, 'batch_size': 128, 'epoch': 290, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:39:57,505]\u001b[0m Finished trial#41 with value: 0.6290000000000001 with parameters: {'lr': 0.04983792846299264, 'lamda': 0.20348173748615742, 'batch_size': 64, 'epoch': 476, 'decay': 5}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:40:03,685]\u001b[0m Finished trial#42 with value: 0.615 with parameters: {'lr': 0.022372524822921697, 'lamda': 0.02573550215525094, 'batch_size': 64, 'epoch': 476, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:40:10,072]\u001b[0m Finished trial#43 with value: 0.6224999999999999 with parameters: {'lr': 0.07096501923541458, 'lamda': 3.410905468804683, 'batch_size': 64, 'epoch': 499, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:40:15,911]\u001b[0m Finished trial#44 with value: 0.6385 with parameters: {'lr': 0.03758025207272484, 'lamda': 0.4081626615306911, 'batch_size': 64, 'epoch': 451, 'decay': 5}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:40:21,860]\u001b[0m Finished trial#45 with value: 0.632 with parameters: {'lr': 0.013046129457094434, 'lamda': 0.45299814364668084, 'batch_size': 64, 'epoch': 451, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:40:30,175]\u001b[0m Finished trial#46 with value: 0.621 with parameters: {'lr': 0.025195816785599037, 'lamda': 0.06328606830783964, 'batch_size': 32, 'epoch': 418, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:40:32,051]\u001b[0m Finished trial#47 with value: 0.626 with parameters: {'lr': 0.09562594586390578, 'lamda': 0.0007929306696588223, 'batch_size': 64, 'epoch': 137, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:40:36,237]\u001b[0m Finished trial#48 with value: 0.627 with parameters: {'lr': 0.03203908359723696, 'lamda': 0.8818385500720607, 'batch_size': 128, 'epoch': 448, 'decay': 6}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:40:42,459]\u001b[0m Finished trial#49 with value: 0.6325000000000001 with parameters: {'lr': 0.0737661908394543, 'lamda': 0.01430021278139615, 'batch_size': 64, 'epoch': 484, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:40:46,181]\u001b[0m Finished trial#50 with value: 0.6265 with parameters: {'lr': 0.008296479168430881, 'lamda': 2.742536725054879e-06, 'batch_size': 128, 'epoch': 399, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:40:52,297]\u001b[0m Finished trial#51 with value: 0.629 with parameters: {'lr': 0.03620298934416497, 'lamda': 0.15380398777636511, 'batch_size': 64, 'epoch': 474, 'decay': 5}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:40:57,874]\u001b[0m Finished trial#52 with value: 0.6265 with parameters: {'lr': 0.055244202629848985, 'lamda': 3.5300991029464666e-07, 'batch_size': 64, 'epoch': 438, 'decay': 5}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:04,269]\u001b[0m Finished trial#53 with value: 0.6275000000000001 with parameters: {'lr': 0.02268966030420803, 'lamda': 0.3397076495346536, 'batch_size': 64, 'epoch': 498, 'decay': 5}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:08,737]\u001b[0m Finished trial#54 with value: 0.6279999999999999 with parameters: {'lr': 0.04227178100926009, 'lamda': 4.305247393333181, 'batch_size': 64, 'epoch': 349, 'decay': 6}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:14,923]\u001b[0m Finished trial#55 with value: 0.638 with parameters: {'lr': 0.01877647070109515, 'lamda': 0.08220034030469425, 'batch_size': 64, 'epoch': 483, 'decay': 5}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:19,429]\u001b[0m Finished trial#56 with value: 0.6234999999999999 with parameters: {'lr': 0.01918861612592772, 'lamda': 0.0742964810015803, 'batch_size': 128, 'epoch': 485, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:22,827]\u001b[0m Finished trial#57 with value: 0.6255 with parameters: {'lr': 0.010752557263623095, 'lamda': 1.4478993560677276, 'batch_size': 64, 'epoch': 260, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:27,085]\u001b[0m Finished trial#58 with value: 0.633 with parameters: {'lr': 0.006400602765760427, 'lamda': 2.702473055683764, 'batch_size': 128, 'epoch': 456, 'decay': 6}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:32,579]\u001b[0m Finished trial#59 with value: 0.623 with parameters: {'lr': 0.02908059334759265, 'lamda': 1.8655021234551203e-05, 'batch_size': 64, 'epoch': 426, 'decay': 4}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:36,770]\u001b[0m Finished trial#60 with value: 0.6165 with parameters: {'lr': 0.004341746081025862, 'lamda': 0.3046711929244767, 'batch_size': 128, 'epoch': 445, 'decay': 3}. Best is trial#7 with value: 0.64.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:39,547]\u001b[0m Finished trial#61 with value: 0.6449999999999999 with parameters: {'lr': 0.0667915036872918, 'lamda': 0.10240258158875458, 'batch_size': 64, 'epoch': 212, 'decay': 5}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:42,167]\u001b[0m Finished trial#62 with value: 0.6335000000000001 with parameters: {'lr': 0.06843645074404062, 'lamda': 0.01746550695821125, 'batch_size': 64, 'epoch': 193, 'decay': 5}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:44,762]\u001b[0m Finished trial#63 with value: 0.6355000000000001 with parameters: {'lr': 0.03269963919034029, 'lamda': 0.004001358088938705, 'batch_size': 64, 'epoch': 189, 'decay': 6}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:47,476]\u001b[0m Finished trial#64 with value: 0.6300000000000001 with parameters: {'lr': 0.09846383201286006, 'lamda': 0.07564627343318257, 'batch_size': 64, 'epoch': 205, 'decay': 5}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:50,431]\u001b[0m Finished trial#65 with value: 0.621 with parameters: {'lr': 0.013814225892837043, 'lamda': 0.633272305698523, 'batch_size': 64, 'epoch': 223, 'decay': 5}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:53,918]\u001b[0m Finished trial#66 with value: 0.6195 with parameters: {'lr': 0.00011175319452502274, 'lamda': 6.287685954186087, 'batch_size': 64, 'epoch': 265, 'decay': 4}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:41:58,687]\u001b[0m Finished trial#67 with value: 0.631 with parameters: {'lr': 0.05490252732680999, 'lamda': 0.03234762298684827, 'batch_size': 32, 'epoch': 239, 'decay': 7}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:00,189]\u001b[0m Finished trial#68 with value: 0.6285000000000001 with parameters: {'lr': 0.017783161802988574, 'lamda': 0.34276321430512297, 'batch_size': 128, 'epoch': 151, 'decay': 3}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:01,617]\u001b[0m Finished trial#69 with value: 0.61 with parameters: {'lr': 0.0016184087150104452, 'lamda': 0.11350622842655379, 'batch_size': 64, 'epoch': 101, 'decay': 4}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:05,788]\u001b[0m Finished trial#70 with value: 0.6285000000000001 with parameters: {'lr': 0.04797710982952, 'lamda': 0.9963347233234408, 'batch_size': 64, 'epoch': 321, 'decay': 10}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:08,894]\u001b[0m Finished trial#71 with value: 0.6305000000000001 with parameters: {'lr': 0.03005012630417522, 'lamda': 0.00398024334304384, 'batch_size': 64, 'epoch': 236, 'decay': 6}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:11,454]\u001b[0m Finished trial#72 with value: 0.6245 with parameters: {'lr': 0.07971974063434598, 'lamda': 0.0004232115326151044, 'batch_size': 64, 'epoch': 194, 'decay': 6}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:13,828]\u001b[0m Finished trial#73 with value: 0.6174999999999999 with parameters: {'lr': 0.0308281894745217, 'lamda': 0.002148252606640288, 'batch_size': 64, 'epoch': 180, 'decay': 5}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:15,765]\u001b[0m Finished trial#74 with value: 0.635 with parameters: {'lr': 0.012133520944059532, 'lamda': 0.00016310378566392, 'batch_size': 64, 'epoch': 139, 'decay': 7}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:18,049]\u001b[0m Finished trial#75 with value: 0.6365 with parameters: {'lr': 0.018731984614928456, 'lamda': 0.01345835798098685, 'batch_size': 64, 'epoch': 169, 'decay': 6}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:19,613]\u001b[0m Finished trial#76 with value: 0.642 with parameters: {'lr': 0.016719954836468678, 'lamda': 0.008186704917770016, 'batch_size': 128, 'epoch': 158, 'decay': 5}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:20,881]\u001b[0m Finished trial#77 with value: 0.6235 with parameters: {'lr': 0.05753851181416914, 'lamda': 0.04310053564834912, 'batch_size': 128, 'epoch': 127, 'decay': 5}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:22,501]\u001b[0m Finished trial#78 with value: 0.635 with parameters: {'lr': 0.009394380544679937, 'lamda': 0.22928274383922292, 'batch_size': 128, 'epoch': 162, 'decay': 3}. Best is trial#61 with value: 0.6449999999999999.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:27,177]\u001b[0m Finished trial#79 with value: 0.645 with parameters: {'lr': 0.025776300963657732, 'lamda': 0.009584792955448772, 'batch_size': 128, 'epoch': 489, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:31,761]\u001b[0m Finished trial#80 with value: 0.626 with parameters: {'lr': 0.022092596463476395, 'lamda': 0.005433935788134431, 'batch_size': 128, 'epoch': 491, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:36,334]\u001b[0m Finished trial#81 with value: 0.6359999999999999 with parameters: {'lr': 0.04264648563647675, 'lamda': 0.008707838554970263, 'batch_size': 128, 'epoch': 483, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:37,547]\u001b[0m Finished trial#82 with value: 0.6279999999999999 with parameters: {'lr': 0.024108632110516216, 'lamda': 0.0015912593144797726, 'batch_size': 128, 'epoch': 118, 'decay': 3}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:41,922]\u001b[0m Finished trial#83 with value: 0.6365000000000001 with parameters: {'lr': 0.017003751666106594, 'lamda': 0.10049328417388378, 'batch_size': 128, 'epoch': 468, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:47,459]\u001b[0m Finished trial#84 with value: 0.635 with parameters: {'lr': 0.015366729834769926, 'lamda': 0.0991252386842332, 'batch_size': 128, 'epoch': 465, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:52,254]\u001b[0m Finished trial#85 with value: 0.6385000000000001 with parameters: {'lr': 0.006795639978660286, 'lamda': 0.027276885646369146, 'batch_size': 128, 'epoch': 499, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:42:57,306]\u001b[0m Finished trial#86 with value: 0.6275 with parameters: {'lr': 0.007496208490102564, 'lamda': 0.02111871872035837, 'batch_size': 128, 'epoch': 492, 'decay': 3}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:03,734]\u001b[0m Finished trial#87 with value: 0.601 with parameters: {'lr': 0.0006904537596614919, 'lamda': 0.042018882500911965, 'batch_size': 128, 'epoch': 480, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:05,896]\u001b[0m Finished trial#88 with value: 0.6305 with parameters: {'lr': 0.012309691373538236, 'lamda': 0.01234690863877169, 'batch_size': 128, 'epoch': 211, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:15,876]\u001b[0m Finished trial#89 with value: 0.6214999999999999 with parameters: {'lr': 0.003931007373133172, 'lamda': 0.051201686780746915, 'batch_size': 32, 'epoch': 500, 'decay': 3}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:20,729]\u001b[0m Finished trial#90 with value: 0.6395000000000001 with parameters: {'lr': 0.006362195789354706, 'lamda': 0.027349987553281798, 'batch_size': 128, 'epoch': 500, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:25,459]\u001b[0m Finished trial#91 with value: 0.629 with parameters: {'lr': 0.025752739493045525, 'lamda': 0.0067338209681214135, 'batch_size': 128, 'epoch': 500, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:30,045]\u001b[0m Finished trial#92 with value: 0.6219999999999999 with parameters: {'lr': 0.006233173113135542, 'lamda': 0.02487197255105148, 'batch_size': 128, 'epoch': 490, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:34,306]\u001b[0m Finished trial#93 with value: 0.6300000000000001 with parameters: {'lr': 0.01000073750287238, 'lamda': 0.0008633675154071068, 'batch_size': 128, 'epoch': 461, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:38,653]\u001b[0m Finished trial#94 with value: 0.6345 with parameters: {'lr': 0.005286421304783483, 'lamda': 0.011152980800825043, 'batch_size': 128, 'epoch': 474, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:43,271]\u001b[0m Finished trial#95 with value: 0.6275000000000001 with parameters: {'lr': 0.036163776833869885, 'lamda': 0.21317253963435115, 'batch_size': 128, 'epoch': 491, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:47,810]\u001b[0m Finished trial#96 with value: 0.6335 with parameters: {'lr': 0.014850903524336082, 'lamda': 0.03187784888250149, 'batch_size': 128, 'epoch': 482, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:50,656]\u001b[0m Finished trial#97 with value: 0.6214999999999999 with parameters: {'lr': 0.06356495274806329, 'lamda': 0.05775296510855414, 'batch_size': 128, 'epoch': 302, 'decay': 3}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:43:56,394]\u001b[0m Finished trial#98 with value: 0.641 with parameters: {'lr': 0.027726534200731474, 'lamda': 1.949962477731034, 'batch_size': 64, 'epoch': 455, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:44:02,025]\u001b[0m Finished trial#99 with value: 0.642 with parameters: {'lr': 0.019073950520274788, 'lamda': 1.6354204201014844, 'batch_size': 64, 'epoch': 438, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:44:06,133]\u001b[0m Finished trial#100 with value: 0.635 with parameters: {'lr': 0.0025378244045213302, 'lamda': 0.021340261693896093, 'batch_size': 128, 'epoch': 441, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:44:11,826]\u001b[0m Finished trial#101 with value: 0.6319999999999999 with parameters: {'lr': 0.020326998241929156, 'lamda': 2.1349338940300457, 'batch_size': 64, 'epoch': 453, 'decay': 6}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:44:17,613]\u001b[0m Finished trial#102 with value: 0.629 with parameters: {'lr': 0.0271855858205995, 'lamda': 0.5295876069536353, 'batch_size': 64, 'epoch': 457, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:44:23,581]\u001b[0m Finished trial#103 with value: 0.6220000000000001 with parameters: {'lr': 0.010443763843548014, 'lamda': 1.127398848483981, 'batch_size': 64, 'epoch': 468, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:44:28,729]\u001b[0m Finished trial#104 with value: 0.616 with parameters: {'lr': 0.019608929725155537, 'lamda': 0.7104226731705549, 'batch_size': 64, 'epoch': 411, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:44:34,249]\u001b[0m Finished trial#105 with value: 0.6204999999999999 with parameters: {'lr': 0.008525882201352518, 'lamda': 1.6553041695252477, 'batch_size': 64, 'epoch': 429, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:44:38,531]\u001b[0m Finished trial#106 with value: 0.6280000000000001 with parameters: {'lr': 0.03426529587621824, 'lamda': 0.14079191677987546, 'batch_size': 64, 'epoch': 337, 'decay': 6}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:44:44,674]\u001b[0m Finished trial#107 with value: 0.6365 with parameters: {'lr': 0.013519258948123315, 'lamda': 4.893444886593402, 'batch_size': 64, 'epoch': 475, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:44:50,163]\u001b[0m Finished trial#108 with value: 0.6275000000000001 with parameters: {'lr': 0.027161502128321614, 'lamda': 0.2834385865269508, 'batch_size': 64, 'epoch': 435, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:44:54,529]\u001b[0m Finished trial#109 with value: 0.6240000000000001 with parameters: {'lr': 0.04594260725241375, 'lamda': 0.08550249221838502, 'batch_size': 128, 'epoch': 448, 'decay': 6}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:45:00,336]\u001b[0m Finished trial#110 with value: 0.5984999999999999 with parameters: {'lr': 0.0015067274131336365, 'lamda': 3.0534485157280336, 'batch_size': 64, 'epoch': 459, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:45:07,104]\u001b[0m Finished trial#111 with value: 0.6305 with parameters: {'lr': 0.03867962843869956, 'lamda': 6.482195215402885, 'batch_size': 64, 'epoch': 499, 'decay': 3}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:45:13,282]\u001b[0m Finished trial#112 with value: 0.627 with parameters: {'lr': 0.08598982620440411, 'lamda': 0.47449399826753086, 'batch_size': 64, 'epoch': 487, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:45:16,715]\u001b[0m Finished trial#113 with value: 0.617 with parameters: {'lr': 0.016883213579286016, 'lamda': 1.3595466219085188, 'batch_size': 64, 'epoch': 272, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:45:23,112]\u001b[0m Finished trial#114 with value: 0.626 with parameters: {'lr': 0.024909961418023707, 'lamda': 0.017415167848164104, 'batch_size': 64, 'epoch': 500, 'decay': 8}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:45:29,219]\u001b[0m Finished trial#115 with value: 0.6214999999999999 with parameters: {'lr': 0.047573807722487226, 'lamda': 2.4326724050180015, 'batch_size': 64, 'epoch': 479, 'decay': 3}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:45:35,266]\u001b[0m Finished trial#116 with value: 0.6060000000000001 with parameters: {'lr': 0.003353301791986858, 'lamda': 0.004154395803906147, 'batch_size': 64, 'epoch': 467, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:45:44,825]\u001b[0m Finished trial#117 with value: 0.6210000000000001 with parameters: {'lr': 0.02170821135299313, 'lamda': 0.15514361365673798, 'batch_size': 32, 'epoch': 493, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:45:48,235]\u001b[0m Finished trial#118 with value: 0.6285 with parameters: {'lr': 0.01175276303917043, 'lamda': 0.007330036088358381, 'batch_size': 128, 'epoch': 367, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:45:54,246]\u001b[0m Finished trial#119 with value: 0.627 with parameters: {'lr': 0.030217282299391756, 'lamda': 0.0028967493877271656, 'batch_size': 64, 'epoch': 476, 'decay': 6}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:45:58,271]\u001b[0m Finished trial#120 with value: 0.633 with parameters: {'lr': 0.06102811331395252, 'lamda': 0.7633965059701889, 'batch_size': 128, 'epoch': 421, 'decay': 3}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:02,267]\u001b[0m Finished trial#121 with value: 0.6359999999999999 with parameters: {'lr': 0.037079730665650415, 'lamda': 0.3708524512677741, 'batch_size': 128, 'epoch': 396, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:08,392]\u001b[0m Finished trial#122 with value: 0.6305 with parameters: {'lr': 0.01688596275757169, 'lamda': 0.10699059884313934, 'batch_size': 128, 'epoch': 467, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:12,840]\u001b[0m Finished trial#123 with value: 0.6329999999999999 with parameters: {'lr': 0.018166398520806606, 'lamda': 0.03575096448602314, 'batch_size': 128, 'epoch': 486, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:14,104]\u001b[0m Finished trial#124 with value: 0.63 with parameters: {'lr': 0.04401064705768239, 'lamda': 3.481570981155562, 'batch_size': 128, 'epoch': 111, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:18,218]\u001b[0m Finished trial#125 with value: 0.633 with parameters: {'lr': 0.006733312527887609, 'lamda': 0.06528175397847155, 'batch_size': 128, 'epoch': 445, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:22,523]\u001b[0m Finished trial#126 with value: 0.626 with parameters: {'lr': 0.022549233649175004, 'lamda': 9.156041412642084, 'batch_size': 128, 'epoch': 472, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:28,799]\u001b[0m Finished trial#127 with value: 0.6279999999999999 with parameters: {'lr': 0.03346309671333383, 'lamda': 0.2795905119100807, 'batch_size': 64, 'epoch': 499, 'decay': 3}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:34,232]\u001b[0m Finished trial#128 with value: 0.631 with parameters: {'lr': 0.0717230475041013, 'lamda': 0.9803170056490533, 'batch_size': 64, 'epoch': 413, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:41,677]\u001b[0m Finished trial#129 with value: 0.6289999999999999 with parameters: {'lr': 0.056892583820046166, 'lamda': 4.980050290452478e-06, 'batch_size': 128, 'epoch': 456, 'decay': 3}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:49,887]\u001b[0m Finished trial#130 with value: 0.6305 with parameters: {'lr': 0.013940746229577711, 'lamda': 8.808988542797918e-07, 'batch_size': 64, 'epoch': 403, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:56,340]\u001b[0m Finished trial#131 with value: 0.6184999999999999 with parameters: {'lr': 0.029683805366108273, 'lamda': 4.961707445727037e-05, 'batch_size': 128, 'epoch': 464, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:46:59,865]\u001b[0m Finished trial#132 with value: 0.628 with parameters: {'lr': 0.01618501520719507, 'lamda': 0.16581852743142075, 'batch_size': 128, 'epoch': 386, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:47:04,290]\u001b[0m Finished trial#133 with value: 0.6315 with parameters: {'lr': 0.020747752364521664, 'lamda': 0.049783329951705, 'batch_size': 128, 'epoch': 484, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:47:11,227]\u001b[0m Finished trial#134 with value: 0.628 with parameters: {'lr': 0.04861298873322727, 'lamda': 0.10451476841518663, 'batch_size': 128, 'epoch': 431, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:47:19,560]\u001b[0m Finished trial#135 with value: 0.6275 with parameters: {'lr': 0.02617077775906693, 'lamda': 0.010755968838787782, 'batch_size': 128, 'epoch': 494, 'decay': 3}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:47:26,435]\u001b[0m Finished trial#136 with value: 0.6295 with parameters: {'lr': 0.04024955322135417, 'lamda': 0.0240924393993984, 'batch_size': 128, 'epoch': 444, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:47:36,012]\u001b[0m Finished trial#137 with value: 0.604 with parameters: {'lr': 0.008862221772844436, 'lamda': 0.4494516059156941, 'batch_size': 64, 'epoch': 480, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:47:44,051]\u001b[0m Finished trial#138 with value: 0.6405 with parameters: {'lr': 0.0138779221044331, 'lamda': 0.015621244527092919, 'batch_size': 128, 'epoch': 500, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:47:51,885]\u001b[0m Finished trial#139 with value: 0.643 with parameters: {'lr': 0.011183555925575725, 'lamda': 0.015138854212083348, 'batch_size': 128, 'epoch': 492, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:47:54,780]\u001b[0m Finished trial#140 with value: 0.6275000000000001 with parameters: {'lr': 0.011604191085572079, 'lamda': 0.014831396503339195, 'batch_size': 64, 'epoch': 154, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:48:02,043]\u001b[0m Finished trial#141 with value: 0.6174999999999999 with parameters: {'lr': 0.010609262785091415, 'lamda': 0.005125104536912039, 'batch_size': 128, 'epoch': 491, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:48:08,972]\u001b[0m Finished trial#142 with value: 0.619 with parameters: {'lr': 0.013734509120555525, 'lamda': 0.03484381481892871, 'batch_size': 128, 'epoch': 473, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:48:16,970]\u001b[0m Finished trial#143 with value: 0.629 with parameters: {'lr': 0.007735620062975407, 'lamda': 0.007252294339591512, 'batch_size': 128, 'epoch': 499, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:48:24,548]\u001b[0m Finished trial#144 with value: 0.623 with parameters: {'lr': 0.0161285981616192, 'lamda': 0.01896673841145619, 'batch_size': 128, 'epoch': 500, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:48:31,884]\u001b[0m Finished trial#145 with value: 0.6345 with parameters: {'lr': 0.01297941254002591, 'lamda': 0.009115728147493774, 'batch_size': 128, 'epoch': 487, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:48:41,984]\u001b[0m Finished trial#146 with value: 0.619 with parameters: {'lr': 0.005692327932571578, 'lamda': 0.024716043356844665, 'batch_size': 64, 'epoch': 500, 'decay': 3}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:48:49,716]\u001b[0m Finished trial#147 with value: 0.6245 with parameters: {'lr': 0.021358003920327443, 'lamda': 0.05343295293994301, 'batch_size': 128, 'epoch': 481, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:05,122]\u001b[0m Finished trial#148 with value: 0.6165 with parameters: {'lr': 0.009422015634262813, 'lamda': 0.015072931520989307, 'batch_size': 32, 'epoch': 464, 'decay': 4}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:15,880]\u001b[0m Finished trial#149 with value: 0.607 with parameters: {'lr': 0.0046711647068297234, 'lamda': 1.7887988877099572, 'batch_size': 64, 'epoch': 491, 'decay': 5}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:23,013]\u001b[0m Finished trial#150 with value: 0.6259999999999999 with parameters: {'lr': 0.026389362677380145, 'lamda': 0.0015053234688541446, 'batch_size': 128, 'epoch': 493, 'decay': 6}. Best is trial#79 with value: 0.645.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:27,282]\u001b[0m Finished trial#151 with value: 0.653 with parameters: {'lr': 0.018609760329893216, 'lamda': 0.09259373377015465, 'batch_size': 128, 'epoch': 471, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:31,361]\u001b[0m Finished trial#152 with value: 0.63 with parameters: {'lr': 0.0327869930288931, 'lamda': 0.07352441632113467, 'batch_size': 128, 'epoch': 456, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:35,725]\u001b[0m Finished trial#153 with value: 0.6359999999999999 with parameters: {'lr': 0.01849325284265909, 'lamda': 0.03563579464762558, 'batch_size': 128, 'epoch': 474, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:39,985]\u001b[0m Finished trial#154 with value: 0.628 with parameters: {'lr': 0.014531585159247918, 'lamda': 0.10888917409042834, 'batch_size': 128, 'epoch': 468, 'decay': 3}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:42,034]\u001b[0m Finished trial#155 with value: 0.626 with parameters: {'lr': 0.02393895957235839, 'lamda': 0.21048973588034, 'batch_size': 128, 'epoch': 218, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:48,056]\u001b[0m Finished trial#156 with value: 0.623 with parameters: {'lr': 0.04047385173185811, 'lamda': 0.6111050118130806, 'batch_size': 64, 'epoch': 482, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:49,744]\u001b[0m Finished trial#157 with value: 0.6355 with parameters: {'lr': 0.018980266081833116, 'lamda': 4.517829099716791, 'batch_size': 128, 'epoch': 178, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:53,848]\u001b[0m Finished trial#158 with value: 0.6300000000000001 with parameters: {'lr': 0.011673249209825024, 'lamda': 0.07597317363078225, 'batch_size': 128, 'epoch': 461, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:49:59,402]\u001b[0m Finished trial#159 with value: 0.6285000000000001 with parameters: {'lr': 0.029600112984433238, 'lamda': 0.02722184658622169, 'batch_size': 64, 'epoch': 441, 'decay': 3}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:50:05,262]\u001b[0m Finished trial#160 with value: 0.628 with parameters: {'lr': 0.007448281728824693, 'lamda': 0.011028539811282913, 'batch_size': 64, 'epoch': 450, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:50:09,792]\u001b[0m Finished trial#161 with value: 0.63 with parameters: {'lr': 0.015893335463431194, 'lamda': 0.14284682097930515, 'batch_size': 128, 'epoch': 500, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:50:19,304]\u001b[0m Finished trial#162 with value: 0.6329999999999999 with parameters: {'lr': 0.02149906201393322, 'lamda': 0.05022853678514917, 'batch_size': 128, 'epoch': 487, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:50:28,253]\u001b[0m Finished trial#163 with value: 0.6085 with parameters: {'lr': 0.017631334360441408, 'lamda': 0.0920001824594951, 'batch_size': 128, 'epoch': 475, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:50:36,239]\u001b[0m Finished trial#164 with value: 0.64 with parameters: {'lr': 0.03368366843810809, 'lamda': 0.2354595164294943, 'batch_size': 128, 'epoch': 453, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:50:45,331]\u001b[0m Finished trial#165 with value: 0.615 with parameters: {'lr': 0.05420066261998476, 'lamda': 0.22037172708005567, 'batch_size': 128, 'epoch': 426, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:50:54,549]\u001b[0m Finished trial#166 with value: 0.6175 with parameters: {'lr': 0.036022472511224965, 'lamda': 0.40277975073752315, 'batch_size': 128, 'epoch': 454, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:51:03,935]\u001b[0m Finished trial#167 with value: 0.6375 with parameters: {'lr': 0.024583443202394785, 'lamda': 0.13892297952867486, 'batch_size': 128, 'epoch': 467, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:51:13,907]\u001b[0m Finished trial#168 with value: 0.6325 with parameters: {'lr': 0.02480170581084659, 'lamda': 0.16266410069837187, 'batch_size': 128, 'epoch': 481, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:51:27,738]\u001b[0m Finished trial#169 with value: 0.6365000000000001 with parameters: {'lr': 0.027733565270024303, 'lamda': 0.2920032844430239, 'batch_size': 64, 'epoch': 492, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:51:31,902]\u001b[0m Finished trial#170 with value: 0.6325 with parameters: {'lr': 0.032700263096303354, 'lamda': 0.0005475452929951184, 'batch_size': 128, 'epoch': 462, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:51:40,208]\u001b[0m Finished trial#171 with value: 0.6285000000000001 with parameters: {'lr': 0.025545578160080133, 'lamda': 0.2811606170853698, 'batch_size': 64, 'epoch': 490, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:51:50,305]\u001b[0m Finished trial#172 with value: 0.622 with parameters: {'lr': 0.030236402608090837, 'lamda': 1.1644062862890745, 'batch_size': 64, 'epoch': 500, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:51:57,260]\u001b[0m Finished trial#173 with value: 0.6365000000000001 with parameters: {'lr': 0.04383972580202885, 'lamda': 0.5893676227833483, 'batch_size': 128, 'epoch': 437, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:52:04,005]\u001b[0m Finished trial#174 with value: 0.627 with parameters: {'lr': 0.046727386324671674, 'lamda': 2.355397975580239, 'batch_size': 128, 'epoch': 448, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:52:10,464]\u001b[0m Finished trial#175 with value: 0.6025 with parameters: {'lr': 0.06514198191281073, 'lamda': 0.8568494005976234, 'batch_size': 128, 'epoch': 434, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:52:17,876]\u001b[0m Finished trial#176 with value: 0.6305 with parameters: {'lr': 0.01940592245131816, 'lamda': 0.04487695019784986, 'batch_size': 128, 'epoch': 479, 'decay': 3}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:52:28,073]\u001b[0m Finished trial#177 with value: 0.6325000000000001 with parameters: {'lr': 0.02269513110004929, 'lamda': 0.24233899233564402, 'batch_size': 64, 'epoch': 494, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:52:34,563]\u001b[0m Finished trial#178 with value: 0.622 with parameters: {'lr': 0.014252621336343804, 'lamda': 0.13475704653809598, 'batch_size': 128, 'epoch': 469, 'decay': 6}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:52:41,611]\u001b[0m Finished trial#179 with value: 0.626 with parameters: {'lr': 0.03650360320299952, 'lamda': 0.06714833286682509, 'batch_size': 128, 'epoch': 455, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:52:48,801]\u001b[0m Finished trial#180 with value: 0.621 with parameters: {'lr': 0.010119578325700474, 'lamda': 0.02018114269768917, 'batch_size': 128, 'epoch': 469, 'decay': 3}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:52:55,238]\u001b[0m Finished trial#181 with value: 0.6325000000000001 with parameters: {'lr': 0.044044996116497814, 'lamda': 0.6202928656764141, 'batch_size': 128, 'epoch': 433, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:53:00,692]\u001b[0m Finished trial#182 with value: 0.6305000000000001 with parameters: {'lr': 0.05253033161741399, 'lamda': 0.39838472820443693, 'batch_size': 128, 'epoch': 440, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:53:10,666]\u001b[0m Finished trial#183 with value: 0.584 with parameters: {'lr': 1.1298554274443069e-05, 'lamda': 1.5912007752399908, 'batch_size': 64, 'epoch': 489, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:53:17,157]\u001b[0m Finished trial#184 with value: 0.599 with parameters: {'lr': 0.08001845283370447, 'lamda': 0.48486076890449026, 'batch_size': 128, 'epoch': 448, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:53:23,961]\u001b[0m Finished trial#185 with value: 0.596 with parameters: {'lr': 0.0002950648817794057, 'lamda': 0.11882866524151427, 'batch_size': 128, 'epoch': 460, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:53:28,892]\u001b[0m Finished trial#186 with value: 0.6234999999999999 with parameters: {'lr': 0.016306898129523732, 'lamda': 0.16943061010131022, 'batch_size': 128, 'epoch': 311, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:53:38,157]\u001b[0m Finished trial#187 with value: 0.617 with parameters: {'lr': 0.02867876201678236, 'lamda': 0.08923850906758474, 'batch_size': 64, 'epoch': 500, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:53:44,311]\u001b[0m Finished trial#188 with value: 0.6205 with parameters: {'lr': 0.020125995290534035, 'lamda': 0.032060903625472974, 'batch_size': 64, 'epoch': 483, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:53:50,553]\u001b[0m Finished trial#189 with value: 0.6285000000000001 with parameters: {'lr': 0.0329503283289904, 'lamda': 0.2989949929689695, 'batch_size': 64, 'epoch': 500, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:53:57,240]\u001b[0m Finished trial#190 with value: 0.6195 with parameters: {'lr': 0.037867548784594224, 'lamda': 0.009743332566744154, 'batch_size': 64, 'epoch': 477, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:54:04,133]\u001b[0m Finished trial#191 with value: 0.6214999999999999 with parameters: {'lr': 0.02501758398231456, 'lamda': 0.22577588786343283, 'batch_size': 64, 'epoch': 469, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:54:08,113]\u001b[0m Finished trial#192 with value: 0.6305000000000001 with parameters: {'lr': 0.013262135341143243, 'lamda': 0.015536881486814829, 'batch_size': 128, 'epoch': 420, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:54:10,872]\u001b[0m Finished trial#193 with value: 0.6345 with parameters: {'lr': 0.018142947903087358, 'lamda': 0.12484260041831406, 'batch_size': 32, 'epoch': 137, 'decay': 4}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:54:15,318]\u001b[0m Finished trial#194 with value: 0.6395 with parameters: {'lr': 0.04726851357869887, 'lamda': 0.6674280926973849, 'batch_size': 128, 'epoch': 493, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:54:19,915]\u001b[0m Finished trial#195 with value: 0.65 with parameters: {'lr': 0.04226589107329626, 'lamda': 0.9818316204593063, 'batch_size': 128, 'epoch': 491, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:54:22,117]\u001b[0m Finished trial#196 with value: 0.5955 with parameters: {'lr': 0.0652883311498403, 'lamda': 0.9467227639995464, 'batch_size': 128, 'epoch': 237, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:54:28,308]\u001b[0m Finished trial#197 with value: 0.627 with parameters: {'lr': 0.028041084554547298, 'lamda': 1.6173582213613487, 'batch_size': 64, 'epoch': 487, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:54:31,494]\u001b[0m Finished trial#198 with value: 0.6249999999999999 with parameters: {'lr': 0.05520349515706506, 'lamda': 1.3029209786562785, 'batch_size': 128, 'epoch': 335, 'decay': 6}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\u001b[32m[I 2020-05-28 11:54:36,040]\u001b[0m Finished trial#199 with value: 0.62 with parameters: {'lr': 0.04141701289636205, 'lamda': 3.1209739465461253, 'batch_size': 128, 'epoch': 500, 'decay': 5}. Best is trial#151 with value: 0.653.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 0.1)\n",
    "    lamda = trial.suggest_loguniform('lamda', 1e-7, 10)\n",
    "#     k =  trial.suggest_categorical('k', [4,5,8,10])\n",
    "    batch_size =  trial.suggest_categorical('batch_size', [32,64,128])\n",
    "    epoch =  trial.suggest_int('epoch', 100, 500)\n",
    "    decay = trial.suggest_int('decay', 3, 10)\n",
    "    return cross_validate(np.array(X)[:2000,:], y.values,lr=lr,lamda=lamda,batch_size=batch_size,k=5,epoch=epoch,decay=decay)\n",
    "# cross_validate(X_preprocess, y.reshape(-1,1),lr=0.001,epoch=200)\n",
    "\n",
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "study.optimize(func=objective, n_trials=200,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_batch_size</th>\n",
       "      <th>params_decay</th>\n",
       "      <th>params_epoch</th>\n",
       "      <th>params_lamda</th>\n",
       "      <th>params_lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>183</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>00:00:09.966112</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>489</td>\n",
       "      <td>1.591201</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>00:00:01.465565</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>105</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.5905</td>\n",
       "      <td>00:00:03.787736</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>408</td>\n",
       "      <td>5.595204</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>00:00:04.239923</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>459</td>\n",
       "      <td>0.067397</td>\n",
       "      <td>0.068078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>00:00:02.195486</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>237</td>\n",
       "      <td>0.946723</td>\n",
       "      <td>0.065288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>139</td>\n",
       "      <td>0.6430</td>\n",
       "      <td>00:00:07.828468</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>492</td>\n",
       "      <td>0.015139</td>\n",
       "      <td>0.011184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>0.6450</td>\n",
       "      <td>00:00:02.771327</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>212</td>\n",
       "      <td>0.102403</td>\n",
       "      <td>0.066792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>0.6450</td>\n",
       "      <td>00:00:04.668814</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>489</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>0.025776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>00:00:04.591107</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>491</td>\n",
       "      <td>0.981832</td>\n",
       "      <td>0.042266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>151</td>\n",
       "      <td>0.6530</td>\n",
       "      <td>00:00:04.260943</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>471</td>\n",
       "      <td>0.092594</td>\n",
       "      <td>0.018610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     number   value        duration  ...  params_epoch  params_lamda  params_lr\n",
       "183     183  0.5840 00:00:09.966112  ...           489      1.591201   0.000011\n",
       "4         4  0.5900 00:00:01.465565  ...           105      0.000322   0.000229\n",
       "19       19  0.5905 00:00:03.787736  ...           408      5.595204   0.000015\n",
       "24       24  0.5950 00:00:04.239923  ...           459      0.067397   0.068078\n",
       "196     196  0.5955 00:00:02.195486  ...           237      0.946723   0.065288\n",
       "..      ...     ...             ...  ...           ...           ...        ...\n",
       "139     139  0.6430 00:00:07.828468  ...           492      0.015139   0.011184\n",
       "61       61  0.6450 00:00:02.771327  ...           212      0.102403   0.066792\n",
       "79       79  0.6450 00:00:04.668814  ...           489      0.009585   0.025776\n",
       "195     195  0.6500 00:00:04.591107  ...           491      0.981832   0.042266\n",
       "151     151  0.6530 00:00:04.260943  ...           471      0.092594   0.018610\n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "df.sort_values(by=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamda=0.092594,lr=0.018610,decay=4,epoch=500,batch_size=128,print_every=None\n",
    "\n",
    "\n",
    "list_in = np.array(list(X_train_.flatten())+list(X_test_.flatten()))\n",
    "list_in.astype(type(X_train_))\n",
    "list_in = list_in.reshape(-1,1)\n",
    "list_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.924"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DNA sequence as a “language”, known as k-mer counting\n",
    "def getKmers(sequence, size):\n",
    "    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]\n",
    "def get_n_grams(data1,n):\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "\n",
    "    cv = CountVectorizer(analyzer='char',ngram_range=(n,n))\n",
    "    for i in data1:\n",
    "        sentence = ' '.join(getKmers(i[0], size=n))\n",
    "        X_train.append(sentence)\n",
    "        \n",
    "    X_cocat = X_train\n",
    "    X = cv.fit_transform(X_cocat).toarray()\n",
    "    return X\n",
    "\n",
    "X_preprocess = get_n_grams(X_train_,7)\n",
    "\n",
    "# def objective(trial):\n",
    "#     lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "#     lamda = trial.suggest_loguniform('lamda', 0.01, 0.5)\n",
    "#     k =  trial.suggest_categorical('k', [4,5,8,10])\n",
    "#     epoch =  trial.suggest_int('epoch', 10, 20)\n",
    "#     decay = trial.suggest_int('decay', 3, 10)\n",
    "#     return cross_validate(X_preprocess[:2000,:], y,lr=lr,lamda=lamda,k=k,epoch=epoch,decay=decay)\n",
    "\n",
    "cross_validate(X_preprocess[:2000], y,0.001,20,k=5,epoch=10,decay=10)\n",
    "\n",
    "# import optuna\n",
    "\n",
    "# sampler = optuna.samplers.TPESampler()\n",
    "# study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "# study.optimize(func=objective, n_trials=100,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "# df.sort_values(by=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count Vectorizer \n",
    "# def get_count_grams(data1,n):\n",
    "#     cv = CountVectorizer(analyzer='char',ngram_range=(n,n))\n",
    "#     X = cv.fit_transform(data1).toarray()\n",
    "#     return X\n",
    "\n",
    "# X_preprocess = get_n_grams(X_train_.flatten(),8)\n",
    "\n",
    "# # def objective(trial):\n",
    "# #     lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "# #     lamda = trial.suggest_loguniform('lamda', 0.01, 0.5)\n",
    "# #     k =  trial.suggest_categorical('k', [4,5,8,10])\n",
    "# #     epoch =  trial.suggest_int('epoch', 10, 20)\n",
    "# #     decay = trial.suggest_int('decay', 3, 10)\n",
    "# #     return cross_validate(X_preprocess, y,lr=lr,lamda=lamda,k=k,epoch=epoch,decay=decay)\n",
    "\n",
    "# cross_validate(X_preprocess[:2000], y,0.0001,20,k=5,epoch=10,decay=10)\n",
    "\n",
    "# # import optuna\n",
    "\n",
    "# # sampler = optuna.samplers.TPESampler()\n",
    "# # study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "# # study.optimize(func=objective, n_trials=200,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "# df.sort_values(by=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate(X_preprocess, y,lr=0.004433,lamda=0.432127,k=4,epoch=16,decay=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer \n",
    "def get_tf_idf_grams(data1,n):\n",
    "    cv = TfidfVectorizer(analyzer='char',ngram_range=(n,n))\n",
    "    X = cv.fit_transform(data1).toarray()\n",
    "    return X\n",
    "\n",
    "X_preprocess = get_tf_idf_grams(X_train_.flatten(),8)\n",
    "\n",
    "# def objective(trial):\n",
    "#     lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "#     lamda = trial.suggest_loguniform('lamda', 0.01, 0.5)\n",
    "#     k =  trial.suggest_categorical('k', [4,5,8,10])\n",
    "#     epoch =  trial.suggest_int('epoch', 10, 20)\n",
    "#     decay = trial.suggest_int('decay', 3, 10)\n",
    "#     return cross_validate(X_preprocess, y,lr=lr,lamda=lamda,k=k,epoch=epoch,decay=decay)\n",
    "\n",
    "cross_validate(X_preprocess[:2000], y,0.001,20,k=5,epoch=10,decay=10)\n",
    "\n",
    "# import optuna\n",
    "\n",
    "# sampler = optuna.samplers.TPESampler()\n",
    "# study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "# study.optimize(func=objective, n_trials=100,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "# df.sort_values(by=['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After testing all possible dataset preprocessing type now lets stick to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test(X_preprocess[:2000],y,0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980, 1500) (20, 1500) (1980, 1) (20, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in multiply\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7767676767676768\n",
      "0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_preprocess = get_n_grams(X_train_,8)\n",
    "# X_preprocess.shape\n",
    "\n",
    "# print(cross_validate(X_preprocess[:2000,:], y,lr=0.001,lamda=15,k=4,epoch=16,decay=10))\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test(np.array(X)[:2000,:],y.values,0.01)\n",
    "\n",
    "# y,0.001,15,k=5,epoch=10,decay=10)\n",
    "\n",
    "logistic = logisticregression(X_train,y_train,lamda=0.092594,lr=0.018610,decay=4,epoch=500,batch_size=128,print_every=None)\n",
    "logistic.train()\n",
    "        \n",
    "print(logistic.evaluate(X_train,y_train))\n",
    "print(logistic.evaluate(X_test,y_test))\n",
    "cross_validate(np.array(X)[:2000,:], y.values,lamda=0.092594,lr=0.018610,decay=4,epoch=500,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 4096) (600, 4096) (1400, 1) (600, 1)\n",
      "Epoch : 0  Loss: 0.6931471805599454\n",
      "Epoch : 1  Loss: 0.369619629360289\n",
      "Epoch : 2  Loss: 0.24093457515641262\n",
      "Epoch : 3  Loss: 0.23515736618853034\n",
      "Epoch : 4  Loss: 0.2349297795537864\n",
      "Epoch : 5  Loss: 0.23492310904273742\n",
      "Epoch : 6  Loss: 0.23492296082415293\n",
      "Epoch : 7  Loss: 0.23492295817739842\n",
      "Epoch : 8  Loss: 0.23492295813789463\n",
      "Epoch : 9  Loss: 0.2349229581373882\n",
      "0.9664285714285714\n",
      "0.6433333333333333\n"
     ]
    }
   ],
   "source": [
    "X_preprocess = get_count_grams(np.vstack((X_train_,X_test_)).flatten(),6)\n",
    "X_preprocess.shape\n",
    "\n",
    "cross_validate(X_preprocess[:2000,:], y,lr=0.004433,lamda=0.432127,k=4,epoch=16,decay=4)\n",
    "\n",
    "C_count_6 = X_preprocess[2000:,:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test(X_preprocess[:2000,:],y,0.3)\n",
    "\n",
    "\n",
    "logistic_count6 = logisticregression(X_train,y_train,lamda=0.455265,epoch=10,print_every=1,lr=0.000407,decay=11)\n",
    "logistic_count6.train()\n",
    "        \n",
    "print(logistic_count6.evaluate(X_train,y_train))\n",
    "print(logistic_count6.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 256) (600, 256) (1400, 1) (600, 1)\n",
      "Epoch : 0  Loss: 0.6931471805599454\n",
      "Epoch : 1  Loss: 0.6286425932017687\n",
      "Epoch : 2  Loss: 0.6042153578172307\n",
      "Epoch : 3  Loss: 0.6027152714606171\n",
      "Epoch : 4  Loss: 0.6026532220481413\n",
      "Epoch : 5  Loss: 0.6026514005159469\n",
      "Epoch : 6  Loss: 0.6026513600396904\n",
      "Epoch : 7  Loss: 0.6026513593169011\n",
      "Epoch : 8  Loss: 0.6026513593061132\n",
      "Epoch : 9  Loss: 0.6026513593059748\n",
      "0.6907142857142857\n",
      "0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in multiply\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_preprocess = get_count_grams(np.vstack((X_train_,X_test_)).flatten(),4)\n",
    "X_preprocess.shape\n",
    "\n",
    "print(cross_validate(X_preprocess[:2000,:], y,lr=0.004433,lamda=0.432127,k=4,epoch=16,decay=7))\n",
    "\n",
    "C_count_4 = X_preprocess[2000:,:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test(X_preprocess[:2000,:],y,0.3)\n",
    "\n",
    "\n",
    "logistic_count4 = logisticregression(X_train,y_train,lamda=0.455265,epoch=10,print_every=1,lr=0.000407,decay=11)\n",
    "logistic_count4.train()\n",
    "        \n",
    "print(logistic_count4.evaluate(X_train,y_train))\n",
    "print(logistic_count4.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 256) (600, 256) (1400, 1) (600, 1)\n",
      "Epoch : 0  Loss: 0.6931471805599454\n",
      "Epoch : 1  Loss: 0.6260173199159424\n",
      "Epoch : 2  Loss: 0.6002614260404124\n",
      "Epoch : 3  Loss: 0.598726394674755\n",
      "Epoch : 4  Loss: 0.5986622185693936\n",
      "Epoch : 5  Loss: 0.5986603339307318\n",
      "Epoch : 6  Loss: 0.5986602920517511\n",
      "Epoch : 7  Loss: 0.598660291303913\n",
      "Epoch : 8  Loss: 0.5986602912927511\n",
      "Epoch : 9  Loss: 0.5986602912926081\n",
      "0.7028571428571428\n",
      "0.5966666666666667\n"
     ]
    }
   ],
   "source": [
    "X_preprocess = get_tf_idf_grams(np.vstack((X_train_,X_test_)).flatten(),4)\n",
    "X_preprocess.shape\n",
    "\n",
    "cross_validate(X_preprocess[:2000,:], y,lr=0.004433,lamda=0.432127,k=4,epoch=16,decay=4)\n",
    "\n",
    "C_tf_4 = X_preprocess[2000:,:]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test(X_preprocess[:2000,:],y,0.3)\n",
    "\n",
    "\n",
    "logistic_tf4 = logisticregression(X_train,y_train,lamda=0.455265,epoch=10,print_every=1,lr=0.000407,decay=11)\n",
    "logistic_tf4.train()\n",
    "        \n",
    "print(logistic_tf4.evaluate(X_train,y_train))\n",
    "print(logistic_tf4.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 4096) (600, 4096) (1400, 1) (600, 1)\n",
      "Epoch : 0  Loss: 0.6931471805599454\n",
      "Epoch : 1  Loss: 0.3746954683721624\n",
      "Epoch : 2  Loss: 0.23994925474338094\n",
      "Epoch : 3  Loss: 0.23422424550847226\n",
      "Epoch : 4  Loss: 0.23399751150543982\n",
      "Epoch : 5  Loss: 0.23399086485544413\n",
      "Epoch : 6  Loss: 0.23399071716632397\n",
      "Epoch : 7  Loss: 0.2339907145290239\n",
      "Epoch : 8  Loss: 0.2339907144896612\n",
      "Epoch : 9  Loss: 0.23399071448915654\n",
      "0.9635714285714285\n",
      "0.6433333333333333\n"
     ]
    }
   ],
   "source": [
    "X_preprocess = get_tf_idf_grams(np.vstack((X_train_,X_test_)).flatten(),6)\n",
    "X_preprocess.shape\n",
    "\n",
    "cross_validate(X_preprocess[:2000,:], y,lr=0.004433,lamda=0.432127,k=4,epoch=16,decay=7)\n",
    "\n",
    "C_tf_6 = X_preprocess[2000:,:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test(X_preprocess[:2000,:],y,0.3)\n",
    "\n",
    "\n",
    "logistic_tf6 = logisticregression(X_train,y_train,lamda=0.455265,epoch=10,print_every=1,lr=0.000407,decay=11)\n",
    "logistic_tf6.train()\n",
    "        \n",
    "print(logistic_tf6.evaluate(X_train,y_train))\n",
    "print(logistic_tf6.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = scale(np.array(X)[:2000,:])\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "sumbission = []\n",
    "for i in range(len(X_test_final)):\n",
    "    r1 = logistic.predict(X_test_final[i])\n",
    "#     r2 = logistic_count6.predict(C_count_6[i])\n",
    "#     r3 = logistic_tf4.predict(C_tf_4[i])\n",
    "#     r4 = logistic_tf6.predict(C_tf_6[i])\n",
    "    \n",
    "    \n",
    "#     votes = [r1[0],r2[0],r3[0],r4[0]]\n",
    "    \n",
    "#     print(Counter(votes))\n",
    "#     print(Counter(votes).most_common(1)[0][0])\n",
    "    \n",
    "#     break\n",
    "#     sumbission.append([i,int(Counter(votes).most_common(1)[0][0])])\n",
    "    \n",
    "    sumbission.append([i,int(r1)])\n",
    "    \n",
    "# sumbission\n",
    "df = pd.DataFrame(sumbission)\n",
    "df.columns = ['Id','Bound']\n",
    "df.to_csv('cv_64.4.csv',index=False)\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  Bound\n",
       "0    0      1\n",
       "1    1      0\n",
       "2    2      1\n",
       "3    3      0\n",
       "4    4      1\n",
       "5    5      1\n",
       "6    6      1\n",
       "7    7      0\n",
       "8    8      1\n",
       "9    9      1\n",
       "10  10      1\n",
       "11  11      0\n",
       "12  12      0\n",
       "13  13      0\n",
       "14  14      0\n",
       "15  15      0\n",
       "16  16      0\n",
       "17  17      0\n",
       "18  18      0\n",
       "19  19      1\n",
       "20  20      1\n",
       "21  21      1\n",
       "22  22      0\n",
       "23  23      1\n",
       "24  24      0\n",
       "25  25      1\n",
       "26  26      1\n",
       "27  27      0\n",
       "28  28      0\n",
       "29  29      0\n",
       "30  30      0\n",
       "31  31      0\n",
       "32  32      0\n",
       "33  33      1\n",
       "34  34      0\n",
       "35  35      0\n",
       "36  36      1\n",
       "37  37      0\n",
       "38  38      1\n",
       "39  39      0\n",
       "40  40      0\n",
       "41  41      0\n",
       "42  42      1\n",
       "43  43      1\n",
       "44  44      0\n",
       "45  45      0\n",
       "46  46      0\n",
       "47  47      1\n",
       "48  48      1\n",
       "49  49      0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
